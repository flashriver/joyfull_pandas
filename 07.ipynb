{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第7章 文本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2020.06.24,   2020.06.25 review,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、string类型的性质"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. string与object的区别\n",
    "#### string类型和object不同之处有三：\n",
    "#### ① 字符存取方法（string accessor methods，如str.count）会返回相应数据的Nullable类型，而object会随缺失值的存在而改变返回类型\n",
    "#### ② 某些Series方法不能在string上使用，例如： Series.str.decode()，因为存储的是字符串而不是字节\n",
    "#### ③ string类型在缺失值存储或运算时，类型会广播为pd.NA，而不是浮点型np.nan\n",
    "#### 其余全部内容在当前版本下完全一致，但迎合Pandas的发展模式，我们仍然全部用string来操作字符串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. string类型的转换\n",
    "#### 如果将一个其他类型的容器直接转换string类型可能会出错："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series([1,'1.']).astype('string') #报错 ValueError: StringArray requires a sequence of strings or pandas.NA\n",
    "#pd.Series([1,2]).astype('string') #报错 ValueError: StringArray requires a sequence of strings or pandas.NA\n",
    "#pd.Series([True,False]).astype('string') #报错 ValueError: StringArray requires a sequence of strings or pandas.NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 当下正确的方法是分两部转换，先转为str型object，再转为string类型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1    1.\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,'1.']) # 原始类型是 int64 和 str 混合的 object\n",
    "#pd.Series([1,'1.']).astype('string') #直接转换会报错 ValueError: StringArray requires a sequence of strings or pandas.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1    1.\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,'1.']).astype('str') # 第一步转为 str 的object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1    1.\n",
       "dtype: string"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,'1.']).astype('str').astype('string') # 第二步转为 string 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1    1.\n",
       "dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#上述转换是可逆的吗？\n",
    "# 转回object肯定是没问题的, object就类似于 \"不能识别的数据类型\"\n",
    "pd.Series([1,'1.']).astype('str').astype('string').astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2]) #原始类型是 int64\n",
    "#pd.Series([1,2]).astype('string') #直接转换会报错--ValueError: StringArray requires a sequence of strings or pandas.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2]).astype('str') # 第一步转为 str 类型的 object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "dtype: string"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2]).astype('str').astype('string') # 第二步转为 string 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 能否转回 int64?\n",
    "pd.Series([1,2]).astype('str').astype('string').astype('str')\n",
    "pd.Series([1,2]).astype('str').astype('string').astype('str').astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1.1\n",
       "1    2.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# float 类型呢?\n",
    "pd.Series([1.1,2]).astype('str').astype('string').astype('str').astype('float')\n",
    "# 也没问题\n",
    "# 但是包含缺失值的话就需要注意了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.float64, float)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这个缺失值好像算是正确转换了的\n",
    "type(pd.Series([1.1,2,np.nan]).astype('str').astype('string').astype('str').astype('float')[2]) ,type(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([True,False]) # 原始类型是布尔型\n",
    "#pd.Series([True,False]).astype('string') #直接转换会报错 ValueError: StringArray requires a sequence of strings or pandas.NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([True,False]).astype('str') #第一步转为 str 类型的object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1    False\n",
       "dtype: string"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([True,False]).astype('str').astype('string') #第二步转为 string 类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2020-06-25 09:08:21.423238\n",
       "1   2008-08-08 08:08:08.000000\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将时间转为string\n",
    "from datetime import datetime\n",
    "pd.Series([datetime.now(),datetime(2008,8,8,8,8,8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-06-25 09:07:39.336831\n",
       "1    2008-08-08 08:08:08.000000\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([datetime.now(),datetime(2008,8,8,8,8,8)]).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-06-25 09:07:51.157507\n",
       "1    2008-08-08 08:08:08.000000\n",
       "dtype: string"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([datetime.now(),datetime(2008,8,8,8,8,8)]).astype('str').astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-06-25 09:09:01.577534\n",
       "1    2008-08-08 08:08:08.000000\n",
       "dtype: string"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 时间可以直接转为 string\n",
    "pd.Series([datetime.now(),datetime(2008,8,8,8,8,8)]).astype('string') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2020-06-25 09:20:25.553656\n",
       "1   2008-08-08 08:08:08.000000\n",
       "dtype: datetime64[ns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将string转为时间也可以直接转\n",
    "pd.Series([datetime.now(),datetime(2008,8,8,8,8,8)]).astype('string').astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取日期留待第九章学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 但这种方法也不是特别正确, 因为会把缺失值也按字面显示转为string. 见下文中的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、拆分与拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. str.split方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （a）分割符与str的位置元素选取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a_b_c\n",
       "1    c_d_e\n",
       "2     <NA>\n",
       "3    f_g_h\n",
       "dtype: string"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(['a_b_c', 'c_d_e', np.nan, 'f_g_h'], dtype=\"string\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据某一个元素分割，默认为空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1    [c, d, e]\n",
       "2         <NA>\n",
       "3    [f, g, h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'b', 'c'], ['c', 'd', 'e'], ['f', 'g', 'h'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对比元素级的方法--缺失值没有split的属性方法,因此会报错\n",
    "s[0].split('_'),s[1].split('_'),s[3].split('_'),\n",
    "#s[2].split('_'),# AttributeError: 'NAType' object has no attribute 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因此, 由于缺失值的存在, Series对象的 元素级 apply 方法不能直接使用\n",
    "#s.apply(lambda x: x.split('_')) \n",
    "# AttributeError: 'NAType' object has no attribute 'split'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1    [c, d, e]\n",
       "2         <NA>\n",
       "3    [f, g, h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用apply方法的话,需要分情况\n",
    "# 但是 .str.split 方法为什么能正确执行? --因为, 第一步的 .str 相当于是做了判断和转换: 如不是缺失值则转为str类型, 然后再进行下一步的split, 而缺失值则会什么都不做.\n",
    "def split_func(x):\n",
    "    if x is pd.NA:\n",
    "        y = x\n",
    "    else :\n",
    "        y = x.split('_')\n",
    "    return y\n",
    "s.apply(lambda x: split_func(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1      [c>d>e]\n",
       "2         <NA>\n",
       "3     [f, g>h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 考虑一个例子\n",
    "s_ = pd.Series(['a<b<c', 'c>d>e', np.nan, 'f<g>h'], dtype=\"string\")\n",
    "s_.str.split('<') # 直接调用 .str.split 方法是没问题的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1      [c>d>e]\n",
       "2      [, NA>]\n",
       "3     [f, g>h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但如果先做了 astype 类型转换, 就出现问题了-缺失值被转为了它的字面表示 <NA> ,然后被split\n",
    "s_.astype('str').str.split('<')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#s.apply(lambda x: re.split(x,'_')) # 使用正则表达式的分割函数同样会因为缺失值导致出问题 TypeError: first argument must be string or compiled pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1    [c, d, e]\n",
       "2         <NA>\n",
       "3    [f, g, h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用正则表达式的方法,也需重新定义个函数以处理缺失值--所以re的代码也不够鲁棒, 实际使用时, 应该自行增加这类判断.\n",
    "def my_re_split(x,rep=''):\n",
    "    if x is pd.NA:\n",
    "        y = x\n",
    "    else :\n",
    "        y = re.split(rep,x)\n",
    "    return y\n",
    "s.apply(lambda x: my_re_split(x,'_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a', 'b', 'c'], ['a', 'b', 'c'], ['f', 'g', 'h'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split('_',s[0]),re.split('_',s[0]),    re.split('_',s[3]),\n",
    "# re.split('_',s[2]),#TypeError: expected string or bytes-like object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "\u001b[1;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Split the source string by the occurrences of the pattern,\n",
       "    returning a list containing the resulting substrings.  If\n",
       "    capturing parentheses are used in pattern, then the text of all\n",
       "    groups in the pattern are also returned as part of the resulting\n",
       "    list.  If maxsplit is nonzero, at most maxsplit splits occur,\n",
       "    and the remainder of the string is returned as the final element\n",
       "    of the list.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsplit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\programdata\\anaconda3\\lib\\re.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.split??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这里需要注意split后的类型是object，因为现在Series中的元素已经不是string，而是list(调用split后哪怕没被分割,也是返回list)，而string类型只能含有字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['a', 'b', 'c']\n",
       "1    ['c', 'd', 'e']\n",
       "2               <NA>\n",
       "3    ['f', 'g', 'h']\n",
       "dtype: string"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当然,这时候仍然可以用上述的迂回方式将list转为string---注意这时缺失值 pd.NA 也被转为了字面显示的字符串,幸好该缺失值表示法有类似于转义符的表达形式 <NA>\n",
    "s.apply(lambda x: my_re_split(x,'_')).astype('str').astype('string')\n",
    "# 但实际上很多时候是没必要这么做的,因为我们需要的就是这个list,唯一需要的是要把list分拆成多列,或提取某个元素, 分别见下文的 expand 参数和后边的 extract 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### str方法支持元素的选择，如果该单元格元素是列表，那么str[i]表示取出第i个元素，如果是单个元素，则先把元素转为列表再取出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       b\n",
       "1       d\n",
       "2    <NA>\n",
       "3       g\n",
       "dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_').str[1]\n",
    "# 缺失值则还是返回缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1    [c, d, e]\n",
       "2         <NA>\n",
       "3    [f, g, h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_')\n",
    "# 注意分拆后, 非空的单元格里的元素已经是个list了,但str[1]方法能够准确地提取到这个list的相应位置的元素--这就是Series of list对象的向量化切片方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [b, c]\n",
       "1    [d, e]\n",
       "2      <NA>\n",
       "3    [g, h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量化的切片方式--1\n",
    "s.str.split('_').str[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [c, b, a]\n",
       "1    [e, d, c]\n",
       "2         <NA>\n",
       "3    [h, g, f]\n",
       "dtype: object"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 向量化的切片方式--2\n",
    "# 逆序遍历\n",
    "s.str.split('_').str[::-1]\n",
    "# 这么看来, 其他list的各种切片的操作都应该会被这种方法所支持"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['a\n",
       "1    <NA\n",
       "2    ['d\n",
       "dtype: string"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 字符串对象当然也是支持切片的, 但转为字符串以后就不容易取到list的元素了\n",
    "s.apply(lambda x: my_re_split(x,'_')).astype('str').astype('string').str[:3]\n",
    "# 注意这个奇怪的事情--pd.NA对象也被转为 string 了--这不合适--可以通过replace方法先将 <NA> 字符串替换为缺失值\n",
    "# .astype('str').astype('string') 会把 pd.NA 也按字面显示转为 string, 这就不合适了吧?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas._libs.missing.NAType"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace方法先将 <NA> 字符串替换为缺失值--注意这个方法是针对单元格的,而不是单元格内的内容作为字符串的 .str.replace 方法. \n",
    "type(s.apply(lambda x: my_re_split(x,'_')).astype('str').astype('string').replace('<NA>',pd.NA).str[:3][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<NA>, pandas._libs.missing.NAType)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[2],type(s[2]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<NA>, pandas._libs.missing.NAType)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(lambda x: split_func(x))[2],type(s.apply(lambda x: split_func(x))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<NA>, pandas._libs.missing.NAType)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(lambda x: my_re_split(x,'_'))[2],type(s.apply(lambda x: my_re_split(x,'_'))[2]),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<NA>', str)"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.apply(lambda x: my_re_split(x,'_')).astype('str')[2],type(s.apply(lambda x: my_re_split(x,'_')).astype('str')[2])\n",
    "# 在这一步就出问题了--astype('str')会按字面显示将objec转为字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<NA>', str)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.NA变成了 string--已经出现了数据处理错误\n",
    "s.apply(lambda x: my_re_split(x,'_')).astype('str').astype('string')[2],type(s.apply(lambda x: my_re_split(x,'_')).astype('str').astype('string')[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('None', str)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,'1.',pd.NA]).astype('str').astype('string')[2],type(pd.Series([1,'1.',pd.NA]).astype('str').astype('string')[2]) \n",
    "pd.Series([1,'1.',np.nan]).astype('str').astype('string')[2],type(pd.Series([1,'1.',np.nan]).astype('str').astype('string')[2]) \n",
    "pd.Series([1,'1.',None]).astype('str').astype('string')[2],type(pd.Series([1,'1.',None]).astype('str').astype('string')[2]) \n",
    "#pd.Series([1,2,pd.NA]).astype('str').astype('string')[2], type(pd.Series([1,2,pd.NA]).astype('str').astype('string')[2])\n",
    "#pd.Series([1,2,np.nan]).astype('str').astype('string')[2], type(pd.Series([1,2,np.nan]).astype('str').astype('string')[2])\n",
    "#pd.Series([True,False]).astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mFrameOrSeries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFrameOrSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"raise\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mFrameOrSeries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;34m\"\"\"\n",
       "        Cast a pandas object to a specified dtype ``dtype``.\n",
       "\n",
       "        Parameters\n",
       "        ----------\n",
       "        dtype : data type, or dict of column name -> data type\n",
       "            Use a numpy.dtype or Python type to cast entire pandas object to\n",
       "            the same type. Alternatively, use {col: dtype, ...}, where col is a\n",
       "            column label and dtype is a numpy.dtype or Python type to cast one\n",
       "            or more of the DataFrame's columns to column-specific types.\n",
       "        copy : bool, default True\n",
       "            Return a copy when ``copy=True`` (be very careful setting\n",
       "            ``copy=False`` as changes to values then may propagate to other\n",
       "            pandas objects).\n",
       "        errors : {'raise', 'ignore'}, default 'raise'\n",
       "            Control raising of exceptions on invalid data for provided dtype.\n",
       "\n",
       "            - ``raise`` : allow exceptions to be raised\n",
       "            - ``ignore`` : suppress exceptions. On error return original object.\n",
       "\n",
       "        Returns\n",
       "        -------\n",
       "        casted : same type as caller\n",
       "\n",
       "        See Also\n",
       "        --------\n",
       "        to_datetime : Convert argument to datetime.\n",
       "        to_timedelta : Convert argument to timedelta.\n",
       "        to_numeric : Convert argument to a numeric type.\n",
       "        numpy.ndarray.astype : Cast a numpy array to a specified type.\n",
       "\n",
       "        Examples\n",
       "        --------\n",
       "        Create a DataFrame:\n",
       "\n",
       "        >>> d = {'col1': [1, 2], 'col2': [3, 4]}\n",
       "        >>> df = pd.DataFrame(data=d)\n",
       "        >>> df.dtypes\n",
       "        col1    int64\n",
       "        col2    int64\n",
       "        dtype: object\n",
       "\n",
       "        Cast all columns to int32:\n",
       "\n",
       "        >>> df.astype('int32').dtypes\n",
       "        col1    int32\n",
       "        col2    int32\n",
       "        dtype: object\n",
       "\n",
       "        Cast col1 to int32 using a dictionary:\n",
       "\n",
       "        >>> df.astype({'col1': 'int32'}).dtypes\n",
       "        col1    int32\n",
       "        col2    int64\n",
       "        dtype: object\n",
       "\n",
       "        Create a series:\n",
       "\n",
       "        >>> ser = pd.Series([1, 2], dtype='int32')\n",
       "        >>> ser\n",
       "        0    1\n",
       "        1    2\n",
       "        dtype: int32\n",
       "        >>> ser.astype('int64')\n",
       "        0    1\n",
       "        1    2\n",
       "        dtype: int64\n",
       "\n",
       "        Convert to categorical type:\n",
       "\n",
       "        >>> ser.astype('category')\n",
       "        0    1\n",
       "        1    2\n",
       "        dtype: category\n",
       "        Categories (2, int64): [1, 2]\n",
       "\n",
       "        Convert to ordered categorical type with custom ordering:\n",
       "\n",
       "        >>> cat_dtype = pd.api.types.CategoricalDtype(\n",
       "        ...     categories=[2, 1], ordered=True)\n",
       "        >>> ser.astype(cat_dtype)\n",
       "        0    1\n",
       "        1    2\n",
       "        dtype: category\n",
       "        Categories (2, int64): [2 < 1]\n",
       "\n",
       "        Note that using ``copy=False`` and changing data on a new\n",
       "        pandas object may propagate changes:\n",
       "\n",
       "        >>> s1 = pd.Series([1, 2])\n",
       "        >>> s2 = s1.astype('int64', copy=False)\n",
       "        >>> s2[0] = 10\n",
       "        >>> s1  # note that s1[0] has changed too\n",
       "        0    10\n",
       "        1     2\n",
       "        dtype: int64\n",
       "        \"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mif\u001b[0m \u001b[0mis_dict_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# i.e. Series\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;34m\"Only the Series name can be used for \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;34m\"the key in Series dtype mappings.\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mnew_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;34m\"Only a column name can be used for the \"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[1;34m\"key in a dtype mappings argument.\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mfor\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mif\u001b[0m \u001b[0mcol_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                        \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                    \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melif\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# GH 18099/22869: columnwise conversion to extension dtype\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# GH 24704: use iloc to handle duplicate column names\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;33m]\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;31m# else, only a single dtype is given\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;31m# GH 19920: retain column metadata after concat\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# astype是个比较霸道的类型转换方式, 缺乏对缺失值的有效操作方式.\n",
    "s.astype??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['c', 'd', 'e'], ['c', 'd', 'e'])"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_')[:][1],  s.str.split('_')[1]# 选取效果和上述是有些区别的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        a_b_c\n",
       "1    [a, b, c]\n",
       "dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['a_b_c', ['a','b','c']], dtype=\"object\")#.str[1]\n",
    "#第一个元素先用list('a_b_c')转为['a','_','b','_','c'],然后再切片\n",
    "#第二个元素是个list, list会正常的切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    _\n",
       "1    b\n",
       "dtype: object"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['a_b_c', ['a','b','c']], dtype=\"object\").str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        a_\n",
       "1    [a, b]\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['a_b_c', ['a','b','c']], dtype=\"object\").str[:2]\n",
    "# str虽然看起来是转为str再操作, 但实际上, 对于是字符串的单元格,会选择该字符串相应位置的元素,而对于是list的, 则会使用切片方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （b）其他参数\n",
    "#### expand参数控制了是否将列拆开，n参数代表最多分割多少次(分割成n+1列)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Split strings around given separator/delimiter.\n",
       "\n",
       "Splits the string in the Series/Index from the beginning,\n",
       "at the specified delimiter string. Equivalent to :meth:`str.split`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "pat : str, optional\n",
       "    String or regular expression to split on.\n",
       "    If not specified, split on whitespace.\n",
       "n : int, default -1 (all)\n",
       "    Limit number of splits in output.\n",
       "    ``None``, 0 and -1 will be interpreted as return all splits.\n",
       "expand : bool, default False\n",
       "    Expand the splitted strings into separate columns.\n",
       "\n",
       "    * If ``True``, return DataFrame/MultiIndex expanding dimensionality.\n",
       "    * If ``False``, return Series/Index, containing lists of strings.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Series, Index, DataFrame or MultiIndex\n",
       "    Type matches caller unless ``expand=True`` (see Notes).\n",
       "\n",
       "See Also\n",
       "--------\n",
       "Series.str.split : Split strings around given separator/delimiter.\n",
       "Series.str.rsplit : Splits string around given separator/delimiter,\n",
       "    starting from the right.\n",
       "Series.str.join : Join lists contained as elements in the Series/Index\n",
       "    with passed delimiter.\n",
       "str.split : Standard library version for split.\n",
       "str.rsplit : Standard library version for rsplit.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The handling of the `n` keyword depends on the number of found splits:\n",
       "\n",
       "- If found splits > `n`,  make first `n` splits only\n",
       "- If found splits <= `n`, make all splits\n",
       "- If for a certain row the number of found splits < `n`,\n",
       "  append `None` for padding up to `n` if ``expand=True``\n",
       "\n",
       "If using ``expand=True``, Series and Index callers return DataFrame and\n",
       "MultiIndex objects, respectively.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> s = pd.Series([\"this is a regular sentence\",\n",
       "...                \"https://docs.python.org/3/tutorial/index.html\",\n",
       "...                np.nan])\n",
       "0                       this is a regular sentence\n",
       "1    https://docs.python.org/3/tutorial/index.html\n",
       "2                                              NaN\n",
       "dtype: object\n",
       "\n",
       "In the default setting, the string is split by whitespace.\n",
       "\n",
       ">>> s.str.split()\n",
       "0                   [this, is, a, regular, sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       "Without the `n` parameter, the outputs of `rsplit` and `split`\n",
       "are identical.\n",
       "\n",
       ">>> s.str.rsplit()\n",
       "0                   [this, is, a, regular, sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       "The `n` parameter can be used to limit the number of splits on the\n",
       "delimiter. The outputs of `split` and `rsplit` are different.\n",
       "\n",
       ">>> s.str.split(n=2)\n",
       "0                     [this, is, a regular sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       ">>> s.str.rsplit(n=2)\n",
       "0                     [this is a, regular, sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       "The `pat` parameter can be used to split by other characters.\n",
       "\n",
       ">>> s.str.split(pat = \"/\")\n",
       "0                         [this is a regular sentence]\n",
       "1    [https:, , docs.python.org, 3, tutorial, index...\n",
       "2                                                  NaN\n",
       "dtype: object\n",
       "\n",
       "When using ``expand=True``, the split elements will expand out into\n",
       "separate columns. If NaN is present, it is propagated throughout\n",
       "the columns during the split.\n",
       "\n",
       ">>> s.str.split(expand=True)\n",
       "                                               0     1     2        3\n",
       "0                                           this    is     a  regular\n",
       "1  https://docs.python.org/3/tutorial/index.html  None  None     None\n",
       "2                                            NaN   NaN   NaN      NaN \\\n",
       "             4\n",
       "0     sentence\n",
       "1         None\n",
       "2          NaN\n",
       "\n",
       "For slightly more complex use cases like splitting the html document name\n",
       "from a url, a combination of parameter settings can be used.\n",
       "\n",
       ">>> s.str.rsplit(\"/\", n=1, expand=True)\n",
       "                                    0           1\n",
       "0          this is a regular sentence        None\n",
       "1  https://docs.python.org/3/tutorial  index.html\n",
       "2                                 NaN         NaN\n",
       "\n",
       "Remember to escape special characters when explicitly using regular\n",
       "expressions.\n",
       "\n",
       ">>> s = pd.Series([\"1+1=2\"])\n",
       "\n",
       ">>> s.str.split(r\"\\+|=\", expand=True)\n",
       "     0    1    2\n",
       "0    1    1    2\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"str_split\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"side\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"beginning\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"method\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"split\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mforbid_nonstring_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bytes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s.str.split??\n",
    "#n : int, default -1 (all)\n",
    "#    Limit number of splits in output.\n",
    "#    ``None``, 0 and -1 will be interpreted as return all splits.\n",
    "#expand : bool, default False\n",
    "#    Expand the splitted strings into separate columns.\n",
    "#\n",
    "#    * If ``True``, return DataFrame/MultiIndex expanding dimensionality.\n",
    "#    * If ``False``, return Series/Index, containing lists of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>b</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>d</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2\n",
       "0     a     b     c\n",
       "1     c     d     e\n",
       "2  <NA>  <NA>  <NA>\n",
       "3     f     g     h"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, b, c]\n",
       "1    [c, d, e]\n",
       "2         <NA>\n",
       "3    [f, g, h]\n",
       "dtype: object"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_',n=3) #expand 默认是 False, 因此单独指定 n=3 是没有效果的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>b_c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>d_e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f</td>\n",
       "      <td>g_h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0     a   b_c\n",
       "1     c   d_e\n",
       "2  <NA>  <NA>\n",
       "3     f   g_h"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_',expand=True,n=1) # n=1 则拆分一次, expand=True 则会拆分成多列--由于只进行了一次拆分,于是被拆分成了1+1=2列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>b_c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c</td>\n",
       "      <td>d_e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f</td>\n",
       "      <td>g_h</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1\n",
       "0     a   b_c\n",
       "1     c   d_e\n",
       "2  <NA>  <NA>\n",
       "3     f   g_h"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.split('_',expand=True,n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. str.cat方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （a）不同对象的拼接模式\n",
    "#### cat方法对于不同对象的作用结果并不相同，其中的对象包括：单列、双列、多列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ① 对于单个Series而言，就是指所有的元素进行字符合并为一个字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ab\n",
       "1    <NA>\n",
       "2       d\n",
       "dtype: string"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(['ab',None,'d'],dtype='string')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abd'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.cat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 其中可选sep分隔符参数，和缺失值替代字符na_rep参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab,d'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.cat(sep=',')\n",
    "# oracle 里有 listagg(11.2及以上)和 wm_concat 两个函数实现相同功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ab,*,d'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.cat(sep=',',na_rep='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mike</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>joy</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ross</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>john</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lisa</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lily</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  gender\n",
       "0  allen    male\n",
       "1   <NA>    <NA>\n",
       "2   mike    male\n",
       "3    joy  female\n",
       "4   ross    male\n",
       "5   john  female\n",
       "6   lisa    male\n",
       "7   lily  female"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 配合分组函数使用\n",
    "s_= pd.DataFrame({'name':['allen',None,'mike','joy','ross','john','lisa','lily'],'gender':['male',None,'male','female','male','female','male','female']},dtype='string')\n",
    "s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8 entries, 0 to 7\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   name    7 non-null      string\n",
      " 1   gender  7 non-null      string\n",
      "dtypes: string(2)\n",
      "memory usage: 256.0 bytes\n"
     ]
    }
   ],
   "source": [
    "s_.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "female           joy,john,lily\n",
       "male      allen,mike,ross,lisa\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_.groupby('gender').apply(lambda x:x['name'].str.cat(sep=',',na_rep='?'))\n",
    "# 分组时会忽略缺失值--应该先用一个\"安全\"的字符串替换缺失值,等分组后再替换回去."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "female            joy,john,lily\n",
       "male       allen,mike,ross,lisa\n",
       "unknown                       ?\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先用安全的字符串'unknown'填充缺失值,分组后,在cat前再把name列的'unknown'替换回缺失值\n",
    "s_.fillna('unknown').groupby('gender').apply(lambda x:x['name'].replace('unknown',pd.NA).str.cat(sep=',',na_rep='?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其实还可以进一步先排序再cat.排序时是否可以设置缺失值放在前边还是放在后边?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ② 对于两个Series合并而言，是对应索引的元素进行合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      24\n",
       "1      cc\n",
       "2    <NA>\n",
       "dtype: string"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = pd.Series(['24','cc',None],dtype='string')\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ab24\n",
       "1    <NA>\n",
       "2    <NA>\n",
       "dtype: string"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.cat(s2)\n",
    "# 第二和第三个元素都成了pd.NA了--默认情况下,缺失值会传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 同样也有相应参数，需要注意的是两个缺失值会被同时替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ab,24\n",
       "1     *,cc\n",
       "2      d,*\n",
       "dtype: string"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.cat(s2,sep=',',na_rep='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ab,24\n",
       "1     *,cc\n",
       "2      d,?\n",
       "dtype: string"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#如果只替换其中之一的缺失值,或者分别用不同的字符替换两个series的缺失值,可以先使用 replace 方法替换缺失值后, 再用 str.cat 方法拼接.\n",
    "s.replace(pd.NA,'*').str.cat(s2,sep=',',na_rep='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ③ 多列拼接可以分为表的拼接和多Series拼接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 表的拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ab\n",
       "1    <NA>\n",
       "2       d\n",
       "dtype: string"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1\n",
       "0  1     5\n",
       "1  3     b\n",
       "2  5  <NA>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    ab15\n",
       "1     *3b\n",
       "2     d5*\n",
       "dtype: string"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(s)\n",
    "d_ = pd.DataFrame({0:['1','3','5'],1:['5','b',None]},dtype='string')\n",
    "display(d_)\n",
    "s.str.cat(d_,na_rep='*')\n",
    "# 将一个 Series 对象和一个等长的 DF 对象按索引进行行的拼接,会按索引对其后,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0_l</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ab</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0_l  0     1\n",
       "0    ab  1     5\n",
       "1  <NA>  3     b\n",
       "2     d  5  <NA>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对照来看, 确实是把df的行和series的单元格都拼在一起了.\n",
    "s.to_frame().join(d_,lsuffix='_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s.to_frame().join(d_,lsuffix='_l').str.cat(na_rep='*') \n",
    "#AttributeError: 'DataFrame' object has no attribute 'str'\n",
    "# 想把 DF 的行拼接到一起还不是那么直接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_1</th>\n",
       "      <th>C_2</th>\n",
       "      <th>C_3</th>\n",
       "      <th>C_4</th>\n",
       "      <th>C_5</th>\n",
       "      <th>C_6</th>\n",
       "      <th>C_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_1 C_2 C_3 C_4 C_5 C_6 C_7\n",
       "a   5   2  18  10   2  14   2\n",
       "b  15  17   6  12   2   4  10\n",
       "c   1  18  12  17   8   8   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "a    \n",
       "b    \n",
       "c    \n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "a     +5+2+18+10+2+14+2\n",
       "b    +15+17+6+12+2+4+10\n",
       "c     +1+18+12+17+8+8+9\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个将df按行拼接起来的思路\n",
    "# 先构造索引一致且值全为空字符串的Series,然后用Series拼接df的方式, 用这个Series去拼接表\n",
    "df=pd.DataFrame(np.random.randint(0,20,21).reshape(3,7),index=list('abc'),columns=['C_1','C_2','C_3','C_4','C_5','C_6','C_7']).astype('str')\n",
    "display(df)\n",
    "s1=pd.Series(''*3,index=df.index)\n",
    "display(s1)\n",
    "s1.str.cat(df,sep='+',na_rep='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     5+2+18+10+2+14+2\n",
       "b    15+17+6+12+2+4+10\n",
       "c     1+18+12+17+8+8+9\n",
       "Name: C_1, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 或者, 将df的第一列单独拿出来作为 series, 用上述方法, 与 df 剩余的列进行拼接\n",
    "df.C_1.str.cat(df.iloc[:,1:],sep='+',na_rep='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     5+2+18+10+2+14+2\n",
       "b    15+17+6+12+2+4+10\n",
       "c     1+18+12+17+8+8+9\n",
       "Name: C_1, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以将df拆分成series, 然后用循环,进行拼接\n",
    "s1=df.C_1\n",
    "for col in df.columns[1:]:\n",
    "    s1=s1.str.cat(df[col],sep='+')\n",
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多个Series按索引拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    abab0abab\n",
       "1         <NA>\n",
       "2        dd0dd\n",
       "dtype: string"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.cat([s+'0',s*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "#有没有实现该功能的类似的顶级方法? 来统一处理 Series 和 DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （b）cat中的索引对齐\n",
    "#### 当前版本中，如果两边合并的索引不相同且未指定join参数，默认为左连接，设置join='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    a\n",
       "2    b\n",
       "3    c\n",
       "dtype: string"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = pd.Series(list('abc'),index=[1,2,3],dtype='string')\n",
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ab*\n",
       "1     *a\n",
       "2     db\n",
       "dtype: string"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.cat(s2,na_rep='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ab_*\n",
       "1     *_*\n",
       "2     d_*\n",
       "dtype: string"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果二者索引没有对齐,相当于左连接,然后再拼接--由于左连接可能会引入缺失值,如果索引完全不对齐则会把s的每个元素与一个na_rep参数指定的字符串拼接起来.\n",
    "s.str.cat(s1,sep='_',na_rep='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <NA>\n",
       "1    <NA>\n",
       "2    <NA>\n",
       "dtype: string"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 不指定na_rep参数,则会导致缺失值的扩散\n",
    "s.str.cat(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(a     5+2+18+10+2+14+2\n",
       " b    15+17+6+12+2+4+10\n",
       " c     1+18+12+17+8+8+9\n",
       " Name: C_1, dtype: object,\n",
       " 0      ab\n",
       " 1    <NA>\n",
       " 2       d\n",
       " dtype: string)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1,s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     5+2+18+10+2+14+2*\n",
       "b    15+17+6+12+2+4+10*\n",
       "c     1+18+12+17+8+8+9*\n",
       "Name: C_1, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 更换s1和s2的位置, 发现确实是左连接\n",
    "s1.str.cat(s,na_rep='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、替换\n",
    "#### 广义上的替换，就是指str.replace函数的应用，fillna是针对缺失值的替换，上一章已经提及\n",
    "#### 提到替换，就不可避免地接触到正则表达式，这里默认读者已掌握常见正则表达式知识点，若对其还不了解的，可以通过[这份资料](https://regexone.com/)来熟悉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. str.replace的常见用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3    Aaba\n",
       "4    Baca\n",
       "5        \n",
       "6    <NA>\n",
       "7    CABA\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca','', np.nan, 'CABA', 'dog', 'cat'],dtype=\"string\")\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一个值写r开头的正则表达式，后一个写替换的字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       ***\n",
       "1       ***\n",
       "2         C\n",
       "3    ***aba\n",
       "4    ***aca\n",
       "5          \n",
       "6      <NA>\n",
       "7      CABA\n",
       "8       dog\n",
       "9       cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'^[AB]','***') #把字符串开头(大写)的A或B替换为\"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           ***\n",
       "1           ***\n",
       "2             C\n",
       "3        ***aba\n",
       "4        ***aca\n",
       "5              \n",
       "6          <NA>\n",
       "7    C*********\n",
       "8           dog\n",
       "9           cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'[(A)(B)]','***') # 分组替换, 把字符串当中(大写)的A或B都替换为\"***\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 子组与函数替换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过正整数调用子组（0返回字符本身，从1开始才是子组）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3     ba*\n",
       "4     ca*\n",
       "5        \n",
       "6    <NA>\n",
       "7     BA*\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'([ABC])(\\w+)',repl=lambda x:x.group(2)[1:]+'*')\n",
    "#repl : str or callable\n",
    "#    Replacement string or a callable. The callable is passed the regex\n",
    "#    match object and must return a replacement string to be used.\n",
    "#    See :func:`re.sub`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3    Aaba\n",
       "4    Baca\n",
       "5        \n",
       "6    <NA>\n",
       "7    CABA\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3    Aaba\n",
       "4    Baca\n",
       "5        \n",
       "6    <NA>\n",
       "7    CABA\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0))#[0:]+'*')\n",
    "# group(0) 返回字符串本身--没有匹配到的也是返回整个字符串, 如果是缺失值则还是返回缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3       A\n",
       "4       B\n",
       "5        \n",
       "6    <NA>\n",
       "7       C\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1))#[0:]+'*')\n",
    "# group(1) 返回匹配到的第一个子组--没有匹配的则继续返回整个字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3     aba\n",
       "4     aca\n",
       "5        \n",
       "6    <NA>\n",
       "7     ABA\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2))#[0:]+'*')\n",
    "# group(2) 返回匹配到的第二个子组--没有匹配的则继续返回整个字符串\n",
    "# 特别地,只匹配到一个子组的, 返回的是第一个子组呢,还是整个字符串呢?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>Aaba</td>\n",
       "      <td>Baca</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CABA</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])+(\\w+)',lambda x:x.group(0))</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>aba</td>\n",
       "      <td>aca</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>A</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0))</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>Aaba</td>\n",
       "      <td>Baca</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CABA</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1))</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>C</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2))</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>aba</td>\n",
       "      <td>aca</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ABA</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2)[1:])</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>ba</td>\n",
       "      <td>ca</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>BA</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0)[1:]+'****')</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>aba****</td>\n",
       "      <td>aca****</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>ABA****</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1)[1:]+'****')</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>****</td>\n",
       "      <td>****</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>****</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2)[1:]+'****')</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>ba****</td>\n",
       "      <td>ca****</td>\n",
       "      <td></td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>BA****</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  1  2        3        4  \\\n",
       "0                                                   A  B  C     Aaba     Baca   \n",
       "s.str.replace(r'([ABC])+(\\w+)',lambda x:x.group...  A  B  C      aba      aca   \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0))  A  B  C     Aaba     Baca   \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1))  A  B  C        A        B   \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2))  A  B  C      aba      aca   \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...  A  B  C       ba       ca   \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...  A  B  C  aba****  aca****   \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...  A  B  C     ****     ****   \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...  A  B  C   ba****   ca****   \n",
       "\n",
       "                                                   5     6        7    8    9  \n",
       "0                                                     <NA>     CABA  dog  cat  \n",
       "s.str.replace(r'([ABC])+(\\w+)',lambda x:x.group...    <NA>        A  dog  cat  \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0))    <NA>     CABA  dog  cat  \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1))    <NA>        C  dog  cat  \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2))    <NA>      ABA  dog  cat  \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...    <NA>       BA  dog  cat  \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...    <NA>  ABA****  dog  cat  \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...    <NA>     ****  dog  cat  \n",
       "s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(...    <NA>   BA****  dog  cat  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca','', np.nan, 'CABA', 'dog', 'cat'],dtype=\"string\")\n",
    "dfs=s.to_frame()\n",
    "dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0))\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0))\n",
    "dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1))\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1))\n",
    "dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2))\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2))\n",
    "#dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(3))\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(3)) # IndexError: no such group\n",
    "#dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(4))\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(4))\n",
    "dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2)[1:])\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2)[1:])\n",
    "dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0)[1:]+'****')\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(0)[1:]+'****')#012689列都没有拼接****,但是有返回值\n",
    "dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1)[1:]+'****')\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(1)[1:]+'****')\n",
    "dfs[\"s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2)[1:]+'****')\"]=s.str.replace(r'([ABC])(\\w+)',lambda x:x.group(2)[1:]+'****')\n",
    "dfs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col\t d_\t df\t dfs\t np\t pd\t s\t s1\t s2\t \n",
      "s_\t \n"
     ]
    }
   ],
   "source": [
    "who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Aa=A+a\n",
       "1           B\n",
       "2           C\n",
       "3    Aa=A+aba\n",
       "4    Ba=B+aca\n",
       "5            \n",
       "6        <NA>\n",
       "7        CABA\n",
       "8         dog\n",
       "9         cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还需进一步研究...............\n",
    "# 把ss的第一个元素设置为\n",
    "ss = pd.Series(['Aa', 'B', 'C', 'Aaba', 'Baca','', np.nan, 'CABA', 'dog', 'cat'],dtype=\"string\")\n",
    "ss.str.replace('([ABC])([abc])',lambda x:x.group(0)+'='+x.group(1)+'+'+x.group(2))#[0:]+'*')\n",
    "#ss.str.replace('([ABC])',lambda x:x.group(0)+','+x.group(1))#[0:]+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='A'>\n",
      "<re.Match object; span=(0, 1), match='B'>\n",
      "<re.Match object; span=(0, 1), match='C'>\n",
      "<re.Match object; span=(0, 1), match='A'>\n",
      "<re.Match object; span=(0, 1), match='B'>\n",
      "<re.Match object; span=(0, 1), match='C'>\n",
      "<re.Match object; span=(1, 2), match='A'>\n",
      "<re.Match object; span=(2, 3), match='B'>\n",
      "<re.Match object; span=(3, 4), match='A'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       a\n",
       "1        \n",
       "2        \n",
       "3     aba\n",
       "4     aca\n",
       "5        \n",
       "6    <NA>\n",
       "7        \n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.str.replace('([ABC])',lambda x:print(x))#[0:]+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0Aa=A+a\n",
       "1           1B\n",
       "2           2C\n",
       "3    3Aa=A+aba\n",
       "4    4Ba=B+aca\n",
       "5            5\n",
       "6         <NA>\n",
       "7        7CABA\n",
       "8         8dog\n",
       "9         9cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = pd.Series(['0Aa', '1B', '2C', '3Aaba', '4Baca','5', np.nan, '7CABA', '8dog', '9cat'],dtype=\"string\")\n",
    "ss.str.replace('([ABC])([abc])',lambda x:x.group(0)+'='+x.group(1)+'+'+x.group(2))#[0:]+'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Replace occurrences of pattern/regex in the Series/Index with\n",
       "some other string. Equivalent to :meth:`str.replace` or\n",
       ":func:`re.sub`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "pat : str or compiled regex\n",
       "    String can be a character sequence or regular expression.\n",
       "repl : str or callable\n",
       "    Replacement string or a callable. The callable is passed the regex\n",
       "    match object and must return a replacement string to be used.\n",
       "    See :func:`re.sub`.\n",
       "n : int, default -1 (all)\n",
       "    Number of replacements to make from start.\n",
       "case : bool, default None\n",
       "    Determines if replace is case sensitive:\n",
       "\n",
       "    - If True, case sensitive (the default if `pat` is a string)\n",
       "    - Set to False for case insensitive\n",
       "    - Cannot be set if `pat` is a compiled regex.\n",
       "\n",
       "flags : int, default 0 (no flags)\n",
       "    Regex module flags, e.g. re.IGNORECASE. Cannot be set if `pat` is a compiled\n",
       "    regex.\n",
       "regex : bool, default True\n",
       "    Determines if assumes the passed-in pattern is a regular expression:\n",
       "\n",
       "    - If True, assumes the passed-in pattern is a regular expression.\n",
       "    - If False, treats the pattern as a literal string\n",
       "    - Cannot be set to False if `pat` is a compiled regex or `repl` is\n",
       "      a callable.\n",
       "\n",
       "    .. versionadded:: 0.23.0\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Series or Index of object\n",
       "    A copy of the object with all matching occurrences of `pat` replaced by\n",
       "    `repl`.\n",
       "\n",
       "Raises\n",
       "------\n",
       "ValueError\n",
       "    * if `regex` is False and `repl` is a callable or `pat` is a compiled\n",
       "      regex\n",
       "    * if `pat` is a compiled regex and `case` or `flags` is set\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When `pat` is a compiled regex, all flags should be included in the\n",
       "compiled regex. Use of `case`, `flags`, or `regex=False` with a compiled\n",
       "regex will raise an error.\n",
       "\n",
       "Examples\n",
       "--------\n",
       "When `pat` is a string and `regex` is True (the default), the given `pat`\n",
       "is compiled as a regex. When `repl` is a string, it replaces matching\n",
       "regex patterns as with :meth:`re.sub`. NaN value(s) in the Series are\n",
       "left as is:\n",
       "\n",
       ">>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f.', 'ba', regex=True)\n",
       "0    bao\n",
       "1    baz\n",
       "2    NaN\n",
       "dtype: object\n",
       "\n",
       "When `pat` is a string and `regex` is False, every `pat` is replaced with\n",
       "`repl` as with :meth:`str.replace`:\n",
       "\n",
       ">>> pd.Series(['f.o', 'fuz', np.nan]).str.replace('f.', 'ba', regex=False)\n",
       "0    bao\n",
       "1    fuz\n",
       "2    NaN\n",
       "dtype: object\n",
       "\n",
       "When `repl` is a callable, it is called on every `pat` using\n",
       ":func:`re.sub`. The callable should expect one positional argument\n",
       "(a regex object) and return a string.\n",
       "\n",
       "To get the idea:\n",
       "\n",
       ">>> pd.Series(['foo', 'fuz', np.nan]).str.replace('f', repr)\n",
       "0    <_sre.SRE_Match object; span=(0, 1), match='f'>oo\n",
       "1    <_sre.SRE_Match object; span=(0, 1), match='f'>uz\n",
       "2                                                  NaN\n",
       "dtype: object\n",
       "\n",
       "Reverse every lowercase alphabetic word:\n",
       "\n",
       ">>> repl = lambda m: m.group(0)[::-1]\n",
       ">>> pd.Series(['foo 123', 'bar baz', np.nan]).str.replace(r'[a-z]+', repl)\n",
       "0    oof 123\n",
       "1    rab zab\n",
       "2        NaN\n",
       "dtype: object\n",
       "\n",
       "Using regex groups (extract second group and swap case):\n",
       "\n",
       ">>> pat = r\"(?P<one>\\w+) (?P<two>\\w+) (?P<three>\\w+)\"\n",
       ">>> repl = lambda m: m.group('two').swapcase()\n",
       ">>> pd.Series(['One Two Three', 'Foo Bar Baz']).str.replace(pat, repl)\n",
       "0    tWO\n",
       "1    bAR\n",
       "dtype: object\n",
       "\n",
       "Using a compiled regex with flags\n",
       "\n",
       ">>> import re\n",
       ">>> regex_pat = re.compile(r'FUZ', flags=re.IGNORECASE)\n",
       ">>> pd.Series(['foo', 'fuz', np.nan]).str.replace(regex_pat, 'bar')\n",
       "0    foo\n",
       "1    bar\n",
       "2    NaN\n",
       "dtype: object\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;33m@\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_replace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mforbid_nonstring_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bytes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr_replace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m            \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s.str.replace??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "\u001b[1;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;34m\"\"\"Return the string obtained by replacing the leftmost\n",
       "    non-overlapping occurrences of the pattern in string by the\n",
       "    replacement repl.  repl can be either a string or a callable;\n",
       "    if a string, backslash escapes in it are processed.  If it is\n",
       "    a callable, it's passed the Match object and must return\n",
       "    a replacement string to be used.\"\"\"\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\re.py\n",
       "\u001b[1;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "re.sub??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用?P<....>表达式可以对子组命名调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3     ba*\n",
       "4     ca*\n",
       "5        \n",
       "6    <NA>\n",
       "7     BA*\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'(?P<one>[ABC])(?P<two>\\w+)',lambda x:x.group('two')[1:]+'*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 4), match='Aaba'>\n",
      "\n",
      "<re.Match object; span=(0, 4), match='Baca'>\n",
      "\n",
      "<re.Match object; span=(0, 4), match='CABA'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3        \n",
       "4        \n",
       "5        \n",
       "6    <NA>\n",
       "7        \n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.replace(r'(?P<one>[ABC])(?P<two>\\w+)',lambda x:print(x)) # lambda 中的 x 表示前边的正则表达式匹配到的子串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       A\n",
       "1       B\n",
       "2       C\n",
       "3    Aaba\n",
       "4    Baca\n",
       "5        \n",
       "6    <NA>\n",
       "7    CABA\n",
       "8     dog\n",
       "9     cat\n",
       "dtype: string"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       AFF123\n",
       "1       BFF123\n",
       "2       CFF123\n",
       "3    AabaFF123\n",
       "4    BacaFF123\n",
       "5        FF123\n",
       "6         <NA>\n",
       "7    CABAFF123\n",
       "8     dogFF123\n",
       "9     catFF123\n",
       "dtype: string"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss=s+'FF123'\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFF123 A FF123\n",
      "\n",
      "BFF123 B FF123\n",
      "\n",
      "CFF123 C FF123\n",
      "\n",
      "AabaFF123 A abaFF123\n",
      "\n",
      "BacaFF123 B acaFF123\n",
      "\n",
      "CABAFF123 C ABAFF123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0            \n",
       "1            \n",
       "2            \n",
       "3            \n",
       "4            \n",
       "5       FF123\n",
       "6        <NA>\n",
       "7            \n",
       "8    dogFF123\n",
       "9    catFF123\n",
       "dtype: string"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.str.replace(r'(?P<one>[ABC])(?P<two>\\w+)',lambda x:print(x.group(0),x.group(1),x.group(2))) # lambda 中的 x 表示前边的正则表达式匹配到的子串"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 关于str.replace的注意事项\n",
    "#### 首先，要明确str.replace和replace并不是一个东西：\n",
    "#### str.replace针对的是object类型或string类型，默认是以正则表达式为操作，目前暂时不支持DataFrame上使用\n",
    "#### replace针对的是任意类型的序列或数据框，如果要以正则表达式替换，需要设置regex=True，该方法通过字典可支持多列替换\n",
    "#### 但现在由于string类型的初步引入，用法上出现了一些问题，这些issue有望在以后的版本中修复\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （a）str.replace赋值参数不得为pd.NA\n",
    "#### 这听上去非常不合理，例如对满足某些正则条件的字符串替换为缺失值，直接更改为缺失值在当下版本就会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(['A','B'],dtype='string').str.replace(r'[A]',pd.NA) #报错 TypeError: repl must be a string or callable\n",
    "#pd.Series(['A','B'],dtype='O').str.replace(r'[A]',pd.NA) #报错 TypeError: repl must be a string or callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 此时，可以先转为object类型再转换回来，曲线救国："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    <NA>\n",
       "1       B\n",
       "dtype: string"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['A','B'],dtype='string').astype('O').replace(r'[A]',pd.NA,regex=True).astype('string')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 至于为什么不用replace函数的regex替换（但string类型replace的非正则替换是可以的），原因在下面一条"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （b）对于string类型Series，在使用replace函数时不能使用正则表达式替换\n",
    "#### 该bug现在还未修复"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A\n",
       "1    B\n",
       "dtype: string"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['A','B'],dtype='string').replace(r'[A]','C',regex=True)\n",
    "# dtype='string' 没有被替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    C\n",
       "1    B\n",
       "dtype: object"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['A','B'],dtype='O').replace(r'[A]','C',regex=True)\n",
    "# dtype='o' 时可以替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    C\n",
       "1    B\n",
       "dtype: object"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['A','B'],dtype='str').replace(r'[A]','C',regex=True)\n",
    "# dtype='str' 也可以被替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "dtype: int32"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于 int 类型\n",
    "pd.Series([1,2,3],dtype='int').replace(r'[1]','C',regex=True)\n",
    "# 直接替换并不成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    C\n",
       "1    2\n",
       "2    3\n",
       "dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2,3],dtype='int').astype('str').replace(r'[1]','C',regex=True)\n",
    "# 转为 str 后可以成功替换."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    2\n",
       "2    3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1,2,3],dtype='int').astype('O').replace(r'[1]','C',regex=True)\n",
    "# 转为 object 后, 并未成功替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    C2\n",
       "1    23\n",
       "2    34\n",
       "dtype: object"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([12,23,34],dtype='int').astype('str').replace(r'[1]','C',regex=True)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    C2\n",
       "1    23\n",
       "2    34\n",
       "dtype: object"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([12,23,34],dtype='int').astype('str').replace(r'1','C',regex=True)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9293\n",
       "1      23\n",
       "2      34\n",
       "dtype: int32"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series([1213,23,34],dtype='int').astype('str').replace(r'1','9',regex=True).astype('int')\n",
    "# 可以将某个数字替换为其他数字, 但需要用 astype 再转回数字."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （c）string类型序列如果存在缺失值，不能使用replace替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       B\n",
       "1    <NA>\n",
       "dtype: string"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['A',np.nan],dtype='string').replace('A','B') #报错--没有报错啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      B\n",
       "1    NaN\n",
       "dtype: object"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['A',np.nan],dtype='str').replace('A','B') # str 类型也没有报错\n",
    "pd.Series(['A',np.nan],dtype='O').replace('A','B') # O 类型也没有报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       B\n",
       "1    <NA>\n",
       "dtype: string"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['A',np.nan],dtype='string').str.replace('A','B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 综上，概况的说，除非需要赋值元素为缺失值（转为object再转回来），否则请使用str.replace方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、子串匹配与提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. str.extract方法\n",
    "从单元格里提取符合传入的正则表达式的子串, 也可以用来将一列扩张为多列."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （a）常见用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10-87\n",
       "1    10-88\n",
       "2    10-89\n",
       "dtype: string"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0   1\n",
       "0  10  87\n",
       "1  10  88\n",
       "2  10  89"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(pd.Series(['10-87', '10-88', '10-89'],dtype=\"string\"))\n",
    "pd.Series(['10-87', '10-88', '10-89'],dtype=\"string\").str.extract(r'([\\d]{2})-([\\d]{2})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Extract capture groups in the regex `pat` as columns in a DataFrame.\n",
       "\n",
       "For each subject string in the Series, extract groups from the\n",
       "first match of regular expression `pat`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "pat : str\n",
       "    Regular expression pattern with capturing groups.\n",
       "flags : int, default 0 (no flags)\n",
       "    Flags from the ``re`` module, e.g. ``re.IGNORECASE``, that\n",
       "    modify regular expression matching for things like case,\n",
       "    spaces, etc. For more details, see :mod:`re`.\n",
       "expand : bool, default True\n",
       "    If True, return DataFrame with one column per capture group.\n",
       "    If False, return a Series/Index if there is one capture group\n",
       "    or DataFrame if there are multiple capture groups.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame or Series or Index\n",
       "    A DataFrame with one row for each subject string, and one\n",
       "    column for each group. Any capture group names in regular\n",
       "    expression pat will be used for column names; otherwise\n",
       "    capture group numbers will be used. The dtype of each result\n",
       "    column is always object, even when no match is found. If\n",
       "    ``expand=False`` and pat has only one capture group, then\n",
       "    return a Series (if subject is a Series) or Index (if subject\n",
       "    is an Index).\n",
       "\n",
       "See Also\n",
       "--------\n",
       "extractall : Returns all matches (not just the first match).\n",
       "\n",
       "Examples\n",
       "--------\n",
       "A pattern with two groups will return a DataFrame with two columns.\n",
       "Non-matches will be NaN.\n",
       "\n",
       ">>> s = pd.Series(['a1', 'b2', 'c3'])\n",
       ">>> s.str.extract(r'([ab])(\\d)')\n",
       "     0    1\n",
       "0    a    1\n",
       "1    b    2\n",
       "2  NaN  NaN\n",
       "\n",
       "A pattern may contain optional groups.\n",
       "\n",
       ">>> s.str.extract(r'([ab])?(\\d)')\n",
       "     0  1\n",
       "0    a  1\n",
       "1    b  2\n",
       "2  NaN  3\n",
       "\n",
       "Named groups will become column names in the result.\n",
       "\n",
       ">>> s.str.extract(r'(?P<letter>[ab])(?P<digit>\\d)')\n",
       "  letter digit\n",
       "0      a     1\n",
       "1      b     2\n",
       "2    NaN   NaN\n",
       "\n",
       "A pattern with one group will return a DataFrame with one column\n",
       "if expand=True.\n",
       "\n",
       ">>> s.str.extract(r'[ab](\\d)', expand=True)\n",
       "     0\n",
       "0    1\n",
       "1    2\n",
       "2  NaN\n",
       "\n",
       "A pattern with one group will return a Series if expand=False.\n",
       "\n",
       ">>> s.str.extract(r'[ab](\\d)', expand=False)\n",
       "0      1\n",
       "1      2\n",
       "2    NaN\n",
       "dtype: object\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;33m@\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_extract\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mforbid_nonstring_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bytes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mstr_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s.str.extract??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通过在正则表达式中使用子组名来为拆分后的列增加列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  start   end\n",
       "0    10    87\n",
       "1    10    88\n",
       "2  <NA>  <NA>"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['10-87', '10-88', '-89'],dtype=\"string\").str.extract(r'(?P<start>[\\d]{2})-(?P<end>[\\d]{2})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用?正则标记选择部分提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_1 name_2\n",
       "0     10     87\n",
       "1     10     88\n",
       "2     10   <NA>\n",
       "3   <NA>     89"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['10-87', '10-88', '10-',  '-89'],dtype=\"string\").str.extract(r'(?P<name_1>[\\d]{2})?-(?P<name_2>[\\d]{2})?')\n",
    "# 第一组后的?表示该组是可能缺失的,第一组没匹配到的也能够正确提取第二组.\n",
    "# 在第二组后加上?则第二组匹配不到的也会正确提取第一组."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_1</th>\n",
       "      <th>name_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name_1 name_2\n",
       "0     10     87\n",
       "1     10     88\n",
       "2     10   <NA>"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['10-87', '10-88', '10-'],dtype=\"string\").str.extract(r'(?P<name_1>[\\d]{2})-(?P<name_2>[\\d]{2})?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （b）expand参数（默认为True）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于一个子组的Series，如果expand设置为False，则返回Series，若大于一个子组，则expand参数无效，全部返回DataFrame\n",
    "#### 对于一个子组的Index，如果expand设置为False，则返回提取后的Index，若大于一个子组且expand为False，报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A11    a1\n",
       "B22    b2\n",
       "C33    c3\n",
       "dtype: string"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([\"a1\", \"b2\", \"c3\"], [\"A11\", \"B22\", \"C33\"], dtype=\"string\")\n",
    "s#.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A11</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B22</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C33</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "A11  a\n",
       "B22  b\n",
       "C33  c"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.extract(r'([\\w])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A11    a\n",
       "B22    b\n",
       "C33    c\n",
       "dtype: string"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.extract(r'([\\w])',expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A11</th>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B22</th>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C33</th>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "A11  a\n",
       "B22  b\n",
       "C33  c"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.extract(r'([\\w])',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A11</th>\n",
       "      <td>m</td>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B22</th>\n",
       "      <td>m</td>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C33</th>\n",
       "      <td>m</td>\n",
       "      <td>c</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1  2\n",
       "A11  m  a  1\n",
       "B22  m  b  2\n",
       "C33  m  c  3"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('m'+s).str.extract(r'([\\w])([\\w])([\\w])',expand=False) # 大于1个组, 因此 expand=False 参数就没法起作用了--这和split方法中需要先指定expand, n才能起作用是不一致的."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  A\n",
       "1  B\n",
       "2  C"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.index.str.extract(r'([\\w])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['A', 'B', 'C'], dtype='object')"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.index.str.extract(r'([\\w])',expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  A  1\n",
       "1  B  2\n",
       "2  C  3"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 提取两个子组类型,都提取导了,因此自然会分成两列\n",
    "s.index.str.extract(r'([\\w])([\\d])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3\n",
       "0  NaN  NaN  NaN  NaN\n",
       "1  NaN  NaN  NaN  NaN\n",
       "2  NaN  NaN  NaN  NaN"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s.index.str.extract(r'([\\w])([\\d])([\\w])') # 第三个子组会继续沿用第二个\n",
    "# s.index.str.extract(r'([\\w])([\\d])([\\d])') # 同上\n",
    "s.index.str.extract(r'([\\w])([\\d])([\\w])([\\w])') #再增加一个就都变成 NaN 了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2\n",
       "0  A  1  1\n",
       "1  B  2  2\n",
       "2  C  3  3"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.index.str.extract(r'([\\w])([\\d])([\\d])')#([\\d])') #三个没问题, 再增加一个就都变成 NaN 了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1\n",
       "0  A  1\n",
       "1  B  2\n",
       "2  C  3"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 索引也支持提取\n",
    "#s.index.str.extract(r'([\\w])([\\d])',expand=False) #报错 ValueError: only one regex group is supported with Index\n",
    "s.index.str.extract(r'([\\w])([\\d])',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A11</th>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B22</th>\n",
       "      <td>B</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C33</th>\n",
       "      <td>C</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0  1\n",
       "A11  A  1\n",
       "B22  B  2\n",
       "C33  C  3"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于索引, 也可以先使用 to_series 函数(注意s小写)转为Series再提取--这在从复合型单层索引构造层次索引时会用到.\n",
    "s.index.to_series().str.extract(r'([\\w])([\\d])',expand=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. str.extractall方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 与extract只匹配第一个符合条件的表达式不同，extractall会找出所有符合条件的字符串，并建立多级索引（即使只找到一个）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    a1a2\n",
       "B      b1\n",
       "C      c1\n",
       "dtype: string"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter digit\n",
       "A      a     1\n",
       "B      b     1\n",
       "C      c     1"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"],dtype=\"string\")\n",
    "display(s)\n",
    "two_groups = '(?P<letter>[a-z])(?P<digit>[0-9])'\n",
    "s.str.extract(two_groups, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">A</th>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th>0</th>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        letter digit\n",
       "  match             \n",
       "A 0          a     1\n",
       "  1          a     2\n",
       "B 0          b     1\n",
       "C 0          c     1"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.str.extractall(two_groups)\n",
    "# 将匹配到的多个符合条件的子串分为了多行 -- 这个处理方式非常好, 因为如果分为多列的话会导致大量的单元格是缺失值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>match</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <th>0</th>\n",
       "      <td>c</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        letter digit\n",
       "  match             \n",
       "A 0          a     1\n",
       "B 0          b     1\n",
       "C 0          c     1"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s['A']='a1'\n",
    "s.str.extractall(two_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "For each subject string in the Series, extract groups from all\n",
       "matches of regular expression pat. When each subject string in the\n",
       "Series has exactly one match, extractall(pat).xs(0, level='match')\n",
       "is the same as extract(pat).\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "pat : str\n",
       "    Regular expression pattern with capturing groups.\n",
       "flags : int, default 0 (no flags)\n",
       "    A ``re`` module flag, for example ``re.IGNORECASE``. These allow\n",
       "    to modify regular expression matching for things like case, spaces,\n",
       "    etc. Multiple flags can be combined with the bitwise OR operator,\n",
       "    for example ``re.IGNORECASE | re.MULTILINE``.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "DataFrame\n",
       "    A ``DataFrame`` with one row for each match, and one column for each\n",
       "    group. Its rows have a ``MultiIndex`` with first levels that come from\n",
       "    the subject ``Series``. The last level is named 'match' and indexes the\n",
       "    matches in each item of the ``Series``. Any capture group names in\n",
       "    regular expression pat will be used for column names; otherwise capture\n",
       "    group numbers will be used.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "extract : Returns first match only (not all matches).\n",
       "\n",
       "Examples\n",
       "--------\n",
       "A pattern with one group will return a DataFrame with one column.\n",
       "Indices with no matches will not appear in the result.\n",
       "\n",
       ">>> s = pd.Series([\"a1a2\", \"b1\", \"c1\"], index=[\"A\", \"B\", \"C\"])\n",
       ">>> s.str.extractall(r\"[ab](\\d)\")\n",
       "         0\n",
       "  match\n",
       "A 0      1\n",
       "  1      2\n",
       "B 0      1\n",
       "\n",
       "Capture group names are used for column names of the result.\n",
       "\n",
       ">>> s.str.extractall(r\"[ab](?P<digit>\\d)\")\n",
       "        digit\n",
       "  match\n",
       "A 0         1\n",
       "  1         2\n",
       "B 0         1\n",
       "\n",
       "A pattern with two groups will return a DataFrame with two columns.\n",
       "\n",
       ">>> s.str.extractall(r\"(?P<letter>[ab])(?P<digit>\\d)\")\n",
       "        letter digit\n",
       "  match\n",
       "A 0          a     1\n",
       "  1          a     2\n",
       "B 0          b     1\n",
       "\n",
       "Optional groups that do not match are NaN in the result.\n",
       "\n",
       ">>> s.str.extractall(r\"(?P<letter>[ab])?(?P<digit>\\d)\")\n",
       "        letter digit\n",
       "  match\n",
       "A 0          a     1\n",
       "  1          a     2\n",
       "B 0          b     1\n",
       "C 0        NaN     1\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;33m@\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_extractall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mforbid_nonstring_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bytes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mextractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mstr_extractall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\programdata\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s.str.extractall??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如果想查看第i层匹配，可使用xs方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>letter</th>\n",
       "      <th>digit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>b</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C</th>\n",
       "      <td>c</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  letter digit\n",
       "A      a     2\n",
       "B      b     2\n",
       "C      c     2"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = pd.Series([\"a1a2\", \"b1b2\", \"c1c2\"], index=[\"A\", \"B\", \"C\"],dtype=\"string\")\n",
    "s.str.extractall(two_groups).xs(1,level='match')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. str.contains和str.match\n",
    "#### 前者的作用为检测是否包含某种正则模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     <NA>\n",
       "2     True\n",
       "3     True\n",
       "4     True\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1', None, '3a', '3b', '03c'], dtype=\"string\").str.contains(r'[0-9][a-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     <NA>\n",
       "2    False\n",
       "3     True\n",
       "4    False\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1', None, '3a3', '3b', '03c'], dtype=\"string\").str.contains(r'^[0-9][a-z]$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可选参数为na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2     True\n",
       "3    False\n",
       "4    False\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1', None, '3a', '3b', '03c'], dtype=\"string\").str.contains('a', na=True)#False)\n",
    "# na=False参数会把缺失值的返回值设置为False,反之则会设置为True -- 可以根据业务需求设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### str.match与其区别在于，match依赖于python的re.match，检测内容为是否从头开始包含该正则模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2     True\n",
       "3     True\n",
       "4    False\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1', None, '3a_', '3b', '03c'], dtype=\"string\").str.match(r'[0-9][a-z]',na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1    False\n",
       "2    False\n",
       "3     True\n",
       "4    False\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1', None, '_3a', '3b', '03c'], dtype=\"string\").str.match(r'[0-9][a-z]',na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3     True\n",
       "4     True\n",
       "5    False\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "astr=\"\"\"begin--\n",
    "1this is a string,\n",
    "2with 2 lines,\n",
    "3or 3, maybe.\n",
    "\"\"\"\n",
    "pd.Series([astr,'1'+astr, None, '3a_', '3b', '03c'], dtype=\"string\").str.match(r'[0-9][a-z]',na=False)\n",
    "pd.Series([astr,'1'+astr, None, '3a_', '3b', '03c'], dtype=\"string\").str.match(r'[0-9][a-z]',na=False,flags=re.M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -1\n",
       "1    <NA>\n",
       "2      -1\n",
       "3       0\n",
       "4      -1\n",
       "dtype: Int64"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 补充说明: 字符串的 find 方法并不支持正则表达式\n",
    "pd.Series(['1', None, '3a_', '3b', '03c'], dtype=\"string\").str.find(r'[0-9][a-z]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      -1\n",
       "1    <NA>\n",
       "2       0\n",
       "3      -1\n",
       "4      -1\n",
       "dtype: Int64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1', None, '3a_', '3b', '03c'], dtype=\"string\").str.find('3a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、常用字符串方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 过滤型方法\n",
    "#### （a）str.strip\n",
    "#### 常用于过滤空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['space1', 'space2', 'space3'], dtype='object')"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(list('abc'),index=[' space1  ','space2  ','  space3'],dtype=\"string\").index.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （b）str.lower和str.upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    a\n",
       "dtype: string"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series('A',dtype=\"string\").str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    A\n",
       "dtype: string"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series('a',dtype=\"string\").str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__frozen', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_doc_args', '_freeze', '_get_series_list', '_inferred_dtype', '_is_categorical', '_is_string', '_make_accessor', '_orig', '_parent', '_validate', '_wrap_result', 'capitalize', 'casefold', 'cat', 'center', 'contains', 'count', 'decode', 'encode', 'endswith', 'extract', 'extractall', 'find', 'findall', 'get', 'get_dummies', 'index', 'isalnum', 'isalpha', 'isdecimal', 'isdigit', 'islower', 'isnumeric', 'isspace', 'istitle', 'isupper', 'join', 'len', 'ljust', 'lower', 'lstrip', 'match', 'normalize', 'pad', 'partition', 'repeat', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'slice', 'slice_replace', 'split', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'wrap', 'zfill']\n"
     ]
    }
   ],
   "source": [
    "print(dir(pd.Series('hey joy, How\\'re you doing?',dtype=\"string\").str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Pad a numeric string with zeros on the left, to fill a field of the given width.\n",
       "\n",
       "The string is never truncated.\n",
       "\u001b[1;31mType:\u001b[0m      method_descriptor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series('hey joy, How\\'re you doing?',dtype=\"string\").str.zfill??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （c）str.swapcase, str.title和str.capitalize\n",
    "#### 分别表示交换字母大小写,每个单词首字母大写和整个字符串的首字母大写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ABcd\n",
       "dtype: string"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series('abCD',dtype=\"string\").str.swapcase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Abcd\n",
       "dtype: string"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series('abCD',dtype=\"string\").str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hey Joy, How Are You Doing?\n",
       "dtype: string"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 将每个单词的首字母大写,其他字母小写\n",
    "pd.Series('hey joy, How are you doing?',dtype=\"string\").str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hey joy, how're you doing?\n",
       "dtype: string"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 只将第一个单词的首字母大写,其他的字母小写\n",
    "pd.Series('hey joy, How\\'re you doing?',dtype=\"string\").str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. isnumeric方法\n",
    "#### 检查每一位是否都是数字，请问如何判断是否是数值？（问题二）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4     <NA>\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1.2','1','-0.3','a',np.nan],dtype=\"string\").str.isnumeric()\n",
    "# 对于负数和浮点数给出了错误的答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4     <NA>\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1.2','1','-0.3','a',np.nan],dtype=\"string\").str.isdecimal()\n",
    "# 同样错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    False\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4     <NA>\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(['1.2','1','-0.3','a',np.nan],dtype=\"string\").str.isdigit()\n",
    "# 错误"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalnum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Return True if the string is an alpha-numeric string, False otherwise.\n",
       "\n",
       "A string is alpha-numeric if all characters in the string are alpha-numeric and\n",
       "there is at least one character in the string.\n",
       "\u001b[1;31mType:\u001b[0m      method_descriptor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(['1.2','1','-0.3','a',np.nan],dtype=\"string\").str.isalnum??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4    False\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "8    False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先判断是否是缺失值, 如果不是, 再使用正则表达式匹配浮点数模式: 以零或一个负号(-)开头,接下来至少包含一个数字,最后以零或一个\"小数点加零或多个数字\"结尾\n",
    "pd.Series(['42','1.223','1.','-0.3456','a1','1a',np.nan,None,pd.NA],dtype=\"string\").apply(lambda x:True if x is not pd.NA and re.match('^-?\\d+(\\.\\d*)?$',x) else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、问题与练习\n",
    "### 1. 问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【问题一】 str对象方法和df/Series对象方法有什么区别？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "针对的对象不同. \n",
    "* str 对象方法是将单元格内的内容当作字符串,来执行针对字符串的各种操作.  \n",
    "* 而 pandas 对象 DataFrame 和 Series 的对象方法,操作的多是 DF 的列的所有元素(如 sum, min 等聚合函数),或者是单元格, 并且是对单元格内的内容整体进行操作.\n",
    "    * 例如, str.replace可以把单元格内的字符串的一部分替换掉,但是df.replace是把整个单元格内的内容替换掉(并且要数据类型一致)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astr\t col\t d_\t day2c\t df\t df1\t df2\t dfs\t l1\t \n",
      "month2c\t np\t num2c\t num_to_c\t num_to_c1\t num_to_char\t pd\t re\t s\t \n",
      "s1\t s2\t s_\t s_string\t ss\t two_groups\t \n"
     ]
    }
   ],
   "source": [
    "who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_1</th>\n",
       "      <th>C_2</th>\n",
       "      <th>C_3</th>\n",
       "      <th>C_4</th>\n",
       "      <th>C_5</th>\n",
       "      <th>C_6</th>\n",
       "      <th>C_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  C_1 C_2 C_3 C_4 C_5 C_6 C_7\n",
       "a   5   2  18  10   2  14   2\n",
       "b  15  17   6  12   2   4  10\n",
       "c   1  18  12  17   8   8   9"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C_1</th>\n",
       "      <th>C_2</th>\n",
       "      <th>C_3</th>\n",
       "      <th>C_4</th>\n",
       "      <th>C_5</th>\n",
       "      <th>C_6</th>\n",
       "      <th>C_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>5的平方是25</td>\n",
       "      <td>2的平方是4</td>\n",
       "      <td>18的平方是324</td>\n",
       "      <td>10的平方是100</td>\n",
       "      <td>2的平方是4</td>\n",
       "      <td>14的平方是196</td>\n",
       "      <td>2的平方是4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>15的平方是225</td>\n",
       "      <td>17的平方是289</td>\n",
       "      <td>6的平方是36</td>\n",
       "      <td>12的平方是144</td>\n",
       "      <td>2的平方是4</td>\n",
       "      <td>4的平方是16</td>\n",
       "      <td>10的平方是100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>1的平方是1</td>\n",
       "      <td>18的平方是324</td>\n",
       "      <td>12的平方是144</td>\n",
       "      <td>17的平方是289</td>\n",
       "      <td>8的平方是64</td>\n",
       "      <td>8的平方是64</td>\n",
       "      <td>9的平方是81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         C_1        C_2        C_3        C_4      C_5        C_6        C_7\n",
       "a    5的平方是25     2的平方是4  18的平方是324  10的平方是100   2的平方是4  14的平方是196     2的平方是4\n",
       "b  15的平方是225  17的平方是289    6的平方是36  12的平方是144   2的平方是4    4的平方是16  10的平方是100\n",
       "c     1的平方是1  18的平方是324  12的平方是144  17的平方是289  8的平方是64    8的平方是64    9的平方是81"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上述df中的单元格内的对象是str类型.\n",
    "df.applymap(lambda x: x+f'的平方是{int(x)**2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-06-26 19:09:09.338082\n",
       "0    2020-06-26 19:09:09.338082\n",
       "0    2020-06-26 19:09:09.338082\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#　再举一个例子\n",
    "from datetime import datetime\n",
    "st=pd.Series([datetime.now()]) # datetime类型\n",
    "st.values[0]\n",
    "sts = pd.concat([st,st.astype('str'),st.astype('string')])\n",
    "display(sts) # 字面上看来, 三个元素完全一致,但实际上三个元素的数据类型不同 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-06-26 19:09:09.338082\n",
       "0                            --\n",
       "0                            --\n",
       "dtype: object"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.replace('2020-06-26 19:09:09.338082','--') # 于是使用Series的replace,就会只替换字符串,而略过时间类型的第一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                            --\n",
       "0    2020-06-26 19:09:09.338082\n",
       "0    2020-06-26 19:09:09.338082\n",
       "dtype: object"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 但是如果是用时间去替换..\n",
    "from dateutil.parser import parse\n",
    "sts.replace(parse('2020-06-26 19:09:09.338082'),'--') \n",
    "# 就只会替换第一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NaN\n",
       "0     --\n",
       "0     --\n",
       "dtype: object"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.str.replace('2020-06-26 19:09:09.338082','--') # 但是使用 str.replace, 奇怪的事情发生了.... 第一个元素被替换为了空"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           NaN\n",
       "0    2020-06-26 19:09:09.338082\n",
       "0    2020-06-26 19:09:09.338082\n",
       "dtype: object"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.str.replace('--','--')\n",
    "# 是因为str方法的原因--这算是个bug吗?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2020-06-26 19:09:09.338082\n",
       "0    2020-06-26 19:09:09.338082\n",
       "0    2020-06-26 19:09:09.338082\n",
       "dtype: object"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.astype('str').str.replace('--','--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    --\n",
       "0    --\n",
       "0    --\n",
       "dtype: object"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这种情况下, 先转一下数据类型, 再操作就能得到期望的结果了.\n",
    "sts.astype('str').str.replace('2020-06-26 19:09:09.338082','--') # 但是使用 str.replace, 奇怪的事情发生了...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【问题二】 给出一列string类型，如何判断单元格是否是数值型数据？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         0\n",
       "2    -2.123\n",
       "3       0.2\n",
       "4      -0.3\n",
       "5        1a\n",
       "6        a1\n",
       "7         a\n",
       "8      <NA>\n",
       "dtype: string"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_string = pd.Series(['1','0','-2.123','0.2','-0.3','1a','a1','a',np.nan],dtype=\"string\")\n",
    "display(s_string)# .str.extract(r'\\d')  #ValueError: pattern contains no capture groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2    False\n",
       "3    False\n",
       "4    False\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "8     <NA>\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_string.str.isdecimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     True\n",
       "1     True\n",
       "2     True\n",
       "3     True\n",
       "4     True\n",
       "5    False\n",
       "6    False\n",
       "7    False\n",
       "8     <NA>\n",
       "dtype: boolean"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用正则表达式匹配\n",
    "s_string.str.contains(r'^-?(\\d+)(\\.\\d+)?$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【问题三】 rsplit方法的作用是什么？它在什么场合下适用？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Split strings around given separator/delimiter.\n",
       "\n",
       "Splits the string in the Series/Index from the end,\n",
       "at the specified delimiter string. Equivalent to :meth:`str.rsplit`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "pat : str, optional\n",
       "    String or regular expression to split on.\n",
       "    If not specified, split on whitespace.\n",
       "n : int, default -1 (all)\n",
       "    Limit number of splits in output.\n",
       "    ``None``, 0 and -1 will be interpreted as return all splits.\n",
       "expand : bool, default False\n",
       "    Expand the splitted strings into separate columns.\n",
       "\n",
       "    * If ``True``, return DataFrame/MultiIndex expanding dimensionality.\n",
       "    * If ``False``, return Series/Index, containing lists of strings.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "Series, Index, DataFrame or MultiIndex\n",
       "    Type matches caller unless ``expand=True`` (see Notes).\n",
       "\n",
       "See Also\n",
       "--------\n",
       "Series.str.split : Split strings around given separator/delimiter.\n",
       "Series.str.rsplit : Splits string around given separator/delimiter,\n",
       "    starting from the right.\n",
       "Series.str.join : Join lists contained as elements in the Series/Index\n",
       "    with passed delimiter.\n",
       "str.split : Standard library version for split.\n",
       "str.rsplit : Standard library version for rsplit.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The handling of the `n` keyword depends on the number of found splits:\n",
       "\n",
       "- If found splits > `n`,  make first `n` splits only\n",
       "- If found splits <= `n`, make all splits\n",
       "- If for a certain row the number of found splits < `n`,\n",
       "  append `None` for padding up to `n` if ``expand=True``\n",
       "\n",
       "If using ``expand=True``, Series and Index callers return DataFrame and\n",
       "MultiIndex objects, respectively.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> s = pd.Series([\"this is a regular sentence\",\n",
       "...                \"https://docs.python.org/3/tutorial/index.html\",\n",
       "...                np.nan])\n",
       "0                       this is a regular sentence\n",
       "1    https://docs.python.org/3/tutorial/index.html\n",
       "2                                              NaN\n",
       "dtype: object\n",
       "\n",
       "In the default setting, the string is split by whitespace.\n",
       "\n",
       ">>> s.str.split()\n",
       "0                   [this, is, a, regular, sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       "Without the `n` parameter, the outputs of `rsplit` and `split`\n",
       "are identical.\n",
       "\n",
       ">>> s.str.rsplit()\n",
       "0                   [this, is, a, regular, sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       "The `n` parameter can be used to limit the number of splits on the\n",
       "delimiter. The outputs of `split` and `rsplit` are different.\n",
       "\n",
       ">>> s.str.split(n=2)\n",
       "0                     [this, is, a regular sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       ">>> s.str.rsplit(n=2)\n",
       "0                     [this is a, regular, sentence]\n",
       "1    [https://docs.python.org/3/tutorial/index.html]\n",
       "2                                                NaN\n",
       "dtype: object\n",
       "\n",
       "The `pat` parameter can be used to split by other characters.\n",
       "\n",
       ">>> s.str.split(pat = \"/\")\n",
       "0                         [this is a regular sentence]\n",
       "1    [https:, , docs.python.org, 3, tutorial, index...\n",
       "2                                                  NaN\n",
       "dtype: object\n",
       "\n",
       "When using ``expand=True``, the split elements will expand out into\n",
       "separate columns. If NaN is present, it is propagated throughout\n",
       "the columns during the split.\n",
       "\n",
       ">>> s.str.split(expand=True)\n",
       "                                               0     1     2        3\n",
       "0                                           this    is     a  regular\n",
       "1  https://docs.python.org/3/tutorial/index.html  None  None     None\n",
       "2                                            NaN   NaN   NaN      NaN \\\n",
       "             4\n",
       "0     sentence\n",
       "1         None\n",
       "2          NaN\n",
       "\n",
       "For slightly more complex use cases like splitting the html document name\n",
       "from a url, a combination of parameter settings can be used.\n",
       "\n",
       ">>> s.str.rsplit(\"/\", n=1, expand=True)\n",
       "                                    0           1\n",
       "0          this is a regular sentence        None\n",
       "1  https://docs.python.org/3/tutorial  index.html\n",
       "2                                 NaN         NaN\n",
       "\n",
       "Remember to escape special characters when explicitly using regular\n",
       "expressions.\n",
       "\n",
       ">>> s = pd.Series([\"1+1=2\"])\n",
       "\n",
       ">>> s.str.split(r\"\\+|=\", expand=True)\n",
       "     0    1    2\n",
       "0    1    1    2\n",
       "\u001b[1;31mSource:\u001b[0m   \n",
       "    \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"str_split\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"side\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"end\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"method\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"rsplit\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m@\u001b[0m\u001b[0mforbid_nonstring_types\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bytes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;32mdef\u001b[0m \u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr_rsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n",
       "\u001b[0m        \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrap_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns_string\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\programdata\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s.str.rsplit??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【问题四】 在本章的第二到第四节分别介绍了字符串类型的5类操作，请思考它们各自应用于什么场景？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 拆分与拼接\n",
    "    * str.split--拆分\n",
    "    * str.cat--拼接\n",
    "* 替换\n",
    "* 子串匹配与提取\n",
    "    * extract--从pandas对象的一列中提取信息成为新的列,例如将手机号码,邮件从文本中提取出来.\n",
    "    * exractall--提取所有能够匹配到的子串,并建立多级索引\n",
    "    * contain--判断包含某个特定格式的子串\n",
    "    * match--字符串以某个特定子串开头\n",
    "* 过滤与判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]姊佸啗.鍩轰簬鏁板瓧浜烘枃鐨勬櫤鎱у浘涔﹂�嗗缓璁捐矾寰勬帰绱�[J].鎵嶆櫤,2020(18):241.\n",
      "\n",
      "[2]寮犲┓.鍙版咕鍦板尯鏁板瓧浜烘枃鐮旂┒鐗硅壊涓庡疄璺靛惎绀篬J].楂樻牎鍥句功棣嗗伐浣�,2020,40(04):41-45.\n",
      "\n",
      "[3]鏉ㄤ匠棰�,閭撶拹鑺�,璁搁懌.瑙呮睙鍗椾匠棣旓細澶氭簮鍙や粖鏂囨湰鏁版嵁铻嶅悎鐨勬勃涓婇ギ椋熷浘璋辨瀯寤篬J/OL].鍥句功棣嗚�哄潧:1-9[2020-06-26].http://kns.cnki.net/kcms/detail/44.1306.G2.20200616.1533.002.html.\n",
      "\n",
      "[4]濮氬暩鍗�,璐烘櫒鑺�,寰愬瓭濞�,鍏ㄧ煶宄�.闈㈠悜鏁板瓧浜烘枃鐨勫浘涔﹂�嗕紬鍖呭钩鍙版瀯寤虹爺绌垛�斺�斾互涓婃捣鍥句功棣嗗巻鍙叉枃鐚�浼楀寘骞冲彴涓轰緥[J].鍥句功棣嗘潅蹇�,2020,39(06):105-112.\n",
      "\n",
      "[5]鑼冩�傜孩,璧电函娲�.鍩轰簬鐭ヨ瘑鍥捐氨鐨勫彜绫嶆暟瀛楀寲鐮旂┒鍓嶆部鐑�鐐瑰強婕斿寲瓒嬪娍鍒嗘瀽[J].鍑虹増骞胯��,2020(11):85-87.\n",
      "\n",
      "[6]寰愭櫒椋�,鍙舵捣褰�,鍖呭钩.鍩轰簬娣卞害瀛︿範鐨勬柟蹇楃墿浜ц祫鏂欏疄浣撹嚜鍔ㄨ瘑鍒�妯″瀷鏋勫缓鐮旂┒[J/OL].鏁版嵁鍒嗘瀽涓庣煡璇嗗彂鐜�:1-17[2020-06-26].http://kns.cnki.net/kcms/detail/10.1478.G2.20200609.1040.006.html.\n",
      "\n",
      "[7]鍒樻槺褰�,鍚存枌,鐧藉┓.鍙よ瘲璇嶅浘璋辩殑鏋勫缓鍙婂垎鏋愮爺绌禰J].璁＄畻鏈虹爺绌朵笌鍙戝睍,2020,57(06):1252-1268.\n",
      "\n",
      "[8]宸寸壒,閭撳悰.鏁板瓧浜烘枃瑙嗗煙涓嬫垜鍥介珮鏍″彛杩版牎鍙叉。妗堝缓璁捐矾寰勬�濊�僛J].鍏板彴涓栫晫,2020(06):31-34.\n",
      "\n",
      "[9]鏉庢澗娑�,寮犲崼涓�,宸﹀��.鏁板瓧浜烘枃瑙嗚�掍笅浜烘枃鐮旂┒鑰呭埄鐢ㄦ。妗堥�嗚棌鐨勮�屼负涓庢縺鍔辩爺绌禰J].灞辫タ妗ｆ��,2020(03):77-97.\n",
      "\n",
      "[10]濮滆偛褰�,鏉庨泤鑼�.鍩轰簬鏁板瓧浜烘枃瑙嗚�掔殑鈥滄儏鎰熲�斺�旀椂绌衡�濇ā鍨嬫帰鏋怺J].鍐滀笟鍥句功鎯呮姤瀛︽姤,2020,32(06):23-33.\n",
      "\n",
      "[11]姊佺户鏂�,姹熷窛,鐜嬩笢娉�.鍩轰簬澶氱壒寰佽瀺鍚堢殑鍏堢Е鍏哥睄姹夎嫳鍙ュ瓙瀵归綈鐮旂┒[J/OL].鏁版嵁鍒嗘瀽涓庣煡璇嗗彂鐜�:1-13[2020-06-26].http://kns.cnki.net/kcms/detail/10.1478.G2.20200603.0948.002.html.\n",
      "\n",
      "[12]鍗�涓逛腹,鑱備簯闇�.鏁板瓧浜烘枃瑙嗚�掍笅鍦版柟鐗硅壊妗ｆ�堣祫婧愬紑鍙戣矾寰刐J/OL].灞辫タ妗ｆ��:1-6[2020-06-26].http://kns.cnki.net/kcms/detail/14.1162.G2.20200603.1018.007.html.\n",
      "\n",
      "[13]Tanja Wissik,Jennifer Edmond,Frank Fischer,Franciska de Jong,Stefania Scagliola,Andrea Scharnhorst,Hendrik Schmeer,Walter Scholger,Leon Wessels,閮戠倻妤�,鑲栭箯.鍥介檯瑙嗛噹涓嬬殑鏁板瓧浜烘枃鏁欒偛鈥斺�斿熀纭�璁炬柦瑙嗚�掔殑绀惧尯椹卞姩鍨嬫暟瀛椾汉鏂囪�剧▼鐧昏�颁腑蹇僛J].鍥句功棣嗚�哄潧,2020,40(06):1-27.\n",
      "\n",
      "[14]Marco Fiorucci,Marina Khoroshiltseva,Massimiliano Pontil,Arianna Traviglia,Alessio Del Bue,Stuart James. Machine Learning for Cultural Heritage: A Survey[J]. Elsevier B.V.,2020,133.\n",
      "\n",
      "[15]鍞愮嚂,鍒樺皬姒�,鏉庡仴.鏅烘収鍥句功棣嗙┖闂村啀閫犱笌鏁板瓧浜烘枃鏈嶅姟鍒涙柊鐮旂┒[J].鍥句功棣�,2020(05):74-80.\n",
      "\n",
      "[16]. Information Technology - Information Sciences; Recent Findings from Zhengzhou University Has Provided New Information about Information Sciences (Cross-national digital humanities research collaborations: structure, patterns and themes)[J]. Computer Technology Journal,2020.\n",
      "\n",
      "[17]鏈辨�濊嫅,鍗㈢珷骞�.缇庡浗楂樻牎鍥句功棣嗘暟瀛椾汉鏂囦腑蹇冪綉绔欏缓璁惧�规垜鍥界殑鍚�绀篬J].鍥句功鎯呮姤鐮旂┒,2020,13(02):47-54.\n",
      "\n",
      "[18]. Information Technology - Data Systems; Findings from National Autonomous University of Mexico (UNAM) Yields New Data on Data Systems (Practical Non-monotonic Knowledge-base System for Un-regimented Domains: a Case-study In Digital Humanities)[J]. Information Technology Newsweekly,2020.\n",
      "\n",
      "[19]鐜嬪埄鍚�.婢冲ぇ鍒╀簹楂樻牎鏁板瓧浜烘枃鏁欒偛鐮旂┒[J].鍥句功棣嗗�︾爺绌�,2020(10):11-18.\n",
      "\n",
      "[20]閭撳悰,瀛欑粛涓�,鐜嬮槷,瀹嬪厛鏅�.缇庡浗29鎵�楂樻牎鏁板瓧浜烘枃椤圭洰鐮旂┒鍐呭�硅В鏋怺J].鎯呮姤璧勬枡宸ヤ綔,2020,41(03):31-40.\n",
      "\n",
      "[21]椹�鏄�浠�,浣曟嵎,鍒樺竻甯�.涓�鍥藉彜鍏稿彊浜嬫枃瀛︾殑鏃剁┖鍙欎簨鏁板瓧妯″瀷鐮旂┒鈥斺�斾互銆婃潕濞冧紶銆嬩负渚媅J].鍦扮悆淇℃伅绉戝�﹀�︽姤,2020,22(05):967-977.\n",
      "\n",
      "[22]璁稿垰,绉︽槅.鈥滅┖闂寸患鍚堜汉鏂囧�︿笌绀句細绉戝�︾殑鏄ㄥぉ銆佷粖澶╁拰鏄庡ぉ鈥濇矙榫欑邯瑕乕J].鍦扮悆淇℃伅绉戝�﹀�︽姤,2020,22(05):1176-1177.\n",
      "\n",
      "[23]鏉ㄥ啝鐏�,鍗㈠皬瀹�.闈㈠悜鏁板瓧浜烘枃鐨勪含鍓ц劯璋卞浘鍍忔暟瀛楄祫婧愭瀯寤篬J].妗ｆ�堝�﹂�氳��,2020(03):38-44.\n",
      "\n",
      "[24]鑱備簯闇�,鑲栧潳.鏁板瓧浜烘枃瑙嗗煙涓嬫。妗堝�︿笓涓氬�︾敓鏁版嵁绱犲吇鍩硅偛鎺㈡瀽[J].妗ｆ�堝�﹂�氳��,2020(03):95-103.\n",
      "\n",
      "[25]宸﹀��,寮犲崼涓�.闈㈠悜鏁板瓧浜烘枃鐨勬。妗堣祫婧愭暣鍚堟ā寮忔瀯鎯�:瑙ｆ瀯涓庨噸缁刐J].妗ｆ�堝�﹂�氳��,2020(03):29-37.\n",
      "\n",
      "[26]鍒樿姰,璋�蹇呭媷.绔嬭冻鏈�鍦熶笌璺ㄧ晫铻嶅悎:娆х編鏁板瓧浜烘枃鏁欒偛鍙戝睍鐜扮姸鍙婂惎绀篬J].妗ｆ�堝�﹂�氳��,2020(03):104-112.\n",
      "\n",
      "[27]璧佃枃.鏁板瓧鏃朵唬鐨勨�滀笘鐣屾枃瀛︹�濈爺绌讹細浠庢�傚康妯″瀷鍒拌�＄畻鎵硅瘎[J].澶栧浗鏂囧�﹀姩鎬佺爺绌�,2020(03):35-48.\n",
      "\n",
      "[28]Benito-Santos Alejandro,Sanchez Roberto Theron. A Data-Driven Introduction to Authors, Readings, and Techniques in Visualization for the Digital Humanities.[J]. Pubmed,2020,40(3).\n",
      "\n",
      "[29]Michael L. Black. Usable and Useful: On the Origins of Transparent Design in Personal Computing[J]. SAGE Publications,2020,45(3).\n",
      "\n",
      "[30]鐜嬩附鍗�,鍒樼倻,鍒樺湥濠�.鏁板瓧浜烘枃鐨勭悊璁哄寲瓒嬪娍鍓嶇灮[J].涓�鍥藉浘涔﹂�嗗�︽姤,2020,46(03):17-23.\n",
      "\n",
      "[31]澶忕繝濞�.闈㈠悜浜烘枃鐮旂┒鐨勨�滄暟鎹�鍩虹��璁炬柦鈥濆缓璁锯�斺�旇瘯璁哄浘涔﹂�嗗�﹀�规暟瀛椾汉鏂囩殑鏂规硶璁鸿础鐚甗J].涓�鍥藉浘涔﹂�嗗�︽姤,2020,46(03):24-37.\n",
      "\n",
      "[32]鏉庢儬,渚�鍚涙槑,闄堟稕,鏈卞簡鍗�,鍒樼倻.鏄熸眽绐堟负鈥斺�斾功淇＄綉缁滀腑钑磋棌鐨勪汉闄呭叧绯绘寲鎺榌J].鍥句功棣嗘潅蹇�,2020,39(05):86-92+80.\n",
      "\n",
      "[33]鏂芥檽鍗�,鐜嬫槙.鏁板瓧浜烘枃绀句細缃戠粶鍒嗘瀽鏂规硶搴旂敤涓庣爺绌禰J].鍥句功棣嗘潅蹇�,2020,39(05):93-99.\n",
      "\n",
      "[34]鏉ㄥ弸娓�,鐜嬪埄鍚�,鐜嬩笢浜�.鍔犳嬁澶ч珮鏍℃暟瀛楀�︽湳涓�蹇冭皟鏌ュ垎鏋愪笌鍚�绀篬J].鍥句功棣嗗�︾爺绌�,2020(09):52-59.\n",
      "\n",
      "[35]鍚曡悓钀�.鏁板瓧浜烘枃鑳屾櫙涓嬬殑妗ｆ�堝睍瑙堝彂灞曠瓥鐣ョ爺绌禰J].鍏板彴鍐呭��,2020(14):4-6.\n",
      "\n",
      "[36]Kushwanth Koya,Gobinda Chowdhury. Cultural Heritage Information Practices and iSchools Education for Achieving Sustainable Development[J]. John Wiley & Sons, Inc.,2020,71(6).\n",
      "\n",
      "[37]姊佸皯鍗�,鍚翠腹.鏁板瓧鍥句功棣嗕俊鎭�鏈嶅姟鐨勮瀺鍚堜笌鍒涙柊鈥斺�斿熀浜�2019骞碕CDL骞翠細璁烘枃鐨勭患杩癧J].鍥句功鎯呮姤鐭ヨ瘑,2020(03):97-108.\n",
      "\n",
      "[38]闊╃憺楣�.鏁板瓧浜烘枃瑙嗚�掍笅鑸嗗浘妗ｆ�堝紑鍙戝眰娆℃瀯寤篬J].妗ｆ�堢�＄悊,2020(03):46-48.\n",
      "\n",
      "[39]寮犲崼涓�.鍙嬭█鍦ㄥ厛[J].鍏板彴涓栫晫,2020(05):21.\n",
      "\n",
      "[40]寮犲崼涓�,璧电儴姗�.鏁板瓧浜烘枃鍦ㄦ。妗堝�︾�戝簲鐢ㄧ殑鍙嶆�濅笌鍓嶆櫙[J].鍏板彴涓栫晫,2020(05):22-26+39.\n",
      "\n",
      "[41]寮犲崼涓�,浜庢箾.闈㈠悜鏁板瓧浜烘枃鐨勬。妗堣祫婧愬紑鍙戞柟寮忎笌鍒╃敤璺�寰勬帰绌禰J].鍏板彴涓栫晫,2020(05):27-32.\n",
      "\n",
      "[42]寮犳枌,鏉ㄦ枃.涓�鍥芥。妗堝�︾爺绌剁儹鐐逛笌鍓嶆部闂�棰樻帰璁╗J].鍥句功鎯呮姤鐭ヨ瘑,2020(03):28-40+62.\n",
      "\n",
      "[43]楂樻ⅵ鏉�.鎴戝浗妗ｆ�堥�嗘暟瀛椾汉鏂囨湇鍔＄殑鐜扮姸銆侀殰纰嶅強绛栫暐鐮旂┒[J].鍏板彴鍐呭��,2020(13):11-12.\n",
      "\n",
      "[44]鍚曠拹鎴�,闊╂稕.AI鍦ㄥ浘鎯咃細浜哄伐鏅鸿兘璧嬭兘鍥炬儏鏈嶅姟鈥斺��2019骞村浘涔﹂�嗗墠娌挎妧鏈�璁哄潧(IT4L)浼氳��缁艰堪[J].鍐滀笟鍥句功鎯呮姤瀛︽姤,2020,32(05):13-18.\n",
      "\n",
      "[45]Robert Z. Selden,John E. Dockall,Morgane Dubied. A quantitative assessment of intraspecific morphological variation in Gahagan bifaces from the southern Caddo area and central Texas[J]. Routledge,2020,39(2).\n",
      "\n",
      "[46]. Social Sciences; Ain't No Way Around It: Why We Need to Be Clear About What We Mean by 鈥淒igital Humanities鈥� (Updated April 15, 2020)[J]. Science Letter,2020.\n",
      "\n",
      "[47]褰�鏂囪檸.鏁板瓧浜烘枃鐜�澧冧笅楂樻牎鍥句功棣嗛槄璇绘帹骞垮垱鏂扮爺绌禰J].鍥句功棣嗗�﹀垔,2020,42(04):45-49.\n",
      "\n",
      "[48]鏉庡瓙鏋�,榫欏�跺簡,鐜嬬帀鐝�.浜ゆ祦涓庡悎浣�:缇庡浗鏁板瓧浜烘枃涓庢。妗堥�嗗煙鐨勪簰鍔ㄥ強鍚�绀篬J].妗ｆ�堝�︾爺绌�,2020(02):130-137.\n",
      "\n",
      "[49]閮戞案.鏁板瓧浜烘枃鏃朵唬鍏�鍏卞浘涔﹂�嗗叕鍏辨暟瀛楁湇鍔′綋绯诲缓璁剧爺绌禰J].姹熻嫃绉戞妧淇℃伅,2020,37(11):15-17+24.\n",
      "\n",
      "[50]寮犱箰鑾�,寮犲崼涓�,璧电儴姗�.鏁板瓧浜烘枃浜у搧寮�鍙戣繃绋嬩腑妗ｆ�堥�嗙殑瑙掕壊瀹氫綅鐮旂┒[J/OL].灞辫タ妗ｆ��:1-11[2020-06-26].http://kns.cnki.net/kcms/detail/14.1162.G2.20200417.1446.002.html.\n",
      "\n",
      "[51]楂樺缓杈�.鏁板瓧浜烘枃瑙嗗煙涓嬪皯鏁版皯鏃忓彛杩板巻鍙茶祫鏂欐姠鏁戞�ч噰闆嗘柟娉曠爺绌禰J].鍥句功棣�,2020(03):55-60.\n",
      "\n",
      "[52]灏圭泭姘�.缇庡浗鏅�鏋楁柉椤垮ぇ瀛﹀浘涔﹂�嗘暟瀛椾汉鏂囧缓璁剧幇鐘跺強鍚�绀篬J].鍥句功棣�,2020(03):61-66+80.\n",
      "\n",
      "[53]寮犱紵,鍗庢灄.浜戝崡灏戞暟姘戞棌涔¤�勬皯绾︽。妗堣祫婧愬缓璁剧爺绌禰J/OL].灞辫タ妗ｆ��:1-8[2020-06-26].http://kns.cnki.net/kcms/detail/14.1162.G2.20200410.1118.004.html.\n",
      "\n",
      "[54]Luis A. Pineda,No茅 Hern谩ndez,Iv谩n Torres,Gibr谩n Fuentes,Nydia Pineda De Avila. Practical non-monotonic knowledge-base system for un-regimented domains: A Case-study in digital humanities[J]. Elsevier Ltd,2020,57(3).\n",
      "\n",
      "[55]Penesta Dika. Review of Oliver Grau, Janina Hoth, & Eveline Wandl-Vogt (Eds.) (2019). Digital Art through the Looking Glass: New strategies for archiving, collecting and preserving in digital humanities Hamburg/Krems/Vienna: Edition Donau-Universit盲t Krems and Austrian Academy of Sciences. 312 pp. ISBN 9783903150515 (E-Book)[J]. Springer International Publishing,2020,2(1).\n",
      "\n",
      "[56]璧佃枃. 璁＄畻鍒涢�犲姏涓庤�＄畻鎵硅瘎[N]. 涓�鍥界ぞ浼氱�戝�︽姤,2020-04-03(004).\n",
      "\n",
      "[57]Hannah J. Dean,Ryan L. Boyd. Deep into that darkness peering: A computational analysis of the role of depression in Edgar Allan Poe's life and death[J]. Elsevier B.V.,2020,266.\n",
      "\n",
      "[58]Radu Suciu. Viral Networks: Connecting Digital Humanities and Medical History ed. by E. Thomas Ewing and Katherine Randall (review)[J]. Bulletin of the History of Medicine,2020,94(1).\n",
      "\n",
      "[59]Dean Hannah J,Boyd Ryan L. Deep into that darkness peering: A computational analysis of the role of depression in Edgar Allan Poe's life and death.[J]. Pubmed,2020,266.\n",
      "\n",
      "[60]閮戞案鏅�,娈垫捣钃�.鍙ょ睄鏁板瓧鍖栥�佹暟瀛椾汉鏂囦笌鍙や唬鏂囧�︾爺绌垛�斺�旇�夸腑鍥界ぞ浼氱�戝�﹂櫌閮戞案鏅撴暀鎺圼J].鍚夐�栧ぇ瀛﹀�︽姤(绀句細绉戝�︾増),2020,41(02):144-151.\n",
      "\n",
      "[61]鐜嬬帀鐝�.瀛︾�戣瀺鍚堜笌杈圭晫鎺㈢储锛氶潰鍚戞暟瀛椾汉鏂囩殑妗ｆ�堢爺绌禰J].灞辫タ妗ｆ��,2020(02):1.\n",
      "\n",
      "[62]瀛欒緣.璁ょ煡绉戝�﹁�嗚�掍笅瀵规暟瀛楀彶瀛︾殑閫忚�哰J].鏂囩尞涓庢暟鎹�瀛︽姤,2020,2(01):57-67.\n",
      "\n",
      "[63]. Information Technology - Information Management; Researchers from Faculty of Business and Economics Report on Findings in Information Management (Crowdfunding In Digital Humanities: Some Evidence From Indonesian Social Enterprises)[J]. Computers, Networks & Communications,2020.\n",
      "\n",
      "[64]Cornia Marcella,Stefanini Matteo,Baraldi Lorenzo,Corsini Massimiliano,Cucchiara Rita. Explaining digital humanities by aligning images and textual descriptions[J]. North-Holland,2020,129(C).\n",
      "\n",
      "[65]榄忔竻鍗�,鍒樺嫄.闈炵墿璐ㄦ枃鍖栭仐浜х煡璇嗗簱鏋勫缓鈥斺�斾互鐢樿們鐪佸浗瀹剁骇闈為仐涓轰緥[J].鍥句功棣嗗�︾爺绌�,2020(06):33-38.\n",
      "\n",
      "[66]鍛ㄦ枃鏉�.鍥炬儏妗ｅ�︾�戝彂灞曠殑鏁版嵁鍖栬秼鍚戣В鏋愨�斺�斿熀浜�2019骞村害瀛︽湳鐑�鐐圭殑绯荤粺鎬ф枃鐚�璋冩煡[J].鎯呮姤璧勬枡宸ヤ綔,2020,41(02):20-30.\n",
      "\n",
      "[67]闂�鎱�.2019骞翠腑鍥藉浘涔︽儏鎶ヤ笌妗ｆ�堢�＄悊棰嗗煙鐮旂┒鐑�鐐瑰洖椤綶J].鎯呮姤璧勬枡宸ヤ綔,2020,41(02):5-19.\n",
      "\n",
      "[68]璧佃枃.缃戠粶鍒嗘瀽涓庝汉鐗╃悊璁篬J].鏂囪壓鐞嗚�轰笌鎵硅瘎,2020(02):38-46.\n",
      "\n",
      "[69]鐜嬪啗.浠庝汉鏂囪�＄畻鍒板彲瑙嗗寲鈥斺�旀暟瀛椾汉鏂囩殑鍙戝睍鑴夌粶姊崇悊[J].鏂囪壓鐞嗚�轰笌鎵硅瘎,2020(02):18-23.\n",
      "\n",
      "[70]閭变紵浜�.璁烘暟瀛椾汉鏂囩爺绌朵腑鍙�瑙嗗寲鏁版嵁鐨勬剰涔変笌浠峰�尖�斺�斾互鏁板瓧姒傚康鍙茬爺绌朵负渚媅J].鏂囪壓鐞嗚�轰笌鎵硅瘎,2020(02):23-29.\n",
      "\n",
      "[71]涓ョ▼.鏂囪壓鐮旂┒闇�瑕佲�滄暟瀛椻�濆悧锛焄J].鏂囪壓鐞嗚�轰笌鎵硅瘎,2020(02):29-33.\n",
      "\n",
      "[72]鐜嬭春.鈥滄暟瀛椾汉鏂団�濆彇鍚戠殑涓�鍥界幇浠ｆ枃瀛︾爺绌讹細闂�棰樹笌鏂规硶[J].鏂囪壓鐞嗚�轰笌鎵硅瘎,2020(02):33-38.\n",
      "\n",
      "[73]Abbie Levesque DeCamp. XM<LGBT/>: A Schema for Encoding Queer Identities in Qualitative Research[J]. Elsevier Inc.,2020,55.\n",
      "\n",
      "[74]鏉庢枃鍖�,闄堣櫣,鏉庡啲钑�.鏁板瓧浜烘枃瑙嗗煙涓嬬殑鍗楁捣鏇磋矾绨跨患鍚堢爺绌禰J].澶у�﹀浘涔﹂�嗗�︽姤,2020,38(02):91-98.\n",
      "\n",
      "[75]Duan,Chiang,Leyk,Uhl,Knoblock. Automatic alignment of contemporary vector data and georeferenced historical maps using reinforcement learning[J]. Taylor & Francis,2020,34(4).\n",
      "\n",
      "[76]濮滀箹淇�.鍥句功棣嗗�︼細璇濊��浣撶郴涓庣悊璁哄垱鏂癧J].鏂颁笘绾�鍥句功棣�,2020(03):11-18.\n",
      "\n",
      "[77].銆婃柊涓栫邯鍥句功棣嗐�嬬紪杈戦儴2020骞寸敤绋块�夐�樻寚鍗梉J].鏂颁笘绾�鍥句功棣�,2020(03):18.\n",
      "\n",
      "[78]鑼冩枃娲�,鏉庡繝鍑�,榛勬按娓�.鍩轰簬绀句細缃戠粶鍒嗘瀽鐨勩�婂乏浼犮�嬫垬浜夎�￠噺鍙婂彲瑙嗗寲鐮旂┒[J].鍥句功鎯呮姤宸ヤ綔,2020,64(06):90-99.\n",
      "\n",
      "[79]鏉庢槑鏉�,寮犵氦鏌�,闄堟ⅵ鐭�.鍙ょ睄鏁板瓧鍖栫爺绌惰繘灞曡堪璇�(2009-2019)[J].鍥句功鎯呮姤宸ヤ綔,2020,64(06):130-137.\n",
      "\n",
      "[80]閮戝媷,寮犳晱.鏁板瓧浜烘枃瑙嗗煙涓嬫。妗堟枃鍖栬祫婧愬紑鍙戞暣鍚堢爺绌禰J].灞辫タ妗ｆ��,2020(02):109-112+32.\n",
      "\n",
      "[81]. Information Technology - Data Analytics; New Findings from University of Helsinki Update Understanding of Data Analytics (Using the Semantic Web in digital humanities: Shift from data publishing to data-analysis and serendipitous knowledge discovery)[J]. Information Technology Newsweekly,2020.\n",
      "\n",
      "[82]榛勬樉.浣滀负鍙叉枡鐨勬暟瀛楀獟浠�:鍩轰簬S甯傚煄甯傚巻鍙茬敓浜х殑涓�妗堣�冨療[J].鏂伴椈鐭ヨ瘑,2020(03):27-34.\n",
      "\n",
      "[83].鏈�鏈熻仛鐒�[J].涓�鍥界炕璇�,2020,41(02):1.\n",
      "\n",
      "[84]鑳″紑瀹�,榛戦粺.鏁板瓧浜烘枃瑙嗗煙涓嬬炕璇戠爺绌讹細鐗瑰緛銆侀�嗗煙涓庢剰涔塠J].涓�鍥界炕璇�,2020,41(02):5-15+187.\n",
      "\n",
      "[85]绔ヨ尩.鏁板瓧浜烘枃鑼冨紡妗嗘灦涓庣粯鐢绘枃鐗〢I鏅鸿兘鐮旂┒[J].璁＄畻鏈轰骇鍝佷笌娴侀��,2020(03):133-135.\n",
      "\n",
      "[86]璧靛畤缈�,鍒樺懆棰�,鍒樼倻,楂樿儨瀵�,寮犵��.鍒涙剰绫诲紑鏀炬暟鎹�绔炶禌浣滃搧璇勪环鎸囨爣浣撶郴鏋勫缓涓庢祴瀹氣�斺�斾互鏁板瓧浜烘枃椤圭洰涓轰緥[J].涓�鍥藉浘涔﹂�嗗�︽姤,2020,46(02):75-95.\n",
      "\n",
      "[87]瀹嬪畞杩�,鐜嬫檽鍏�.鍩轰簬鎯呰妭鏈�浣撶殑鍙欎簨鎬ф枃鏈�璇�涔夌粨鏋勫寲琛ㄧず鏂规硶鐮旂┒[J].涓�鍥藉浘涔﹂�嗗�︽姤,2020,46(02):96-113.\n",
      "\n",
      "[88]寮犲叴鏃�.浠庘�滀腑鎰忔枃鍖栫浉浜测�濈幇璞＄湅鎰忓ぇ鍒╂按涓嬫枃鍖栭仐浜ф暟瀛楀浘涔﹂�嗗缓璁惧�规垜鍥界殑鍚�绀篬J].楂樻牎鍥句功棣嗗伐浣�,2020,40(02):26-33.\n",
      "\n",
      "[89]鍒樿�樺崕.杩滅▼闃呰�绘椂浠ｈ瘲瀛﹀�硅瘽鐨勬柟娉曡�哄缓鏋刐J].鍗庝笢甯堣寖澶у�﹀�︽姤(鍝插�︾ぞ浼氱�戝�︾増),2020,52(02):84-94+195.\n",
      "\n",
      "[90].鍏氬憳澶ц�嗛噹[J].瀹炶返(鍏氱殑鏁欒偛鐗�),2020(03):48-49.\n",
      "\n",
      "[91]鐜嬫稕. 鍙戞尌鏁板瓧浜烘枃鍦ㄥ彶瀛︾爺绌朵腑鐨勬晥鐢╗N]. 绀句細绉戝�︽姤,2020-03-12(005).\n",
      "\n",
      "[92]鐜嬭磭,寮犳斂.缈昏瘧鐮旂┒鏂拌矾寰�:鏁板瓧浜烘枃鏂伴噴[J].澶栬��鏁欏��,2020,41(02):81-86.\n",
      "\n",
      "[93]绔ヨ尩.璇�涔夊寲鐭ヨ瘑妯″瀷鏋勫缓涓庡叧鑱旀暟鎹�鐮旂┒鈥斺�旇懀鍏舵槍鏁板瓧浜烘枃鏁版嵁缁樺埗鎶ュ憡[J].绉戞妧浼犳挱,2020,12(05):135-137.\n",
      "\n",
      "[94]鍏跺叾鏍�.鏁板瓧浜烘枃椤圭洰涓�鏁板瓧鍖栨爣鍑嗗垵鎺�[J].绉戞妧鍒涙柊涓庣敓浜у姏,2020(03):36-39.\n",
      "\n",
      "[95]鍒樻�傞攱,鑻忔枃鎴�,鍗㈢珷骞�.鍔犲己鎴樼暐瑙勫垝  瀹炵幇杞�鍨嬪崌绾р�斺�斺�滃浘涔﹂�嗘垬鐣ヨ�勫垝涓庡�︾�戝彂灞曞浗闄呯爺璁ㄤ細鈥濈患杩癧J].鍥句功鎯呮姤鐮旂┒,2020,13(01):122-128.\n",
      "\n",
      "[96]榫欏�跺簡,鐜嬬帀鐝�,鏉庡瓙鏋�,璁镐匠娆�.鏁板瓧浜烘枃瀵规垜鍥芥。妗堥�嗗煙鐨勫奖鍝�:鎸戞垬銆佹満閬囦笌瀵圭瓥[J].妗ｆ�堝�︾爺绌�,2020(01):104-110.\n",
      "\n",
      "[97]. Science - Linguistics; New Findings from University of Victoria in the Area of Linguistics Reported (Shelf life: Identifying the abandonment of online digital humanities projects)[J]. Science Letter,2020.\n",
      "\n",
      "[98]璁告旦,妗備腹浜�,鍚翠腹.浜轰笌淇℃伅鏃犲�勪笉鍦ㄧ殑杩炴帴涓庝氦浜掆�斺��2019骞碅SIS&T骞翠細璁烘枃缁艰堪[J].鍥句功鎯呮姤鐭ヨ瘑,2020(02):82-92.\n",
      "\n",
      "[99]. Science - Linguistics; Investigators at University of Hamburg Detail Findings in Linguistics (Beyond digitization? Digital humanities and the case of Hebrew literature)[J]. Science Letter,2020.\n",
      "\n",
      "[100]榄忓缓浜�.閲嶅�℃灄鍏村畢鐨勭郴缁熻�烘枃鑹哄�︽�濇兂[J].灞变笢绀句細绉戝��,2020(03):97-103.\n",
      "\n",
      "[101]鐜嬫槉,钖涙枃钀�,鍛ㄦ槉,璋�蹇呭媷.鏁板瓧浜烘枃瑙嗚�掍笅鐨勭孩鑹叉枃鍖栦紶鎵库�斺�斾互绾㈠珎绮剧�炰负渚媅J].灞辫タ妗ｆ��,2020(02):92-100.\n",
      "\n",
      "[102]钖涙枃钀�,鍛ㄦ槉,鐜嬫槉,璋�蹇呭媷.鏁板瓧浜烘枃瑙嗚�掍笅鐨勭孩鑹叉。妗堣祫婧愬缓璁锯�斺�斾互娌傝挋绾㈠珎妗ｆ�堜负渚媅J].灞辫タ妗ｆ��,2020(02):85-91.\n",
      "\n",
      "[103]Arianne Hartsell-Gundy,Kelley Lawton,Hannah Rozear. 17 librarians and one big undertaking: creating a digital project from start to finish[J]. Routledge,2020,32(1).\n",
      "\n",
      "[104]Aroyo Lora,de Jong Franciska,Hyv枚nen Eero,Tonelli Sara. Web Semantics for Digital Humanities[J]. Elsevier,2020,61(C).\n",
      "\n",
      "[105]宕斿嚖濞�.鎺㈢储鏁板瓧浜烘枃[J].璺ㄦ枃鍖栫爺绌�,2019(02):210-244+275-276.\n",
      "\n",
      "[106]閮戣晩,鏉滆崳鑺�.鏁板瓧浜烘枃鏃朵唬楂樻牎鍥句功棣嗚挋鍙ゅ�﹀�︾�戞湇鍔″崌绾х爺绌垛�斺�斾互鍐呰挋鍙ら珮鏍″浘涔﹂�嗕负渚媅J].鍛间鸡璐濆皵瀛﹂櫌瀛︽姤,2020,28(01):102-106.\n",
      "\n",
      "[107]Shung-eun Hwang. 雮挫毄 欷戩嫭 頃滉淡鞏� 甑愳湣鞚� 鞙勴暅 頃欖垹鞏错湗 甑愳湣 鞐瓣惮-鞓�霛检澑 靷�鞝� 臧滊皽鞚� 鞙勴暅 旮办磮 鞐瓣惮[J]. 頃滉淡旎错摠韯办爼氤错暀須�,2020,25(2).\n",
      "\n",
      "[108]鏉庢槑鏉�,鏉ㄧ拹鍢�.鍩轰簬GIS鐨勬槑浠ｅ彜绫嶇増鍒诲湴鐞嗕俊鎭�绯荤粺鐨勮�捐�′笌瀹炵幇[J].淇℃伅璧勬簮绠＄悊瀛︽姤,2020,10(03):125-133.\n",
      "\n",
      "[109]瀛ｅ崼涓�.鏂版枃绉戠殑瀛︽湳鑼冨紡涓庨泦缇ゅ寲[J].涓婃捣浜ら�氬ぇ瀛﹀�︽姤(鍝插�︾ぞ浼氱�戝�︾増),2020,28(01):11-14.\n",
      "\n",
      "[110]鑻忚姵鑽�,甯镐汉鏉�.鏁板瓧浜烘枃鐮旂┒鈥斺�旇法瀛︾�戞�с�佸悎浣滄ā寮忓強涓婚�榌J].鍥句功鎯呮姤瀵煎垔,2020,5(02):53-58.\n",
      "\n",
      "[111]灏氬�嬪畤,浠绘枃鏂�.闈㈠悜鏁板瓧浜烘枃鐨勯珮鏍″紑鏀炬。妗堢粍缁囩瓥鐣ュ強鍒╃敤妯″紡鐮旂┒[J].灞辫タ妗ｆ��,2020(02):101-108.\n",
      "\n",
      "[112]Evangeline Tsao. Bodies of information: intersectional feminism and digital humanities[J]. Routledge,2020,23(3).\n",
      "\n",
      "[113].銆婃柊涓栫邯鍥句功棣嗐�嬬紪杈戦儴2020骞寸敤绋块�夐�樻寚鍗梉J].鏂颁笘绾�鍥句功棣�,2020(02):16.\n",
      "\n",
      "[114]鏉庢柊鏉�,钄￠�歌摀,鏉庝腹闃�.淇℃伅绉戝�︾爺绌跺墠娌夸笌鐑�鐐光�斺��2019骞碅SIS&T骞翠細缁艰堪[J/OL].鍥句功棣嗚�哄潧:1-11[2020-06-26].http://kns.cnki.net/kcms/detail/44.1306.G2.20200219.1112.002.html.\n",
      "\n",
      "[115].闃呰�讳腑鐨勯潚骞村�﹁�呪�斺��2019骞村害鍥句功鎯呮姤涓庢。妗堢�＄悊闃呰�讳功鍗昜J].鍥句功鎯呮姤鐭ヨ瘑,2020(01):115-127.\n",
      "\n",
      "[116]閽熻繙钖�.鍑℃槸杩囧線,鐨嗕负搴忕珷鈥斺�旇瘎銆婃暟瀛椾汉鏂�:鏀瑰彉鐭ヨ瘑鍒涙柊涓庡垎浜�鐨勬父鎴忚�勫垯銆媅J/OL].鍥句功棣嗚�哄潧,2020(07):1-8[2020-06-26].http://kns.cnki.net/kcms/detail/44.1306.G2.20200217.1542.002.html.\n",
      "\n",
      "[117]鑻忔晱.缇庡浗楂樻牎鍥句功棣嗗紑灞曟暟瀛椾汉鏂囨湇鍔＄殑璺�寰勪笌鍚�绀篬J/OL].鎯呮姤鐞嗚�轰笌瀹炶返:1-11[2020-06-26].http://kns.cnki.net/kcms/detail/11.1762.g3.20200217.1538.002.html.\n",
      "\n",
      "[118]鍛ㄦ祾.澶ф暟鎹�瑙嗛噹涓嬬殑鏁板瓧浜烘枃涓庢柊闂诲彶浜虹墿鐮旂┒[J].浼犲獟瑙傚療,2020(02):87-93.\n",
      "\n",
      "[119]鑸掑繝姊�.鏁板瓧浜烘枃鑳屾櫙涓嬬殑妗ｆ�堢煡璇嗗浘璋辨瀯寤虹爺绌禰J].灞辫タ妗ｆ��,2020(02):53-60.\n",
      "\n",
      "[120]寮犲姏鍏�,鐜嬪啗.鍩轰簬绀句細缃戠粶鍔ㄥ姏瀛︾殑涓ゅ畫瀛︽湳鍜屾斂娌讳綋绯绘瘮杈冨垎鏋怺J].鎯呮姤宸ョ▼,2020,6(01):34-49.\n",
      "\n",
      "[121]Stefanos Geroulanos,Leif Weatherby. Cybernetics and the human sciences[J]. SAGE Publications,2020,33(1).\n",
      "\n",
      "[122]寰愬織鐜�,鍒樼帀鍗�.鍥句功鎯呮姤瀛﹀�︾�戣�嗗煙涓嬬殑澶ф暟鎹�鎸栨帢鐮旂┒椤圭洰鍐呭�瑰垎鏋愨�斺�斾互鈥滄寲鎺樻暟鎹�鎸戞垬鈥濋」鐩�涓轰緥[J].鎯呮姤鎺㈢储,2020(02):72-76.\n",
      "\n",
      "[123]瀛熷缓,鑳″�﹀嘲.鏁板瓧浜烘枃鐮旂┒:瓒呭�︾�戞柟娉曡�虹殑涓�绉嶈�ょ煡涓庨槓閲奫J].鐜颁唬浼犳挱(涓�鍥戒紶濯掑ぇ瀛﹀�︽姤),2020,42(02):13-17+24.\n",
      "\n",
      "[124]. Science - Linguistics; Recent Findings in Linguistics Described by Researchers from University of Bergamo (Syncretic Modality In Slideshows In the Era of Digital Humanities: Towards a Reconceptualization of Visuals?)[J]. Science Letter,2020.\n",
      "\n",
      "[125]鐢冲�╁��,璐哄溅骞�,椹�闆�姊�.缇庡浗椹�閲屽叞澶у�︽暟瀛椾汉鏂囧�︾�戞暀鑲插疄璺电爺绌禰J].绉戞暀瀵煎垔(涓婃棳鍒�),2020(02):8-9+12.\n",
      "\n",
      "[126]鏉庡瓙鏋�,璁镐匠娆�.妗ｆ�堝湪鏁板瓧浜烘枃鐮旂┒涓�鐨勭壒娈婃�с�侀」鐩�璇嗗埆鍙婂簲鐢╗J].灞辫タ妗ｆ��,2020(02):61-69.\n",
      "\n",
      "[127]. Look Up, Look Out: Discrepant Stories from the Old Eighth Ward[J]. Pennsylvania State University Press,2020,87(1).\n",
      "\n",
      "[128]. Reframing, Rethinking, and Remembering: Considering the Digital Harrisburg Project[J]. Pennsylvania State University Press,2020,87(1).\n",
      "\n",
      "[129]Charles Travis,Francis Ludlow,Al Matthews,Kevin Lougheed,Kieran Rankin,Bernard Allaire,Robert Legg,Patrick Hayes,John Nicholls,Lydia Towns,Poul Holm. Inventing the Grand Banks: A deep chart[J]. Geo: Geography and Environment,2020,7(1).\n",
      "\n",
      "[130]闄堥潤.澶嶆暟鐨勬暟瀛椾汉鏂団�斺�旀瘮杈冭�嗛噹涓嬬殑涓�鍥芥暟瀛椾汉鏂嘯J].绀句細绉戝�︽枃鎽�,2020(01):104-106.\n",
      "\n",
      "[131]Marcella Cornia,Matteo Stefanini,Lorenzo Baraldi,Massimiliano Corsini,Rita Cucchiara. Explaining digital humanities by aligning images and textual descriptions[J]. Elsevier B.V.,2020,129.\n",
      "\n",
      "[132]鏉ㄥ獩.鏁板瓧闊充箰瀛︾爺绌剁患杩癧J].鍖楁柟闊充箰,2020(02):4-5+14.\n",
      "\n",
      "[133]鍚翠附鍗�.鎴戝浗鏁板瓧浜烘枃棰嗗煙鐮旂┒鐗瑰緛鍒嗘瀽鈥斺�斿熀浜嶤NKI鏈熷垔鏁版嵁[J].鍐呰挋鍙ょ�戞妧涓庣粡娴�,2020(02):156-158+160.\n",
      "\n",
      "[134]闄堢惓.鍩轰簬鐭ヨ瘑鍥捐氨鐨勫浗鍐呮暟瀛椾汉鏂囩爺绌剁幇鐘跺垎鏋怺J].鍥句功棣嗗�﹀垔,2020,42(01):89-94.\n",
      "\n",
      "[135].2019骞村害涓�鍥藉浘鎯呮。瀛︾晫鍗佸ぇ瀛︽湳鐑�鐐筟J].鎯呮姤璧勬枡宸ヤ綔,2020,41(01):5-12.\n",
      "\n",
      "[136]绋嬮潤.鍥藉唴澶栨暟瀛椾汉鏂囨湇鍔″钩鍙板缓璁剧幇鐘跺強鎬濊�僛J].鍥句功棣嗗�︾爺绌�,2020(02):37-42.\n",
      "\n",
      "[137]鍙堕潠瀹�.鏁板瓧浜烘枃鎶�鏈�鍦ㄦ。妗堢紪鐮斾腑鐨勫簲鐢ㄧ爺绌禰J].鐢佃剳鐭ヨ瘑涓庢妧鏈�,2020,16(03):13-15.\n",
      "\n",
      "[138]瑗胯挋路椹�闇嶅凹.鏁板瓧浜烘枃鐨勭爺绌惰��瑷�鍜岀爺绌跺�硅薄锛堣嫳鏂囷級[J].涓�鍥芥瘮杈冩枃瀛�,2020(01):96-113.\n",
      "\n",
      "[139].銆婃暟瀛椾汉鏂囥�嬪垱鍒婁华寮忔毃鏁板瓧浜烘枃宸ヤ綔鍧婂湪娓呭崕澶у�︿妇琛孾J].娓呭崕澶у�﹀�︽姤(鍝插�︾ぞ浼氱�戝�︾増),2020,35(01):207.\n",
      "\n",
      "[140]鏈辨槬娲�.鏁板瓧浜烘枃瑙嗚�掍笅涓�鍥藉彜浠ｆ祦璐�鐮旂┒鏂囩尞鍙�瑙嗗寲鍒嗘瀽[J].鍥句功棣�,2020(01):29-35.\n",
      "\n",
      "[141]涓佸叞鍏�.鏁板瓧浜烘枃瑙嗛噹涓嬮珮鏍″浘涔﹂�嗗憳涓庝汉鏂囧�﹁�呭悎浣滄帰鏋怺J].婀栧窞甯堣寖瀛﹂櫌瀛︽姤,2020,42(01):113-116.\n",
      "\n",
      "[142]娈靛姏钀�,榄忓嚖.鏂囩尞璁￠噺瀛﹁�嗚�掍笅鐨勫叏鐞冩暟瀛椾汉鏂囧彂灞曠幇鐘剁爺绌禰J].鍥句功棣�,2020(01):36-43.\n",
      "\n",
      "[143]鍛ㄧ瑧鐩�,榄忓ぇ濞�.鏁板瓧浜烘枃鑳屾櫙涓嬪熀浜庨渶姹傜殑鐭ヨ瘑鍙�瑙嗗寲鏂规硶鐮旂┒鈥斺�斾互鍥藉浘鍏�寮�璇剧殑瑙嗛�戝唴瀹瑰彲瑙嗗寲涓轰緥[J].鍥句功棣�,2020(01):20-28.\n",
      "\n",
      "[144]閭瑰父鍕�.澶ф暟鎹�瑙嗗煙涓嬬炕璇戝�︽暟瀛椾汉鏂囩爺绌禰J].瑗垮崡浜ら�氬ぇ瀛﹀�︽姤(绀句細绉戝�︾増),2020,21(01):123-129.\n",
      "\n",
      "[145]闄堝ぉ鏃�.鏁板瓧浜烘枃涓�鐨勫畾鎬т笌瀹氶噺鐮旂┒[J].鍏板彴鍐呭��,2020(02):36-37.\n",
      "\n",
      "[146]瑭归�哥弬.鏁板瓧浜烘枃椤圭洰鍓嶇��鍘嗗彶妗ｆ�堣祫婧愪紬鍖呮帰鏋愶細鐗瑰緛銆侀�庨櫓鍙婂叾鎺у埗[J].灞辫タ妗ｆ��,2020(02):77-84.\n",
      "\n",
      "[147]鑻忔晱.鍥句功棣嗗紑灞曟暟瀛椾汉鏂囨湇鍔＄爺绌剁患杩癧J].鎯呮姤鎺㈢储,2020(01):120-126.\n",
      "\n",
      "[148].闃呰�讳腑鐨勯潚骞村�﹁��:2019骞寸紪杈戝嚭鐗堝�﹂槄璇讳功鍗昜J].鍑虹増绉戝��,2020,28(01):116-128.\n",
      "\n",
      "[149]鑼冪帀鍒�.鏂囧寲杞�鍚戜笌鏁板瓧浜烘枃瑙嗗煙涓�鐨勭編瀛︾爺绌惰寖寮忛噸鏋刐J].涓�鍥介珮鏍＄ぞ浼氱�戝��,2020(01):124-134.\n",
      "\n",
      "[150]闄堣嫍,鍒樻櫁鏈�.鏁板瓧浜烘枃鐮旂┒鐑�鐐逛笌鍙戝睍瓒嬪娍鐨勬柇闈㈣�冨療鈥斺�斾互銆婃暟瀛椾汉鏂囧�ｅ垔銆嬪拰銆婁汉鏂囧�︾�戜腑鐨勬暟瀛楄�＄畻銆嬩负涓�蹇僛J].鍥句功棣嗙爺绌朵笌宸ヤ綔,2020(01):10-16.\n",
      "\n",
      "[151]娼樺▉,鐜嬪摬,婊″織鏁�.杩�20骞存潵鍘嗗彶鍦扮悊淇℃伅鍖栫殑鍙戝睍鎴愬氨[J].涓�鍥藉巻鍙插湴鐞嗚�轰笡,2020,35(01):25-35+82.\n",
      "\n",
      "[152]Simon Mahony.Language and the subjects of study in the Digital Humanities[J/OL].涓�鍥芥瘮杈冩枃瀛�:1-16[2020-06-26].https://doi.org/10.16234/j.cnki.cn31-1694/i.20200109.001.\n",
      "\n",
      "[153]Alex H. Poole,Deborah A. Garwood. Digging into data management in public鈥恌unded, international research in digital humanities[J]. John Wiley & Sons, Inc.,2020,71(1).\n",
      "\n",
      "[154]鏁︽枃鏉�.鏂版妧鏈�鐜�澧冧笅鍥句功棣嗗�逛紶缁熸枃鍖栫殑浼犳壙涓庡彂灞昜J].鍥句功棣嗗�﹀垔,2019,41(12):30-35.\n",
      "\n",
      "[155]鐜嬫�濆��.鏁板瓧浜烘枃瑙嗛槇涓嬫。妗堟暟瀛楀寲鐢熷瓨璺�寰勭殑鍒涙柊鎬濊�僛J].灞辫タ妗ｆ��,2020(02):70-76.\n",
      "\n",
      "[156]鍒樺畞闈�,鍒橀煶,鐜嬭帿瑷�,閮�鏅�.鏁板瓧浜烘枃瑙嗚�掍笅瀛︽湳鍚嶄汉鐭ヨ瘑妯″瀷鏋勫缓鐮旂┒鈥斺�斾互鏉庢斂閬撴暟瀛楄祫婧愪腑蹇冧负渚媅J].鍥句功鎯呮姤宸ヤ綔,2019,63(23):113-121.\n",
      "\n",
      "[157]Radhika Gajjala. New Digital Worlds: Postcolonial Digital Humanities in Theory, Praxis, and Pedagogy[J]. Routledge,2020,41(1).\n",
      "\n",
      "[158]Pamela VanHaitsma. Between archival absence and information abundance: Reconstructing Sallie Holley's abolitionist rhetoric through digital surrogates and metadata[J]. Routledge,2020,106(1).\n",
      "\n",
      "[159]R. Cecilia Knight,Elizabeth Rodrigues,Rebecca Ciota. Facilitating Collaborative Metadata Creation for Faculty-initiated Digital Projects[J]. Routledge,2020,20(1).\n",
      "\n",
      "[160]鍚寸函,榛勮瘲鑾�.鍙版咕鍥句功棣嗗垎榫勫垎浼楁湇鍔″疄璺靛強鍚�绀篬J].褰撲唬鍥句功棣�,2019(04):28-31+38.\n",
      "\n",
      "[161]旯�氙检��. 雿办澊韯半矤鞚挫姢搿滌劀鞚� 靸侅儊鞚� 氚曤�缄磤瓿� 靸堧�滌毚 靹滌偓: 霐旍��韯� 鞚鸽�疙暀 甏�鞝愳潉 欷戩嫭鞙茧�淸J]. 頃滉淡鞝勳嫓靷办梾鞙淀暕鞐瓣惮鞗�,2019,37(5).\n",
      "\n",
      "[162]寤栨斂璐�.鍙版咕鍦板尯鍏�鍏卞浘涔﹂�嗗弬涓庢暟瀛椾汉鏂囧彂灞曟帰鏋怺J].鍥句功棣嗙悊璁轰笌瀹炶返,2019(12):12-16.\n",
      "\n",
      "[163]鏋楁檽鐕�. 鍥為【涓庢�荤粨  瀹堟�ｅ張姹傛柊[N]. 涓�鍥界ぞ浼氱�戝�︽姤,2019-12-30(004).\n",
      "\n",
      "[164]Gabriele Griffin. Intersectionalized Professional Identities and Gender in the Digital Humanities in the Nordic Countries[J]. SAGE Publications,2019,33(6).\n",
      "\n",
      "[165]澶忓┓濠�,鐜嬭��娆�,閭瑰┓,鐜嬭姽.鏁板瓧浜烘枃瑙嗚�掍笅鐨勮嫃宸炲悕浜烘晠灞呬繚鎶や笌鍒╃敤鐮旂┒[J].鑷�鐒朵笌鏂囧寲閬椾骇鐮旂┒,2019,4(12):60-64.\n",
      "\n",
      "[166]寰愭櫒椋�,鍖呭钩.闈㈠悜鍐滃彶棰嗗煙鐨勬暟瀛椾汉鏂囩爺绌跺熀纭�璁炬柦寤鸿�剧爺绌垛�斺�斾互鏂瑰織鐗╀骇鐭ヨ瘑搴撴瀯寤轰负寮昜J].涓�鍥藉啘鍙�,2019,38(06):40-51.\n",
      "\n",
      "[167]Jean-Baptiste Lamy. Visualizing undirected graphs and symmetric square matrices as overlapping sets[J]. Springer US,2019,78(23).\n",
      "\n",
      "[168]L谩szl贸 Bengi. Calculation as a cultural practice in modern literature[J]. Springer International Publishing,2019,46(2).\n",
      "\n",
      "[169]钄￠��,钄¤繋鏄�.缇庡浗澶у�﹀浘涔﹂�嗛�嗚棌璧勬簮缁勭粐杩涘睍[J].鍥句功棣嗚�哄潧,2020,40(06):166-173.\n",
      "\n",
      "[170]鏈辩孩鑹�,钂嬭帋.鎴戝浗楂樻牎鏁板瓧浜烘枃鏁欒偛妯″紡鍒濇帰[J].鍥涘窛鍥句功棣嗗�︽姤,2019(06):85-88.\n",
      "\n",
      "[171]鐜嬭櫣.鈥滃弻涓�娴佲�濋珮鏍″浘涔﹂�嗙壒鑹茶祫婧愬缓璁剧幇鐘惰皟鏌ョ爺绌禰J].鍥句功棣嗗�︾爺绌�,2019(24):43-50.\n",
      "\n",
      "[172]鍛ㄦ枃娉�,鍒橀潤.鏁板瓧浜烘枃鍜屽浘涔︽儏鎶ヤ笌妗ｆ�堢�＄悊鐨勫弻鍚戞瀯寤鸿�佺偣鐮旂┒[J].鍥句功涓庢儏鎶�,2019(06):101-110.\n",
      "\n",
      "[173]宸﹀��,寮犲崼涓�.鏁板瓧浜烘枃瑙嗚�掍笅鐨勬。妗堝�︾爺绌禰J].鍥句功涓庢儏鎶�,2019(06):94-100.\n",
      "\n",
      "[174]涓佹暚杈�,椴佽幑.鏂�鍧︾�忓ぇ瀛︽暟瀛椾汉鏂囨暀鑲插疄璺靛強鍥句功棣嗘敮鎾戞湇鍔�[J].鍐滀笟鍥句功鎯呮姤,2019,31(11):15-22.\n",
      "\n",
      "[175]寮犵惇,绾�鏈変功,鐜嬩笢娉�.瀹氶噺瀵规瘮瑙嗚�掍笅鐨勫浗澶栨暟瀛椾汉鏂囩爺绌惰繘灞曞垎鏋怺J].鍐滀笟鍥句功鎯呮姤,2019,31(11):4-14.\n",
      "\n",
      "[176]閭变紵浜�.鎴戝浗鍙版咕鏁板瓧浜烘枃鐮旂┒杩涚▼(2009鈥�2017)[J/OL].鍥句功棣嗚�哄潧,2020(07):1-11[2020-06-26].http://kns.cnki.net/kcms/detail/44.1306.g2.20191223.0921.006.html.\n",
      "\n",
      "[177]璧甸洩鑺�,鑾�闀块暛,鏉庡ぉ濞�,缃楄��.浜烘枃瀛﹁�呯殑鏁板瓧浜烘枃鎺ュ彈鎰忔効褰卞搷鍥犵礌鈥斺�斾互鍘嗗彶瀛﹁�呬负涓�蹇冪殑鑰冨療[J].鍥句功棣嗚�哄潧,2020,40(04):40-49.\n",
      "\n",
      "[178]Simon Colreavy-Donnelly,Stuart O鈥機onnor,Elmina Homapour. I-Ulysses: A technical report[J]. Elsevier B.V.,2019,32.\n",
      "\n",
      "[179]鐜嬫柊鐏�.鏁板瓧浜烘枃鑳屾櫙涓嬬殑鍥句功棣嗚�昏�呮湇鍔″伐浣滄帰璁╗J].绉戞妧鍒涙柊瀵兼姤,2019,16(36):257-258.\n",
      "\n",
      "[180]Christian Sieg. Topic Modeling von Fallgeschichten[J]. Springer Berlin Heidelberg,2019,49(4).\n",
      "\n",
      "[181]Adam Jatowt,Daisuke Kawai,Katsumi Tanaka. Time-focused analysis of connectivity and popularity of historical persons in Wikipedia[J]. Springer Berlin Heidelberg,2019,20(4).\n",
      "\n",
      "[182]鏉庣編,榛勫垰.鈥滀竴甯︿竴璺�鈥濅腑鍥�-涓滅洘鏁板瓧浜烘枃涓庣壒鑹查�嗚棌鐨勬櫘閫傛�у钩鍙板姛鑳藉畾浣嶅缓鏋勭爺绌禰J].澶т紬绉戞妧,2019,21(12):135-138+153.\n",
      "\n",
      "[183]鍔犲皬鍙�,鍐�鎯犵幉.鈥淪CP2鈥濇暟瀛椾汉鏂囨暀鑲茬患鍚堜綋绯荤殑鏋勫缓涓庡簲鐢╗J].鍥句功棣嗚�哄潧,2020,40(04):22-29.\n",
      "\n",
      "[184]鏈辨�︿俊,澶忕繝濞�.鍛藉悕瀹炰綋璇嗗埆鍦ㄦ暟瀛椾汉鏂囦腑鐨勫簲鐢ㄢ�斺�斿熀浜嶦TL鐨勫疄鐜癧J].鍥句功棣嗚�哄潧,2020,40(05):16-20.\n",
      "\n",
      "[185]璧垫槦,鏉庝功瀹�,鑲栦簹鐢�.鏁板瓧浜烘枃瑙嗗煙涓嬪熀浜庡�氭簮鏁版嵁铻嶅悎鐨勪汉鐗╀笓棰樻暟鎹�搴撳缓璁锯�斺�斾互涓婃捣鍥句功棣�2018寮�鏀炬暟鎹�搴旂敤寮�鍙戠珵璧涗綔鍝佲�滄爲浜鸿�呪�濅负渚媅J].鍥句功棣嗘潅蹇�,2019,38(12):45-51.\n",
      "\n",
      "[186]椹�鍒涙柊,姊佺ぞ浼�,闄堝皬鑽�.鍏堢Е璇稿�跺�︽淳鐨勭浉鍏崇郴鏁颁笌鐗瑰緛璇嶇爺绌禰J].涓�鏂囦俊鎭�瀛︽姤,2019,33(12):129-134.\n",
      "\n",
      "[187]鑳″＋棰�.閬撴暀鏁板瓧浜烘枃骞冲彴寤鸿�惧垗璁甗J].涓�鍥介亾鏁�,2019(06):46-49.\n",
      "\n",
      "[188]褰�鍗庢澃.鏁板瓧浜烘枃:鍥句功棣嗘湇鍔″垱鏂扮殑涓�閬撻�庢櫙绾库�斺�旇瘎銆婂浘涔﹂�嗙殑鏁板瓧浜烘枃瀹炵幇妯″紡鐮旂┒銆媅J].娌冲崡鍥句功棣嗗�﹀垔,2019,39(12):36-38.\n",
      "\n",
      "[189]寮犵孩浼�,閭�涓�,闄堢幉,瑙ｇ礌鑺�,鐜嬫槬姊�.鍩轰簬鐢ㄦ埛鐢诲儚鐨勬暟瀛椾汉鏂囨湇鍔＄瓥鐣�[J].涓�鍗庡尰瀛﹀浘涔︽儏鎶ユ潅蹇�,2019,28(12):58-61.\n",
      "\n",
      "[190]鍒樼倻.浣滀负鏁板瓧浜烘枃鍩虹��璁炬柦鐨勫浘涔﹂�嗭細浠庝笉鍙�鎴栫己鍒版棤鍙�鏇夸唬[J].鍥句功棣嗚�哄潧,2020,40(05):1-2.\n",
      "\n",
      "[191]璐烘櫒鑺�,寮犵��.鍥句功棣嗘暟瀛椾汉鏂囦紬鍖呴」鐩�瀹炶返[J].鍥句功棣嗚�哄潧,2020,40(05):3-9.\n",
      "\n",
      "[192]榛勬檽鐟�.宸磋渶姘戜織鏂囧寲鐨勬暟瀛椾汉鏂囦紶鎾�鐮旂┒[J].浼犳挱鍔涚爺绌�,2019,3(35):24-25.\n",
      "\n",
      "[193]寮犲叺.楂樻牎鍥句功棣嗗憳涓撲笟鍖栧缓璁炬帰绌禰J].鏁欒偛鐜颁唬鍖�,2019,6(99):289-290.\n",
      "\n",
      "[194]Taylor Arnold,Nicolas Ballier,Paula Liss贸n,Lauren Tilton. Beyond lexical frequencies: using R for text analysis in the digital humanities[J]. Springer Netherlands,2019,53(4).\n",
      "\n",
      "[195]Sarah Schulz,Nora Ketschik. From 0 to 10 million annotated words: part-of-speech tagging for Middle High German[J]. Springer Netherlands,2019,53(4).\n",
      "\n",
      "[196]Jonas Kuhn. Computational text analysis within the Humanities: How to combine working practices from the contributing fields?[J]. Springer Netherlands,2019,53(4).\n",
      "\n",
      "[197]Aynat Rubinstein. Historical corpora meet the digital humanities: the Jerusalem Corpus of Emergent Modern Hebrew[J]. Springer Netherlands,2019,53(4).\n",
      "\n",
      "[198]Erhard Hinrichs,Marie Hinrichs,Sandra K眉bler,Thorsten Trippel. Language technology for digital humanities: introduction to the special issue[J]. Springer Netherlands,2019,53(4).\n",
      "\n",
      "[199]W.X. Wilcke,V. de Boer,M.T.M. de Kleijn,F.A.H. van Harmelen,H.J. Scholten. User-centric pattern mining on knowledge graphs: An archaeological case study[J]. Elsevier B.V.,2019,59.\n",
      "\n",
      "[200]Mariano Rico,Daniel Vila-Suero,Iuliana Botezan,Asunci贸n G贸mez-P茅rez. Evaluating the impact of semantic technologies on bibliographic systems: A user-centred and comparative approach[J]. Elsevier B.V.,2019,59.\n",
      "\n",
      "[201]. Institutes for Advanced Topics in the Digital Humanities (NEH)[J]. Federal Grants & Contracts,2019,44(1).\n",
      "\n",
      "[202]Steffen Roth,Harry F. Dahms,Frank Welz,Sandro Cattacin. Print theories of computer societies. Introduction to the digital transformation of social theory[J]. Elsevier Inc.,2019,149.\n",
      "\n",
      "[203]Gidal,Gavin. Infrastructural semantics: postal networks and statistical accounts in Scotland, 1790鈥�1845[J]. Taylor & Francis,2019,33(12).\n",
      "\n",
      "[204]Moncla,Gaio,Joliveau,Le Lay,Boeglin,Mazagol. Mapping urban fingerprints of odonyms automatically extracted from French novels[J]. Taylor & Francis,2019,33(12).\n",
      "\n",
      "[205]. Indigenous Approaches to Climate Resilience: A Roundtable Discussion with the Digital Environmental Humanities Lab[J]. University of Nebraska Press,2019,7(1).\n",
      "\n",
      "[206]寮犳檽鏄�. 鏁板瓧浜烘枃鏃朵唬灏嗘潵涓碵N]. 鍗椾含鏃ユ姤,2019-11-29(B06).\n",
      "\n",
      "[207]Purves Ross S,Winter Stephan,Kuhn Werner. Places in Information Science.[J]. Pubmed,2019,70(11).\n",
      "\n",
      "[208]鏋楁辰鏂�,娆х煶鐕�.鍩轰簬鍦ㄧ嚎鐧剧�戠殑澶ц�勬ā浜虹墿绀句細缃戠粶鎶藉彇涓庡垎鏋怺J].涓�鍥藉浘涔﹂�嗗�︽姤,2019,45(06):100-118.\n",
      "\n",
      "[209]娼樼帴鏂�. 娣卞寲鏁板瓧浜烘枃鐮旂┒[N]. 涓�鍥界ぞ浼氱�戝�︽姤,2019-11-25(001).\n",
      "\n",
      "[210]Sarah Werner,浣曚簹涓�,鏈卞惈闆�,鑲栭箯.褰撳疄浣撲功鏂囧寲閬囦笂鏁板瓧浜烘枃[J].鍥句功棣嗚�哄潧,2020,40(01):73-77+165.\n",
      "\n",
      "[211]鏋楀己,鏉庢収瀛�.楂樻牎鍥句功棣嗘暟瀛椾汉鏂囨湇鍔＄瓥鐣ョ爺绌禰J].浼犳挱鍔涚爺绌�,2019,3(33):291-292.\n",
      "\n",
      "[212]璧靛溅鏄�.鈥滅��涓�灞婁笢鍖楀湴鍖烘。妗堝�︽湳鐮旇�ㄤ細鈥濆湪杈藉畞澶у�﹀彫寮�[J].妗ｆ�堝�﹂�氳��,2019(06):108.\n",
      "\n",
      "[213]鏉庢瞾钃�.鍖诲�﹀崥鐗╅�嗙殑寤鸿�炬剰涔夊拰鍙戝睍鎺㈣�╗J].鍏板彴涓栫晫,2019(S2):48-49.\n",
      "\n",
      "[214]鏉庣偣.鏁板瓧浜烘枃鐨勭悊璁轰笌鏂规硶涔嬩簤[J].娴欐睙甯堣寖澶у�﹀�︽姤(绀句細绉戝�︾増),2019,44(06):1-5.\n",
      "\n",
      "[215]鍚存枃鑱�.鐢辩偣鍒伴潰锛岀敱褰㈠強绁炩�斺�斿熀浜庢暟瀛椾汉鏂囩殑3D鎵撳嵃鎶�鏈�鍦ㄥ姩鐢荤數褰变腑鐨勫簲鐢ㄧ爺绌惰秼鍔夸笌鏂瑰悜[J].鍔炲叕鑷�鍔ㄥ寲,2019,24(22):59-61.\n",
      "\n",
      "[216]闊╄鲍鍝�.鍥藉�栨暟瀛椾汉鏂囬」鐩�鐗圭偣鍒嗘瀽鍙婂惎绀篬J].鍥句功棣�,2019(11):18-23.\n",
      "\n",
      "[217]浠橀箯,鍠绘�ｅか,鏉庤壋.缇庡浗19鎵�楂樻牎鍥句功棣嗘暟瀛椾汉鏂囩┖闂磋皟鏌ヤ笌鍒嗘瀽[J].鍥句功棣�,2019(11):24-29.\n",
      "\n",
      "[218]闊╁畤,鐜嬭暰,鍙舵箘.寰藉窞鏂囦功鏁版嵁搴撳缓璁剧殑鐜扮姸涓庡彂灞曡秼鍔縖J].楂樻牎鍥句功棣嗗伐浣�,2019,39(06):54-60+68.\n",
      "\n",
      "[219]. From postcard to book cover: illustrating connections between medical history and digital humanities[J]. Medical Library and Historical Journal,2019,107(4).\n",
      "\n",
      "[220]鐜嬪悜濂�,琚佸��. 璁哄綋浠ｆ。妗堝�﹀湪鏁板瓧浜烘枃鐑�娼�涓嬬殑鐞嗘�х┖闂碵C]. 涓�鍥芥。妗堝�︿細.2019骞村叏鍥介潚骞存。妗堝�︽湳璁哄潧璁烘枃闆�.涓�鍥芥。妗堝�︿細:涓�鍥芥。妗堝�︿細,2019:101-107.\n",
      "\n",
      "[221]鍒樺吂鎭�,钁ｈ垶鑹�.楂樻牎鍥句功棣嗗弬涓庢暟瀛椾汉鏂囩殑涓�浜涙�濊�僛J].澶у�﹀浘涔︽儏鎶ュ�﹀垔,2019,37(06):3-6.\n",
      "\n",
      "[222]闊╄鲍鍝�.浜烘枃瀛︾�戝�︾敓瀵规暟瀛椾汉鏂囩殑璁ょ煡璋冩煡鈥斺�斾互鍖椾含澶у�︽枃瀛︺�佸彶瀛︽湰绉戠敓涓轰緥[J].澶у�﹀浘涔︽儏鎶ュ�﹀垔,2019,37(06):81-85.\n",
      "\n",
      "[223]閮�鑻卞墤. 鑰堕瞾澶у�︼細鏁板瓧浜烘枃瀹為獙瀹ょ殑鍓嶄笘浠婄敓[N]. 涓�鍥界�戝�︽姤,2019-11-06(007).\n",
      "\n",
      "[224]Stan Lipovetsky. Digital Humanities and Film Studies: Visualising Dziga Vertov's Work[J]. Technometrics,2019,61(4).\n",
      "\n",
      "[225]鏉ㄦ檽娉�,鍟嗘�濅匠.鈥滄暟瀛椾汉鏂団�濊儗鏅�涓婱TI鏁欏�︾殑鈥滄暟瀛楀寲+娣卞害缈昏瘧鈥濇ā寮忔帰绱�[J].鏁欒偛鐜颁唬鍖�,2019,6(88):201-203.\n",
      "\n",
      "[226]. Toward a New Complicity for New Media[J]. Pennsylvania State University Press,2019,56(4).\n",
      "\n",
      "[227]鏈卞叞鍏�,钖勭敯闆�.鏁板瓧浜烘枃瑙嗗煙涓嬪�惰氨妗ｆ�堣祫婧愬�氬厓鍖栧紑鍙慬J].娴欐睙妗ｆ��,2019(10):31-33.\n",
      "\n",
      "[228]璧垫椽娉�,缃楃幉,鏉庡ぇ鑾�.鏁板瓧浜烘枃瑙嗗煙涓嬫櫤鎱у浘涔﹂�嗗缓璁剧殑妯″紡涓庤矾寰刐J].鍥句功棣嗗�﹀垔,2019,41(09):110-113+122.\n",
      "\n",
      "[229]旯�靹彪��,旯�氚旊��. 霐旍��韯� 攵勳劃 旮半矔鞚� 頇滌毄頃� 鞁滌“氍疙暀 鞐瓣惮 - 鞚戈车歆�電�(AI) 鞝滌灛 彀届瀾 鞁滌“毳� 欷戩嫭鞙茧�� -[J]. 頃滉淡氍疙檾鞙淀暕頃欗殞,2019,41(5).\n",
      "\n",
      "[230]鍚存壙鏂�,褰�瀵肩惁,楂樻ⅵ鐩�.鏁板瓧浜烘枃瑙嗚�掍笅鍩庡競澹伴煶璁板繂寮�鍙戠瓥鐣ヤ笌浠峰�煎疄鐜帮細浠ヨタ瀹夊競涓轰緥[J].鍥句功棣嗙悊璁轰笌瀹炶返,2019(10):105-109+112.\n",
      "\n",
      "[231]Adam Mazel,Catherine Dumas. Computational analyses of the relation of theme and genre in the writing of aphra behn[J]. John Wiley & Sons, Inc.,2019,56(1).\n",
      "\n",
      "[232]Xiaoguang Wang,Xu Tan,Qingyu Duan. Enhancing scholar supportive data: Surveying the landscape of information resources for digital dunhuang[J]. John Wiley & Sons, Inc.,2019,56(1).\n",
      "\n",
      "[233]Jenna Jordan. Put relational databases in your data curation toolbox[J]. John Wiley & Sons, Inc.,2019,56(1).\n",
      "\n",
      "[234]Xiaojuan Xu,Manli Wu. Using crowdsourcing to construct special collection resources under the perspective of digital humanities[J]. John Wiley & Sons, Inc.,2019,56(1).\n",
      "\n",
      "[235]Dan Wu,Shuang Xu,Xiaoyun Xu,Xi Chen. Users' visual attention flow on the search result page of digital cultural heritage collection[J]. John Wiley & Sons, Inc.,2019,56(1).\n",
      "\n",
      "[236]Eugenia S. Kim. Lithium hindsight 360: Increasing access and ensuring preservation of virtual reality narratives[J]. John Wiley & Sons, Inc.,2019,56(1).\n",
      "\n",
      "[237]閮戣晩.鍩轰簬灏忕▼搴忕殑楂樻牎鍥句功棣嗕釜鎬у寲鏈嶅姟娣卞寲鐮旂┒[J].鐭ヨ瘑绠＄悊璁哄潧,2019,4(05):310-319.\n",
      "\n",
      "[238]鐗涘姏,鍒樻収鐞�,鏇鹃潤鎬�,闊╁皬姹�.鏁板瓧鏃朵唬妗ｆ�堣祫婧愬紑鍙戝埄鐢ㄧ殑閲嶆柊瀹¤�哰J].妗ｆ�堝�︾爺绌�,2019(05):67-71.\n",
      "\n",
      "[239]缃楅摽.鏁板瓧浜烘枃鑳屾櫙涓嬩鲸鎵规。妗堣祫婧愮殑寮�鍙戞ā寮忕爺绌禰J].妗ｆ�堝�︾爺绌�,2019(05):83-87.\n",
      "\n",
      "[240]绋嬪崼鍗�,鏉庡唹.娉曞浗鍥藉�跺浘涔﹂�嗘暟瀛椾汉鏂囨湇鍔＄殑鍚�绀篬J].灞变笢鍥句功棣嗗�﹀垔,2019(05):108-113.\n",
      "\n",
      "[241]澶忕繝濞�.鏂囧寲璁板繂瑙嗗煙涓嬪�惰氨鏂囩尞浠峰�肩殑鍐嶈�よ瘑鍜屽唴瀹圭殑娣卞紑鍙慬J].鍥句功涓庢儏鎶�,2019(05):49-57.\n",
      "\n",
      "[242]鑺︽檽鍗�,鏉ㄦ檽娉�.涓�鍥藉吀绫嶇殑鈥滄繁搴︾炕璇戔�濇ā寮忔帰绌垛�斺�斾互銆婅�存枃瑙ｅ瓧鍙欍�嬩负渚媅J].鑻辫��骞垮満,2019(11):14-16.\n",
      "\n",
      "[243]鐜嬭吹娴�.鎴戝浗鏁板瓧浜烘枃鐮旂┒婕旇繘璺�寰勫強鍥句功棣嗘敮鎸佺瓥鐣ユ帰鏋怺J].鍥句功棣嗗伐浣滀笌鐮旂┒,2019(10):106-113.\n",
      "\n",
      "[244]Leonard Neidorf,Yi Zhao,Jie Yu. Line Length in Old English Poetry: A Chronological and Stylistic Criterion[J]. Springer Netherlands,2019,103(4).\n",
      "\n",
      "[245]閲戝�剁惔,澶忕繝濞�.鏁板瓧浜烘枃鏁版嵁鍩虹��璁炬柦寤鸿�句腑鏈烘瀯鏈�浣撶殑鏋勫缓锛氱爺绌跺拰搴旂敤[J].鍥句功棣嗚�哄潧,2020,40(04):30-39.\n",
      "\n",
      "[246]寰愬瓭濞�,浣曟櫒鏅�,鏉ㄧ埥,浠囨澃.娓告垙鍖栨ā寮忓�规暟瀛椾汉鏂囩被浼楀寘椤圭洰杩愪綔鐨勫惎绀篬J].榛勫北瀛﹂櫌瀛︽姤,2019,21(05):3-7.\n",
      "\n",
      "[247]濮滄枃娑�.浣滀负涓�绉嶆枃瀛︾爺绌舵柟娉曠殑鏁板瓧浜烘枃鈥斺�斿嵃鍒锋枃鍖栧熀纭�璁炬柦锛�20涓栫邯鏂囧�︽壒璇勫彶锛屼互鍙婃枃瀛︾ぞ浼氬��[J].涓�鍥芥瘮杈冩枃瀛�,2019(04):29-47.\n",
      "\n",
      "[248]璧佃枃.浠庢�傚康妯″瀷鍒拌�＄畻鎵硅瘎:鏁板瓧鏃朵唬鐨勨�滀笘鐣屾枃瀛︹�濈爺绌禰J].涓�鍥芥瘮杈冩枃瀛�,2019(04):48-66.\n",
      "\n",
      "[249]璧佃枃.瀛︽湳鍓嶆部:姣旇緝瑙嗛噹涓�鐨勬暟瀛椾汉鏂嘯J].涓�鍥芥瘮杈冩枃瀛�,2019(04):1-2.\n",
      "\n",
      "[250]闄堥潤.澶嶆暟鐨勬暟瀛椾汉鏂団�斺�旀瘮杈冭�嗛噹涓嬬殑涓�鍥芥暟瀛椾汉鏂嘯J].涓�鍥芥瘮杈冩枃瀛�,2019(04):14-28.\n",
      "\n",
      "[251]鐧戒簯.椹�鍏嬫�濊憲浣滃湪鎴戝浗鑹烘湳瀛︾爺绌朵腑鐨勫�︽湳褰卞搷鍔涘垎鏋愨�斺�斿熀浜嶤SSCI(1998鈥�2016)鏁版嵁[J].鍖椾含鑱斿悎澶у�﹀�︽姤(浜烘枃绀句細绉戝�︾増),2019,17(04):45-53.\n",
      "\n",
      "[252]鏉ㄥ崈.鏁板瓧浜烘枃瑙嗗煙涓嬫垜鍥芥。妗堣祫婧愬悎浣滃紑鍙戞ā寮忕爺绌禰J].妗ｆ�堜笌寤鸿��,2019(10):8-12.\n",
      "\n",
      "[253]寰愭枌. 琛ㄥ崡灞变负闃�:鍩轰簬鍦版柟蹇楁暟鎹�搴撶殑鍙や唬鍩庡競瑙勫垝鏂规硶鐮旂┒[C]. 涓�鍥藉煄甯傝�勫垝瀛︿細銆侀噸搴嗗競浜烘皯鏀垮簻.娲诲姏鍩庝埂 缇庡ソ浜哄眳鈥斺��2019涓�鍥藉煄甯傝�勫垝骞翠細璁烘枃闆嗭紙04鍩庡競瑙勫垝鍘嗗彶涓庣悊璁猴級.涓�鍥藉煄甯傝�勫垝瀛︿細銆侀噸搴嗗競浜烘皯鏀垮簻:涓�鍥藉煄甯傝�勫垝瀛︿細,2019:7-24.\n",
      "\n",
      "[254]钄¤繋鏄�. 姘戝浗鏃舵湡鏂囩尞鐩�褰曟暟鎹�骞冲彴鐨勬瀯寤轰笌瀹炶返[C]. 涓�鍥界储寮曞�︿細銆佸�嶆棪澶у�﹀浘涔﹂��.2019骞翠腑鍥界储寮曞�︿細骞翠細鏆ㄥ�︽湳鐮旇�ㄤ細璁烘枃闆�.涓�鍥界储寮曞�︿細銆佸�嶆棪澶у�﹀浘涔﹂��:涓�鍥界储寮曞�︿細,2019:128-135.\n",
      "\n",
      "[255]. Science - Library Science; Researchers from National Chengchi University Report Recent Findings in Library Science (Development and Evaluation of an Automatic Text Annotation System for Supporting Digital Humanities Research)[J]. Computer Technology Journal,2019.\n",
      "\n",
      "[256]Ewing E Thomas,Randall Katherine,Reznick Jeffrey S. From postcard to book cover: illustrating connections between medical history and digital humanities.[J]. Pubmed,2019,107(4).\n",
      "\n",
      "[257]Ilona Pikkanen. The metrics and poetics of historical drama[J]. Orbis Litterarum,2019,74(5).\n",
      "\n",
      "[258]閭卞缓鍗�.Omeka绯荤粺鍦ㄦ暟瀛椾汉鏂囩爺绌朵腑鐨勫簲鐢ㄥ墫鏋怺J].鎯呮姤鎺㈢储,2019(10):104-109.\n",
      "\n",
      "[259]鐜嬪啝缇�.浜斿洓鏃舵湡鍏堣繘鐭ヨ瘑鍒嗗瓙閫夋嫨椹�鍏嬫�濅富涔夌殑鏁板瓧浜烘枃鑰冭瘉鈥斺�斾互銆婃柊闈掑勾銆嬫潅蹇椾负渚媅J].椹�鍏嬫�濅富涔夌悊璁哄�︾�戠爺绌�,2019,5(05):78-87.\n",
      "\n",
      "[260]瀹�骞�.OMEKA鍦ㄥ浘涔﹂�嗘暟瀛椾汉鏂囬」鐩�涓�鐨勫簲鐢╗J].鍥句功棣嗙爺绌朵笌宸ヤ綔,2019(10):92-95.\n",
      "\n",
      "[261]Nektaria Potha,Efstathios Stamatatos. Improving author verification based on topic modeling[J]. John Wiley & Sons, Inc.,2019,70(10).\n",
      "\n",
      "[262]Sonia H Stephens. A narrative approach to interactive information visualization in the digital humanities classroom[J]. SAGE Publications,2019,18(4).\n",
      "\n",
      "[263]Viola,Verheul. The Media Construction of Italian Identity: A Transatlantic, Digital Humanities Analysis of italianit脿 , Ethnicity, and Whiteness, 1867-1920[J]. Routledge,2019,19(4).\n",
      "\n",
      "[264]Pamella R. Lach,Elizabeth A. Pollard. Visualizing History in the Classroom: A Faculty-Librarian Partnership in the Digital Age[J]. Routledge,2019,25(2-4).\n",
      "\n",
      "[265]James W. Malazita,Korryn Resetar. Infrastructures of abstraction: how computer science education produces anti-political subjects[J]. Routledge,2019,30(4).\n",
      "\n",
      "[266]Rahul K. Gairola. Tools to dismantle the Master鈥檚 DH(ouse): towards a genealogy of Partition, digital necropolitics and Bollywood cinema<xref ref-type=\"fn\" rid=\"FN0000\"/>[J]. Routledge,2019,22(4).\n",
      "\n",
      "[267]Niels Br眉gger,Ian Milligan,Anat Ben-David,Sophie Gebeil,Federico Nanni,Richard Rogers,William J. Turkel,Matthew S. Weber,Peter Webster. Internet histories and computational methods: a 鈥渞ound-doc鈥� discussion[J]. Routledge,2019,3(3-4).\n",
      "\n",
      "[268]Stan Lipovetsky. Digital Humanities and Film Studies: Visualising Dziga Vertov鈥檚 Work[J]. Taylor & Francis,2019,61(4).\n",
      "\n",
      "[269]Darren Purcell,Cayton Moore. Selling Southern Places: An Examination of Delta's Sky Magazine City Profiles.[J]. Southeastern Geographer,2019,59(3).\n",
      "\n",
      "[270]. A Digital Edition between Stylometry and OCR[J]. Indiana University Press,2019,12(2).\n",
      "\n",
      "[271]. #DigEarlyAm: Reflections on Digital Humanities and Early American Studies[J]. Omohundro Institute of Early American History and Culture,2019,76(4).\n",
      "\n",
      "[272]閭撳悰,瀹嬪厛鏅�,閽熸�氫緷.鎴戝浗鏁板瓧浜烘枃棰嗗煙鐮旂┒鐑�鐐瑰強鍓嶆部鎺㈡瀽[J].鐜颁唬鎯呮姤,2019,39(10):154-164.\n",
      "\n",
      "[273]鏈卞环鍔�.鈥滀汉宸ユ櫤鑳芥椂浠ｄ笅鏁板瓧浜烘枃鐮旂┒鎺㈡柊鈥濅笓棰樼畝浠媅J].蹇冪悊鎶�鏈�涓庡簲鐢�,2019,7(10):577.\n",
      "\n",
      "[274]鑲栭箯,濮氭�氭櫀.璧板悜鏁板瓧鏃朵唬鐨勪汉鏂囧�﹁��:娉汱IS瑙嗗煙涓嬬殑杩涘睍涓庡弽鎬漑J].鏂囩尞涓庢暟鎹�瀛︽姤,2019,1(03):18-25.\n",
      "\n",
      "[275]靻§澑鞛�. 頃滉淡 臧滊厫靷�鞚� 鞚措�犾爜 韮愳儔鞐� 雽�頃� 瓿犾鞍瓿� 鞝勲�漑J]. 瓿犽牑雽�頃欔祼 頃滉淡頃欖棸甑�靻�,2019,70.\n",
      "\n",
      "[276]鞓る倶鞓�,鞚措炒鞎�. 歃濌皶順勳嫟旮办垹鞚� 鞙淀暕霅� 鞁滊�疙暀 響滍槃 鞐瓣惮[J]. 頃滉淡鞝勳嫓靷办梾鞙淀暕鞐瓣惮鞗�,2019,37(4).\n",
      "\n",
      "[277]澶忓ぉ.妗ｆ�堜俊鎭�鍜ㄨ��浜掑姩鏁版嵁闆嗘瀯寤轰笌鍒嗘瀽[J].娴欐睙妗ｆ��,2019(09):26-29.\n",
      "\n",
      "[278]寮犳��.鏁板瓧浜烘枃鐨勫伐鍏峰睘鎬х爺绌禰J].鍥句功棣嗙爺绌�,2019,49(05):19-23.\n",
      "\n",
      "[279]闊╀寒,閮庣瓲.鍦版柟闄㈡牎鍥句功棣嗙壒鑹茶祫婧愬缓璁撅細鐜扮姸銆侀棶棰樹笌灞曟湜鈥斺�斾互瑗垮崕甯堣寖澶у�﹀浘涔﹂�嗕负鐮旂┒瀵硅薄鐨勫弽鎬漑J].鍥句功棣嗙爺绌�,2019,49(05):62-69.\n",
      "\n",
      "[280]鏉ㄥ缓鏋�,鑻楄暰.鎯呮姤瀛﹀�︾�戝缓璁鹃潰涓寸殑涓昏�侀棶棰樹笌鍙戝睍鏂瑰悜[J].绉戞妧鎯呮姤鐮旂┒,2019,1(01):29-50.\n",
      "\n",
      "[281]鏉滅�ユ��.鍩轰簬鏁板瓧浜烘枃鐨勬。妗堜俊鎭�璧勬簮浼犳挱鏂版�濊�僛J].鍏板彴鍐呭��,2019(27):14-16.\n",
      "\n",
      "[282]Karin Priem,Lynn Fendler. Shifting epistemologies for discipline and rigor in educational research: Challenges and opportunities from digital humanities[J]. SAGE Publications,2019,18(5).\n",
      "\n",
      "[283]楂樼懢.閲忓寲鏁板瓧浜烘枃缁艰堪[J].鍥句功棣嗚�哄潧,2020,40(01):54-72.\n",
      "\n",
      "[284]鏉庣惉瀹�,璐洪害鏅�,寮犲己.鏂囧�﹀彶銆佸嵃鍒锋枃鍖栦笌鏁扮爜浜烘枃[J].涓栫晫鍗庢枃鏂囧�﹁�哄潧,2019(03):38-41.\n",
      "\n",
      "[285]鏉ㄦ枃.鏁板瓧浜烘枃瑙嗛槇涓嬬殑绀句細璁板繂鏋勫缓鐮旂┒[J].鎯呮姤璧勬枡宸ヤ綔,2019,40(05):38-45.\n",
      "\n",
      "[286]璧佃穬. 鏁板瓧浜烘枃瑙嗗煙涓嬫。妗堜俊鎭�璧勬簮寮�鍙戞ā寮忔瀯寤篬N]. 灞辫タ绉戞妧鎶�,2019-09-24(A06).\n",
      "\n",
      "[287]F. McGee,M. Ghoniem,G. Melan莽on,B. Otjacques,B. Pinaud. The State of the Art in Multilayer Network Visualization[J]. Computer Graphics Forum,2019,38(6).\n",
      "\n",
      "[288]璧靛唹.鏁板瓧浜烘枃鑳屾櫙涓嬫櫤鎱у浘涔﹂�嗙殑寤鸿�炬ā寮忎笌绛栫暐鐮旂┒[J].澶т紬鏍囧噯鍖�,2019(12):8+10.\n",
      "\n",
      "[289]鐭冲織鏉�.娆ф床鐮旂┒鍥句功棣嗗崗浼氭暟瀛椾汉鏂囧彂灞曠瓥鐣ユ帰鏋怺J].澶у�﹀浘涔﹂�嗗�︽姤,2019,37(05):24-31.\n",
      "\n",
      "[290]寮犵拠,瀛熺ゥ淇�.闈㈠悜鏁板瓧浜烘枃鐨勯珮鏍℃暟鎹�绱犲吇鏁欒偛妗堜緥鐮旂┒[J].澶у�﹀浘涔﹂�嗗�︽姤,2019,37(05):87-94.\n",
      "\n",
      "[291]璁搁泤鐐�.鏁板瓧浜烘枃鑳屾櫙涓嬪浘涔﹂�嗛槄璇绘帹骞跨瓥鐣ユ帰鏋怺J].鍥句功棣嗗伐浣滀笌鐮旂┒,2019(S1):42-45.\n",
      "\n",
      "[292]鐜嬫弧鏄�.鎴戝浗妗ｆ�堥�嗗紑灞曟暟瀛椾汉鏂囬」鐩�鐨勭瓥鐣ユ帰绌禰J].榛戞渤瀛﹀垔,2020(02):164-166.\n",
      "\n",
      "[293]寮犳枌,鏉庡瓙鏋�.鏁板瓧浜烘枃鑳屾櫙涓嬫。妗堥�嗗彂灞曠殑鏂版�濊�僛J].鍥句功鎯呮姤鐭ヨ瘑,2019(06):68-76.\n",
      "\n",
      "[294]缈熸枃鏁�.VR鏃朵唬楂樻牎鍥句功棣嗘湇鍔℃�濈淮鎺㈢储[J].绉戞妧鏂囩尞淇℃伅绠＄悊,2019,33(03):38-40.\n",
      "\n",
      "[295]鏉庢椂闆�.鏂版椂浠ｅ浘涔﹂�嗘暟瀛椾汉鏂囧彂灞曟帰璁╗J].涓�鍥界�＄悊淇℃伅鍖�,2019,22(18):183-184.\n",
      "\n",
      "[296]鐜嬩附绾�.鏁板瓧浜烘枃鑳屾櫙鍜屽浘涔﹂�嗙粡鍏搁槄璇绘帹骞挎湇鍔＄殑杞�鍨媅J].鏂囨暀璧勬枡,2019(26):112-113.\n",
      "\n",
      "[297]Francesca Giannetti. 鈥楽o near while apart鈥�: Correspondence Editions as Critical Library Pedagogy and Digital Humanities Methodology[J]. Elsevier Inc.,2019,45(5).\n",
      "\n",
      "[298]Christoph K. Streb,Thomas Kolnberger,Sonja Kmec. The material culture of burial and its microgeography: A Luxembourg cemetery as a methodological example of an object-centred approach to quantitative material culture studies[J]. SAGE Publications,2019,24(3).\n",
      "\n",
      "[299]Anke Finger,Niko Tracksdorf. Go Global, Build Networks, Create Nodes: Integrating the Humanities and the Professions[J]. Die Unterrichtspraxis/Teaching German,2019,52(2).\n",
      "\n",
      "[300]鏋楁辰鏄�,鑳¤繜,鏅忓疄姹�.寰藉窞姘寸郴鏂囧寲鍦板浘鐗硅壊鏁版嵁搴撴瀯寤烘帰鏋怺J].澶у�﹀浘涔︽儏鎶ュ�﹀垔,2019,37(05):16-19.\n",
      "\n",
      "[301]鐜嬪厗楣�,閮戞案鏅�,鍒樹含鑷�.鍊熷櫒涔嬪娍锛屽嚭閬撲箣鏂扳�斺�斺�滄暟瀛椾汉鏂団�濇氮娼�涓嬬殑鍙ゅ吀鏂囧�︾爺绌朵笁浜鸿皥[J].鏂囪壓鐮旂┒,2019(09):79-88.\n",
      "\n",
      "[302]鍒樻収鐞�.鏁板瓧浜烘枃锛氭暟鎹�鏃朵唬涓嬬殑鏂囧寲淇濇姢涓庝紶鎵縖J].灞辫タ妗ｆ��,2019(05):72-79.\n",
      "\n",
      "[303]姹�瓒�.澶氶噸缃戠粶涓�鐨勮瘝瀛︾爺绌禰J].娴欐睙澶у�﹀�︽姤(浜烘枃绀句細绉戝�︾増),2019,49(05):204-209.\n",
      "\n",
      "[304]Roopika Risam. Beyond the Migrant 鈥淧roblem鈥�: Visualizing Global Migration[J]. SAGE Publications,2019,20(6).\n",
      "\n",
      "[305]鍒樿姰,璋�蹇呭媷.鍥藉�栨暟瀛椾汉鏂囧湴鍥鹃」鐩�鐨勫疄璺典笌鍚�绀篬J].灞辫タ妗ｆ��,2019(05):87-93.\n",
      "\n",
      "[306]鏈辨�濊嫅,鍗㈢珷骞�.浜烘枃瀛﹁�呭�︽湳鐮旂┒鐨勬暟瀛楄兘鍔涚幇鐘跺垎鏋怺J].鍥句功鎯呮姤宸ヤ綔,2019,63(17):84-92.\n",
      "\n",
      "[307]Patricia Martin-Rodilla,Cesar Gonzalez-Perez. Metainformation scenarios in Digital Humanities: Characterization and conceptual modelling strategies[J]. Elsevier Ltd,2019,84.\n",
      "\n",
      "[308]鏂界帴棣�,鐜嬬帀鐝�,鏉庡瓙鏋�.鏁板瓧浜烘枃鍙橀潻瀵规。妗堥�嗕笟鍔℃椿鍔ㄧ殑褰卞搷[J].灞辫タ妗ｆ��,2019(05):80-86.\n",
      "\n",
      "[309]Jacobs,Tsch枚tschel. Topic models meet discourse analysis: a quantitative tool for a qualitative approach[J]. Routledge,2019,22(5).\n",
      "\n",
      "[310]濮氬ぉ娉�,闄堣壋姊�,鍒橀潻,椴佽秴.鍩轰簬CIDOC-CRM鐨勬暟瀛椾汉鏂囧彶鏂欒祫婧愯��涔夊寲鐭ヨ瘑缁勭粐鐮旂┒鈥斺�斾互寮犲�﹁壇鍙叉枡璧勬簮涓轰緥[J].鍥句功棣嗗�﹀垔,2019,41(07):35-43.\n",
      "\n",
      "[311]Winters,Prescott. Negotiating the born-digital: a problem of search[J]. Routledge,2019,47(3).\n",
      "\n",
      "[312]Elena Marchevska. Digital Bodies: Creativity and Technology in the Arts and Humanities[J]. Routledge,2019,15(3).\n",
      "\n",
      "[313]Gooding,Smith,Mann. The forensic imagination: interdisciplinary approaches to tracing creativity in writers鈥� born-digital archives[J]. Routledge,2019,47(3).\n",
      "\n",
      "[314]旯�鞓侂！. 鈥橅寣鞖办姢韸胳潣 韮勳嫕鈥欔臣 頂缄惮霛� - 毵れ泊靷�鞝� 甏�鞝愳棎靹� 氚旊澕氤� 霐旍��韯� 鞁滊寑 氍胳槇頃欖潣 臧�電レ劚[J]. 頃滉淡鞕戈淡鞏措寑頃欔祼 鞕戈淡氍疙暀鞐瓣惮靻�,2019(75).\n",
      "\n",
      "[315]闄堟稕,鍒樼倻,鍗曡搲钃�,鏈卞簡鍗�.鐭ヨ瘑鍥捐氨鍦ㄦ暟瀛椾汉鏂囦腑鐨勫簲鐢ㄧ爺绌禰J].涓�鍥藉浘涔﹂�嗗�︽姤,2019,45(06):34-49.\n",
      "\n",
      "[316]闄堥��,鑸掕搲.鏁板瓧浜烘枃鑳屾櫙涓嬮珮鏍″浘涔﹂�嗗彂灞曠瓥鐣ョ爺绌禰J].鍐呰挋鍙ょ�戞妧涓庣粡娴�,2019(16):144-146.\n",
      "\n",
      "[317]J茅r茅mie Pelletier-Gagnon,Geoffrey Rockwell. The Replaying Japan Conference: Bringing Japan GameStudies and Digital Humanities[J]. Japanese Association for Digital Humanities,2019,4(1).\n",
      "\n",
      "[318]寮犵編鑺�.闈㈠悜鏁板瓧浜烘枃鐨勫０鍍忔。妗堜俊鎭�璧勬簮缁勭粐鍒╃敤鐨勭爺绌禰J].妗ｆ�堝�︾爺绌�,2019(04):72-76.\n",
      "\n",
      "[319]鐜嬩綑鍏�,琛℃槑鏄�.2018骞存枃鐚�瀛︾爺绌惰繘灞昜J].灞变笢鍥句功棣嗗�﹀垔,2019(04):14-23.\n",
      "\n",
      "[320]Daniel Torres-Salinas,Esteban Romero-Fr铆as,Wenceslao Arroyo-Machado. Mapping the backbone of the Humanities through the eyes of Wikipedia[J]. Elsevier Ltd,2019,13(3).\n",
      "\n",
      "[321]Johanna Barzen,Frank Leymann. Quantum humanities: a vision for quantum computing in digital humanities[J]. Springer Berlin Heidelberg,2019(prepublish).\n",
      "\n",
      "[322]钄¤繋鏄�. 缇庡浗鍥句功棣嗘満鍒躲�佽祫婧愪笌鏈嶅姟鐨勪紶鎵垮拰鍒涙柊[N]. 鏂板崕涔︾洰鎶�,2019-08-23(018).\n",
      "\n",
      "[323]鐢扮嚂椋�,鐩涘皬骞�.缇庡浗楂樻牎鍥句功棣嗘暟瀛椾汉鏂囨湇鍔＄爺绌跺強鍚�绀篬J].鍥句功棣嗗伐浣滀笌鐮旂┒,2019(08):32-40.\n",
      "\n",
      "[324]鐜嬫柊闆�.闈㈠悜鏁板瓧浜烘枃鐨勫浘涔﹂�嗙煡璇嗘湇鍔℃ā寮忕爺绌禰J].鍥句功棣嗗伐浣滀笌鐮旂┒,2019(08):71-76.\n",
      "\n",
      "[325]. Passing Through[J]. Pennsylvania State University Press,2019,29(1).\n",
      "\n",
      "[326]鍒樹咕鍑�.闈㈠悜鏁板瓧浜烘枃鐨勯兘甯傚啘涓氭湰浣撶殑鏋勫缓[J].鍥句功棣嗘潅蹇�,2019,38(08):53-58.\n",
      "\n",
      "[327]鐜嬩箣绾�,瀛欑憸,鏉ㄦ磱,寰愬�呮湪,椹�绱�妗�.鍩轰簬涓�鍥戒紶缁熻壓鏈�椋庢牸鐨勬暟瀛椾汉鏂囨櫙瑙備綋楠岃�捐�♀�斺�斾互2019鍖椾含涓栧洯浼氫腑鍥介�嗏�滅ゥ鍜岄�稿眳鈥濆睍椤硅�捐�′负渚媅J].瑁呴グ,2019(08):106-109.\n",
      "\n",
      "[328]榻愰搧妤�.鏁板瓧浜烘枃涓庣數瀛愭枃浠剁�＄悊闂�棰樼殑鑻ュ共鎬濊�僛J].榛戦緳姹熸。妗�,2019(04):45.\n",
      "\n",
      "[329]鐜嬩附涓�,鐜涗附鑹剧帥路鏍奸浄鍘勫��.鏁板瓧浜烘枃鏂规硶鐮旂┒鏂囧�︾殑鎸戞垬涓庢満閬囷細鏍奸浄鍘勫�嗘暀鎺堣�胯皥[J].澶栧浗璇�鏂囩爺绌�,2019,5(04):1-7.\n",
      "\n",
      "[330]鏈变寒浜�,浣熼洩钀�,瀹嬮攱妫�.绠�璋堜笓涓氬唴瀹硅祫婧愮殑涓撻�樻暟鎹�搴撳缓璁锯�斺�斿吋璋堝ぇ瀛﹀嚭鐗堢ぞ瀛︽湳鏁版嵁搴撳缓璁剧粡楠孾J].绉戞妧涓庡嚭鐗�,2019(08):87-90.\n",
      "\n",
      "[331]Valdivia Paola,Buono Paolo,Plaisant Catherine,Dufournaud Nicole,Fekete Jean-Daniel. Analyzing Dynamic Hypergraphs with Parallel Aggregated Ordered Hypergraph Visualization.[J]. Pubmed,2019.\n",
      "\n",
      "[332]浣曠帀鐠�.妗ｆ�堥�嗘暟瀛椾汉鏂囨灦鏋勪笌绛栫暐鐮旂┒[J].鍏板彴涓栫晫,2019(08):49-51.\n",
      "\n",
      "[333]璧电彏鐞�.浜烘枃绀句細绉戝�﹂�嗗煙缃戠粶璧勬簮瀛樻。鍒╃敤鐜扮姸缁艰堪[J].淇℃伅璧勬簮绠＄悊瀛︽姤,2019,9(03):33-40.\n",
      "\n",
      "[334]绗�绔犻毦,姹�铇�.浠ヨ�＄畻鐨勬柟娉曞弽瀵硅�＄畻鏂囧�︾爺绌禰J].灞变笢绀句細绉戝��,2019(08):24-39.\n",
      "\n",
      "[335]濮滄枃娑�,鎴村畨寰�.涓绘寔浜鸿�璠J].灞变笢绀句細绉戝��,2019(08):23-24.\n",
      "\n",
      "[336]闇嶄紛鐗孤锋湕,鑻忕湡,瀹夊痉椴伮锋淳鍗�,娉板痉路瀹夊痉浼嶅痉,椹�鍏嬄烽樋灏斿悏-浼戜紛鐗�,鍑�鐟熺惓路浼�寰�,绗�绔犻毦,姹�铇�.鎺ㄨ繘璁＄畻鏂囧�︾爺绌垛�斺�斿�圭��绔犻毦銆婁互璁＄畻鐨勬柟娉曞弽瀵硅�＄畻鏂囧�︾爺绌躲�嬩竴鏂囩殑璁ㄨ�篬J].灞变笢绀句細绉戝��,2019(08):40-55.\n",
      "\n",
      "[337]娌堟檽鑹�.鏁板瓧浜烘枃鏃朵唬鍏�鍏卞浘涔﹂�嗙粡鍏搁槄璇绘帹骞挎帰璁╗J].鍏板彴鍐呭��,2019(22):51-52.\n",
      "\n",
      "[338]. 鈥淭o Transcribe Them in a Fair and Legible Hand鈥�: Hartford Female Seminary's Handwritten Newspapers[J]. Pennsylvania State University Press,2019,40.\n",
      "\n",
      "[339]鏉ㄤ腹涓�.绗�鍥涙湡:鈥滄暟瀛椾汉鏂団�濈殑鍏磋捣涓庢枃瀛︾爺绌剁殑杞�鍚慬J].褰撲唬浣滃�惰瘎璁�,2019(04):209.\n",
      "\n",
      "[340]鐜嬫棴涓�.淇℃伅鍙插�﹀缓鏋勭殑璺ㄥ�︾�戞帰绱�[J].涓�鍥界ぞ浼氱�戝��,2019(07):159-185+208.\n",
      "\n",
      "[341]鏈辨収鏁�,鏉ㄦ矇.鏁板瓧浜烘枃棣嗗憳:缂樿捣銆佽�掕壊瀹氫綅鍙婅兘鍔涙瀯寤篬J].鍥句功棣嗗�︾爺绌�,2019(14):26-33.\n",
      "\n",
      "[342]鑲栧钩,妯婃尟浣�.闈㈠悜澶у�︾敓鍒涙柊鍒涗笟鐨勯珮鏍″浘涔﹂�嗘暟瀛椾汉鏂囨暀鑲叉湇鍔＄爺绌禰J].鍥句功棣嗗�︾爺绌�,2019(14):71-76.\n",
      "\n",
      "[343]Stacy Warren,Trevor Harris,Michael Goodchild,Patricia Sol铆s. Commentaries on 鈥淓valuating the Geographic in GIS 鈥漑J]. Geographical Review,2019,109(3).\n",
      "\n",
      "[344]寮犺��閾�.浜哄伐鏅鸿兘椹卞姩鐨勪汉鏂囩ぞ浼氱�戝�︾爺绌惰浆鍨媅J].娴庡崡澶у�﹀�︽姤(绀句細绉戝�︾増),2019,29(04):20-28+157.\n",
      "\n",
      "[345]閭辨簮,寰愭柟,閭辩珷涔�.鏁板瓧浜烘枃鐜�澧冧笅鈥滃畨寰藉洓澶ф枃鍖栧湀鈥濈爺绌禰J].鏂颁笘绾�鍥句功棣�,2019(07):59-64.\n",
      "\n",
      "[346]閮戠埥,涓佸崕涓�.鏁板瓧浜烘枃瀵规。妗堣�板繂鍔熻兘瀹炵幇鐨勫惎绀篬J].妗ｆ�堜笌寤鸿��,2019(07):23-26.\n",
      "\n",
      "[347]钁ｈ垶鑹�,姊佸叴鍫�.涓�鍥戒汉鏂囧�﹁�呭弬涓庢暟瀛椾汉鏂囧姩鏈虹殑浜屽厓缁撴瀯鍙婅�屼负璺�寰刐J].涓�鍥藉浘涔﹂�嗗�︽姤,2019,45(04):86-103.\n",
      "\n",
      "[348]楂樺獩.鏁板瓧浜烘枃鏃朵唬楂樻牎浜烘枃绀剧�戝浘涔﹂�嗙煡璇嗘湇鍔℃ā寮忕爺绌禰J].娌冲崡鍥句功棣嗗�﹀垔,2019,39(07):42-43+46.\n",
      "\n",
      "[349]椴佷腹,鏉庢��,闄堥噾浼�.鍩轰簬API鎶�鏈�鐨勬暟瀛椾汉鏂囧熀纭�璁炬柦鐨勬瀯寤篬J].鍥句功棣嗗�︾爺绌�,2019(13):42-46+57.\n",
      "\n",
      "[350]鍒樺悏鐚�.鈥滄暟瀛椾汉鏂団�濊儗鏅�涓嬬殑鍥句功棣嗚�昏�呮湇鍔″伐浣滆В鏋怺J].涓滄柟浼佷笟鏂囧寲,2019(S1):14.\n",
      "\n",
      "[351]搴勬捣鐕�.浠ユ暟瀛椾汉鏂囦负鑳屾櫙鈥斺�旂綉缁滄枃妗ｅ彲瑙嗗寲鎺㈢┒[J].鍖哄煙娌荤悊,2019(28):177-181.\n",
      "\n",
      "[352]娆ч槼鍓�,褰�鏉炬灄,鏉庤嚮.鏁板瓧浜烘枃鑳屾櫙涓嬪浘涔﹂�嗕汉鏂囨暟鎹�缁勭粐涓庨噸鏋刐J].鍥句功鎯呮姤宸ヤ綔,2019,63(11):15-24.\n",
      "\n",
      "[353]寮犲湥纾�,姣曞�︽垚.楂樻牎鍥句功棣嗗湪鐮旂┒浠�涔�?鈥斺�斿熀浜庨�嗗憳鍙戞枃鏁版嵁鐨勬枃鐚�璁￠噺鍒嗘瀽[J].澶у�﹀浘涔︽儏鎶ュ�﹀垔,2019,37(04):120-125.\n",
      "\n",
      "[354]鐗涘姏,鏇鹃潤鎬�. 鏁板瓧鏃朵唬妗ｆ�堝垱鏂板紑鍙戝埄鐢ㄧ殑鍑犵偣鎬濊�僛C]. 涓�鍥芥。妗堝�︿細銆佷腑鍥芥枃鐚�褰卞儚鎶�鏈�鍗忎細.2019骞存捣宄′袱宀告。妗堟毃缂╁井瀛︽湳浜ゆ祦浼氳�烘枃闆�.涓�鍥芥。妗堝�︿細銆佷腑鍥芥枃鐚�褰卞儚鎶�鏈�鍗忎細:涓�鍥芥。妗堝�︿細,2019:53-57.\n",
      "\n",
      "[355].鍥句功鎺ㄤ粙[J].璐㈢粡闂�棰樼爺绌�,2019(07):130.\n",
      "\n",
      "[356]Matthew Hannah,Sarah Huber,Sorin Adam Matei. Collecting Virtual and Augmented Reality in the Twenty-First Century Library[J]. Routledge,2019,44(2-4).\n",
      "\n",
      "[357]Honae H. Cuffe. Lend me your ears: the rise of the history podcast in Australia[J]. Routledge,2019,16(3).\n",
      "\n",
      "[358]Majdik. A Computational Approach to Assessing Rhetorical Effectiveness: Agentic Framing of Climate Change in the Congressional Record, 1994鈥�2016[J]. Routledge,2019,28(3).\n",
      "\n",
      "[359]Bradford Lee Eden. The Shape of Data in Digital Humanities: Modeling Texts and Text-Based Resources[J]. Routledge,2019,31(3).\n",
      "\n",
      "[360]Borgo Ton. Magic lantern shows through a macroscopic lens: topic modelling and mapping as methods for media archaeology[J]. Routledge,2019,17(3-4).\n",
      "\n",
      "[361]Dellmann. Analogue objects online. Epistemological reflections on digital reproductions of lantern slides[J]. Routledge,2019,17(3-4).\n",
      "\n",
      "[362]Massimo Lollini. Research and Teaching with the Oregon Petrarch Open Book[J]. Routledge,2019,37(2).\n",
      "\n",
      "[363]Brendan Hennessey. Standards and Peculiarities: The Christopher G. Wagstaff Film Collection as a Potential DH Resource[J]. Routledge,2019,37(2).\n",
      "\n",
      "[364]Serena Ferrando. The Navigli Project: A Digital Uncovering of Milan鈥檚 Aquatic Geographies[J]. Routledge,2019,37(2).\n",
      "\n",
      "[365]Allison Cooper. Kinolab: A Digital Humanities Project for the Analysis of Film Language[J]. Routledge,2019,37(2).\n",
      "\n",
      "[366]Crystal Hall. Digital Humanities and Italian Studies: Intersections and Oppositions[J]. Routledge,2019,37(2).\n",
      "\n",
      "[367]Daniel Leisawitz. The Orlando Furioso Atlas: A Digital-Cartographic Study of a Sixteenth-Century Epic[J]. Routledge,2019,37(2).\n",
      "\n",
      "[368]Crystal Hall. Galileo鈥檚 Library at the Intersection of Digital Humanities and Italian Studies[J]. Routledge,2019,37(2).\n",
      "\n",
      "[369]Chih-Ming Chen,Yung-Ting Chen,Chen-Yu Liu. Development and evaluation of an automatic text annotation system for supporting digital humanities research[J]. Library Hi Tech,2019,37(3).\n",
      "\n",
      "[370]Hsuanwei Michelle Chen. Information visualization skills for academic librarians[J]. Library Hi Tech,2019,37(3).\n",
      "\n",
      "[371]. Deforming Shakespeare's Sonnets: Topic Models as Poems[J]. Wayne State University Press,2019,61(3).\n",
      "\n",
      "[372]旯�氚旊��,臧曥毎攴�. 牍呺嵃鞚错劙鞕� 瓿犾爠氍疙暀 鞐瓣惮氚╇矔搿燵J]. 欷戩暀鞏措�疙暀須�,2019,78.\n",
      "\n",
      "[373]铯充含濮�. 霃欖晞鞁滌晞 瓿犾爠 鞐瓣惮鞚� 鞛愲�岇檧 鞐瓣惮氚╇矔搿犾棎 雽�頃� 靸堧�滌毚 鞚错暣 - 鞗愲掣瓿� 霐旍��韯� 鞛愲�岇潣 甑愳劖 -[J]. 歆勲嫧頃欗殞,2019(132).\n",
      "\n",
      "[374]旯�鞚柬櫂. 鞚鸽�疙暀鞚� 鞙勴暅 鞁犽�� 牍� 雿办澊韯办檧 韰嶌姢韸� 毵堨澊雼漑J]. 欷戩暀鞏措�疙暀須�,2019,78.\n",
      "\n",
      "[375]Youngsue Han. Jungian Character Network in Growing Other Character Archetypes in Films[J]. 頃滉淡旖橅厫旄犿暀須�,2019,15(2).\n",
      "\n",
      "[376]旯�氚旊��. 霐ル煬雼濎溂搿� 攵堦步 鞚疥赴 - Word2Vec鞙茧�� CBETA 攵堦步 雿办澊韯� 鞚疥赴[J]. 鞗愱磻雽�頃欔祼 鞗愲秷甑愳偓靸侅棸甑�鞗�,2019,80.\n",
      "\n",
      "[377]鏈辨収鏁�.楂樻牎鏁板瓧浜烘枃鍥句功棣�:椹卞姩鍥犲瓙銆佸姛鑳芥�嗘灦涓庢瀯寤虹瓥鐣�[J].姹犲窞瀛﹂櫌瀛︽姤,2019,33(03):97-101.\n",
      "\n",
      "[378]寮犻箯. 鏁板瓧浜烘枃鐨勫叕鍏卞叧绯诲��[N]. 绀句細绉戝�︽姤,2019-06-27(005).\n",
      "\n",
      "[379]鐜嬭春.浠庘�滅爺绌惰祫鏂欓泦鈥濆埌鈥滀笓棰樻暟鎹�搴撯�漑J].鑻忓窞鏁欒偛瀛﹂櫌瀛︽姤,2019,36(03):90-95.\n",
      "\n",
      "[380]琚佹簮,缃椾集鐗孤峰�斿埄.鏂囧�︾┖闂寸爺绌朵笌鏁欏��:缃椾集鐗孤峰�斿埄璁胯皥褰曪紙鑻辨枃锛塠J].澶栧浗鏂囧�︾爺绌�,2019,41(03):1-15.\n",
      "\n",
      "[381]鍊�涓借悕,闄堟槬闃�.鍦版柟闄㈡牎鍥句功棣嗘暟瀛椾汉鏂囨湇鍔℃ā寮忔帰鏋怺J].鑾嗙敯瀛﹂櫌瀛︽姤,2019,26(03):79-84.\n",
      "\n",
      "[382]瀹�闆�,鏉ㄩ��.鎴戝浗鏁板瓧浜烘枃鐮旂┒鐑�鐐瑰強鍚堣憲缃戠粶鍙�瑙嗗寲鍒嗘瀽[J].鍥句功鎯呮姤瀵煎垔,2019,4(06):39-45.\n",
      "\n",
      "[383]鏉ㄦ捣鎱�,鐜嬪啗.瀹嬩唬瀛︽湳甯堟壙鐭ヨ瘑鍥捐氨鐨勬瀯寤轰笌鍙�瑙嗗寲[J].鏁版嵁鍒嗘瀽涓庣煡璇嗗彂鐜�,2019,3(06):109-116.\n",
      "\n",
      "[384]闊╂枃濠�,瀹嬪＋鏉�,璧靛畤缈�,鏈卞簡鍗�.鏁板瓧浜烘枃绫讳紬鍖呮妱褰曞钩鍙颁腑浠诲姟缁╂晥鐨勫奖鍝嶅洜绱犵爺绌垛�斺�斿熀浜庝换鍔″�嶆潅搴︿笌棰嗗煙鐭ヨ瘑瑙嗚�抂J].鍥句功涓庢儏鎶�,2019(03):73-84.\n",
      "\n",
      "[385]寮犺僵鎱�,璧靛畤缈�,鐜嬫洶鑺�.鏁板瓧浜烘枃绫诲叕浼楃�戝�﹂」鐩�鍐峰惎鍔ㄩ樁娈电殑鍏�浼楀弬涓庡姩鍥犵爺绌禰J].鍥句功涓庢儏鎶�,2019(03):61-72.\n",
      "\n",
      "[386]Yves Laberge. Renewed virtual spiritualities: How religious discourses are actualised and rejuvenated by the Internet, new technologies and digital humanities[J]. SAGE Publications,2019,34(3).\n",
      "\n",
      "[387]浣欏僵闇�.鏁板瓧浜烘枃鐜�澧冧笅澶у�︾敓鍒涙柊鍒涗笟鑳藉姏鍩瑰吇鐮旂┒[J].浼犳挱鍔涚爺绌�,2019,3(18):229.\n",
      "\n",
      "[388]鐔婂噷,鍞愪紵楣�.鍩轰簬鏁板瓧浜烘枃鐨勫皯鏁版皯鏃忔枃鍖栦繚鎶や笌鍒╃敤鐮旂┒[J].浜戝崡妗ｆ��,2019(06):58-61.\n",
      "\n",
      "[389]寰愭櫒椋�,璧垫枃濞�.鎴戝浗鏁板瓧浜烘枃鐮旂┒棰嗗煙浣滆�呭悎钁楃綉缁滃垎鏋怺J].鍥句功棣嗚�哄潧,2019,39(11):14-24.\n",
      "\n",
      "[390]缁冮潠闆�,鐜嬬�辩憾,璧靛畤缈�.鏁板瓧浜烘枃瀛﹁�呯殑鎶�鏈�灏辩华搴﹁皟鐮斿強鏁板瓧璧嬭兘绛栫暐[J].鍥句功棣嗚�哄潧,2019,39(11):1-13.\n",
      "\n",
      "[391]鞚搓磻靹�,鞙れ瀽順�. 甑�雮� 鈥橂敂歆�韯胳澑氍疙暀鈥欖潣 鞝曥癌瓿� 甑搓场 : 雽�頃� 甑愳湣瓿� 氙鸽敂鞏� 韰岉伂雴�搿滌��鞚� 攵堨晥鞝曧暅 鞝戩啀[J]. 頃滉淡鞏鸽�犾爼氤错暀須�,2019,95.\n",
      "\n",
      "[392]姹�濠�.鏁板瓧浜烘枃鑳屾櫙涓嬮珮鏍″浘涔﹂�嗗憳鑱屼笟绱犲吇鍩硅偛[J].姝︽眽宸ョ▼鑱屼笟鎶�鏈�瀛﹂櫌瀛︽姤,2019,31(02):74-77.\n",
      "\n",
      "[393]闄堣瘹.鏁板瓧浜烘枃鏃朵唬鍥句功棣嗗彜绫嶆枃鐚�璧勬簮鐨勬暟瀛楀寲闀挎湡淇濆瓨鏈哄埗鎺㈣�╗J].涓�鍥戒腑鍖昏嵂鍥句功鎯呮姤鏉傚織,2019,43(03):6-9.\n",
      "\n",
      "[394]鑲栫幉.娴呮瀽鍥句功棣嗙�挎湰鏂囩尞鐨勫紑鍙戝埄鐢╗J].鏅嬪浘瀛﹀垔,2019(03):28-32.\n",
      "\n",
      "[395]寮犳瘏,鏉庢��.闈㈠悜鏁板瓧浜烘枃鐨勭壒钘忚祫婧愭彮绀虹爺绌垛�斺�斾互鏂瑰織鏁版嵁搴撳缓璁句负渚媅J].鍥句功棣�,2019(06):100-105.\n",
      "\n",
      "[396]閲囪枃.鈥滄暟瀛椾汉鏂囨椂浠ｇ殑鏂囧�︾爺绌垛�濆�︽湳鐮旇�ㄤ細鍦ㄥ紑灏佷妇琛孾J].涓�宸炲�﹀垔,2019(06):2.\n",
      "\n",
      "[397]鐜嬪啝缇�,璁稿爟.鏅氭竻銆佹皯鍥芥湡鍒婁腑杩戜唬鑷�鐢辫�傚康鐨勫憟鐜扮爺绌禰J].鐜颁唬浼犳挱(涓�鍥戒紶濯掑ぇ瀛﹀�︽姤),2019,41(06):47-54.\n",
      "\n",
      "[398]鑳℃枃鍗�.鍏ㄧ悆瑙嗛噹涓嬬殑鏁板瓧浜烘枃瀹炶返  鍒樼倻鍋氬�㈠崕涓滃笀鑼冨ぇ瀛﹀浘涔﹂�嗘暟瀛椾汉鏂囨湇鍔＄┖闂碵J].涓婃捣楂樻牎鍥句功鎯呮姤宸ヤ綔鐮旂┒,2019,29(02):8.\n",
      "\n",
      "[399]鍛ㄦ瘏,鏉庡崜鍗�.鏂版枃绉戝缓璁剧殑鐞嗚矾涓庤�捐��[J].涓�鍥藉ぇ瀛︽暀瀛�,2019(06):52-59.\n",
      "\n",
      "[400]Gaudenz Halter,Rafael Ballester鈥怰ipoll,Barbara Flueckiger,Renato Pajarola. VIAN: A Visual Annotation Tool for Film Analysis[J]. Computer Graphics Forum,2019,38(3).\n",
      "\n",
      "[401]. Teaching The Grapes of Wrath in the Digital Humanities Age[J]. Pennsylvania State University Press,2019,16(1).\n",
      "\n",
      "[402]濮滄枃娑�,鎴村畨寰�.涓绘寔浜鸿�璠J].灞变笢绀句細绉戝��,2019(06):29-30.\n",
      "\n",
      "[403]鑹句鸡路鍒�,姹�铇�.閫氬線鎬濊鲸鐨勫熀纭�璁炬柦鐮旂┒[J].灞变笢绀句細绉戝��,2019(06):40-44.\n",
      "\n",
      "[404]鏉庢硥.鏁板瓧浜烘枃:寮�鍒涗汉鏂囩爺绌剁殑鏂扮邯鍏僛J].绂忓缓璁哄潧(浜烘枃绀句細绉戝�︾増),2019(06):156-165.\n",
      "\n",
      "[405]Inge Ven. Creative Reading in the Information Age: Paradoxes of Close and Distant Reading[J]. The Journal of Creative Behavior,2019,53(2).\n",
      "\n",
      "[406]Cantoni Virginio,Dondi Piercarlo,Lombardi Luca,Setti Alessandra. Teaching Computer Graphics Through a Digital Humanities Project.[J]. Pubmed,2019,39(2).\n",
      "\n",
      "[407]. RTI Images for Documentation in Archaeology: The Case of the Iron Age Female Terracotta Figurines from Bu峁�ayra, Jordan[J]. Pennsylvania State University Press,2019,7(2).\n",
      "\n",
      "[408]HunzikerRodewald Regine,Fornaro Peter. RTI Images for Documentation in Archaeology: The Case of the Iron Age Female Terracotta Figurines from Bu峁�ayra, Jordan[J]. Pennsylvania State University Press,2019,7(2).\n",
      "\n",
      "[409]Gary S. Schaal,Sebastian Dumm,Dannica Fleu脽. Die vielen Wahrheiten algorithmenbasierter Interpretation: 鈥濪ie Wahrheit 眉ber Postfaktizit盲t鈥� dekonstruiert[J]. VS Verlag f眉r Sozialwissenschaften,2019,29(2).\n",
      "\n",
      "[410]鎴存伜杩�. 鍩轰簬璇嶅吀鍜屾満鍣ㄥ�︿範鐨勪腑鏂囧皬璇寸殑鎯呮劅鐮旂┒[D].骞胯タ澶у��,2019.\n",
      "\n",
      "[411]. Composure and Composition: Marianne Moore's Serial Imagination[J]. Pennsylvania State University Press,2019,9(1).\n",
      "\n",
      "[412]瀹夋檽涓�. 鐭ヨ瘑鐢熶骇鐨勫垱鎰忓啓浣滀箣缁碵C]. 涓婃捣甯傚崕鏂囧垱鎰忓啓浣滀腑蹇冦�佷笂娴峰ぇ瀛︽枃瀛﹂櫌.涓�鍥藉垱鎰忓啓浣滅爺绌讹紙2019锛�.涓婃捣甯傚崕鏂囧垱鎰忓啓浣滀腑蹇冦�佷笂娴峰ぇ瀛︽枃瀛﹂櫌:涓婃捣甯傚崕鏂囧垱鎰忓啓浣滀腑蹇�,2019:141-149.\n",
      "\n",
      "[413]閯備附鍚�.缇庡浗鐮旂┒鍥句功棣嗗崗浼氱殑鏁板瓧瀛︽湳鏀�鎸佹暀鑲叉椿鍔ㄨ�冨療涓庡惎绀篬J].鎯呮姤璧勬枡宸ヤ綔,2019,40(03):104-112.\n",
      "\n",
      "[414]鍛ㄧ孩姊�.鏁板瓧浜烘枃鑳屾櫙涓嬮珮鏍℃枃鐚�璧勬簮寤鸿�剧殑閫斿緞鐮旂┒[J].绂忓缓鑼跺彾,2019,41(05):216.\n",
      "\n",
      "[415]Teresa Ayala-Perez,Jorge Joo-Nagata. The digital culture of students of pedagogy specialising in the humanities in Santiago de Chile[J]. Elsevier Ltd,2019,133.\n",
      "\n",
      "[416]闈冲槈鏋�,寮犵敳,鐜嬫洶鑺�,浣欏帤寮�.鏂版椂浠ｃ�佹柊鎸戞垬銆佹柊绛栫暐鈥斺�斺��2018骞村�︽湳鍥句功棣嗗彂灞曗�濋珮绔�璁哄潧浼氳��缁艰堪[J].澶у�﹀浘涔﹂�嗗�︽姤,2019,37(03):13-17.\n",
      "\n",
      "[417]鏉滃�ｈ幑.璧板叆鍏�浼椻�斺�旇繎鐜颁唬鑻卞浗鍥藉�舵。妗堢殑绠＄悊娌块潻[J].妗ｆ�堝�﹂�氳��,2019(03):61-69.\n",
      "\n",
      "[418]鍛ㄩ噾鍏�,瑙ｈ崳,鏉ㄦˇ,鍗㈢珷骞�.鍥句功棣嗛槄璇荤枟娉曞疄璺靛奖鍝嶅洜绱犳帰鏋怺J].鍥句功棣�,2019(05):99-105.\n",
      "\n",
      "[419]閲戞妸璺�.浜哄伐鏅鸿兘鏃朵唬鐨勨�滄柊瀛愬�︹�濊瘯鎺�[J].鍚嶄綔娆ｈ祻,2019(15):14-17.\n",
      "\n",
      "[420]宸︿腹,娆х煶鐕�.浜烘枃淇℃伅璧勬簮璇�涔夋弿杩般�佽��涔夌粍缁囩爺绌朵笌瀹炶返杩拌瘎[J].鍥句功棣嗚�哄潧,2019,39(08):21-31.\n",
      "\n",
      "[421]鏂芥檽鍗�. 鐭╅樀鍒嗚В瀛︿範鍙婂叾鍦ㄧ綉缁滅ぞ鍖哄彂鐜颁腑鐨勫簲鐢ㄧ爺绌禰D].涓婃捣浜ら�氬ぇ瀛�,2019.\n",
      "\n",
      "[422]鑳′箖楹�,鐢冲瘜鑻�.鏂颁笘绾�鍥藉�栨枃瀛︾爺绌�:鐑�鐐广�佹柟娉曘�佺珛鍦衡�斺�斿熀浜�6绉岮&HCI鏂囧�︽湡鍒�2014鈥�2018骞寸殑鍒嗘瀽[J].灞变笢绀句細绉戝��,2019(05):163-168.\n",
      "\n",
      "[423]娼橀�栬尮.淇℃伅鍖栨椂浠ｅ叕鍏卞浘涔﹂�嗛潰涓寸殑鎸戞垬鍙婂彂灞曟�濊矾[J].鐜颁唬缁忔祹淇℃伅,2019(09):422.\n",
      "\n",
      "[424]Gian Andrea Giacobone,Lorella Camellina. Computational art experiments for the university: revealing culture, community and social spaces[J]. Routledge,2019,15(2).\n",
      "\n",
      "[425]Weber,Buchanan. Metadata as a machine for feeling in Germaine Greer鈥檚 archive[J]. Routledge,2019,47(2).\n",
      "\n",
      "[426]Maria Teresa Cruz. Art curation and critique in the age of digital humanities[J]. Routledge,2019,15(2).\n",
      "\n",
      "[427]Sarah Bidgood. Undergraduate disarmament and nonproliferation education: gaps, opportunities, and new approaches[J]. Routledge,2019,26(3-4).\n",
      "\n",
      "[428]Charles W. Romney. Consistency and change in the vocabulary of fundamental rights in Hawaiian law, 1847鈥�1902[J]. Routledge,2019,20(2).\n",
      "\n",
      "[429]姣涢��.鍩轰簬LDA鍜孏BDT绠楁硶鐨勫�规枃瀛︿綔鍝佺埍鍥戒富涔夌壒寰佺殑鍒嗙被鐮旂┒[J].鏂囧寲鍒涙柊姣旇緝鐮旂┒,2019,3(13):59-60.\n",
      "\n",
      "[430]宸﹀��. 涓�缇庢暟瀛椾汉鏂囧缓璁鹃」鐩�瀵规瘮鐮旂┒[D].鍚夋灄澶у��,2019.\n",
      "\n",
      "[431]瀹嬩腹涓�. 鏁板瓧浜烘枃瑙嗛槇涓嬩簯鍗楀皯鏁版皯鏃忚妭鏃ユ枃鍖栦俊鎭�璧勬簮寤鸿�剧爺绌禰D].浜戝崡澶у��,2019.\n",
      "\n",
      "[432]榛勫皬娣�. 闈㈠悜鏁板瓧浜烘枃鐨勫浘涔﹂�嗗紑鏀炬暟鎹�绠＄悊妯″紡鐮旂┒[D].杈藉畞甯堣寖澶у��,2019.\n",
      "\n",
      "[433]Wido van Peursen. A Computational Approach to Syntactic Diversity in the Hebrew Bible[J]. 雽�頃滌劚靹滉车須�,2019(44).\n",
      "\n",
      "[434]闈虫枃鍚�.涓�缇庢。妗堥�嗗煙鏁板瓧浜烘枃椤圭洰姣旇緝鐮旂┒[J].娴欐睙妗ｆ��,2019(04):37-39.\n",
      "\n",
      "[435]鏉庡�傞箯.鏁板瓧浜烘枃涓嬪浘涔﹂�嗙殑瑙掕壊[J].鍥句功棣嗙悊璁轰笌瀹炶返,2019(04):10-14.\n",
      "\n",
      "[436]鍒樺仴.鍗氱墿棣嗘暟鎹�鍙�瑙嗗寲鐨勬帰绱�涓庡疄璺碘�斺�斾互涓婃捣鍗氱墿棣嗘暟瀛楀寲寤鸿�句负渚媅J].鍗氱墿闄�,2019(02):91-97.\n",
      "\n",
      "[437]鍒橀煹楦�.鍦扮悊淇℃伅鏁版嵁搴撶殑杞�鍨嬩笌浜烘枃鐮旂┒鐨勪氦铻嶁�斺�斺�滃ぇ鏁版嵁涓庝汉鏂囧湴鐞嗕俊鎭�鏁版嵁搴撳缓璁锯�濆浗闄呬細璁�缁艰堪[J/OL].鍥句功棣嗚�哄潧:1-10[2020-06-26].http://kns.cnki.net/kcms/detail/44.1306.G2.20190424.1717.002.html.\n",
      "\n",
      "[438]寮犲噷闈�,闂�闈�,鐜嬫棴鐔�. 铻嶅悎鏁板瓧浜烘枃绌洪棿鐨勫煄涔¤�勫垝铏氭嫙鍙�瑙嗗寲鏁欏�﹀垱鏂扮爺绌禰C]. AEIC Academic Exchange Information Centre(China).Proceedings of 2019 5th International Conference on Humanities and Social Science Research (ICHSSR 2019)锛圓dvances in Social Science, Education and Humanities Research锛孷OL.319锛�.AEIC Academic Exchange Information Centre(China):International Conference on Humanities and Social Science Research,2019:244-251.\n",
      "\n",
      "[439]寮犳垚涓�.鏁板瓧浜烘枃瑙傚康鍦ㄥ浘涔﹂�嗘暟鎹�涓�鐨勫簲鐢╗J].灞辫タ妗ｆ��,2019(02):106-108.\n",
      "\n",
      "[440]鏇瑰皬瀹�.鍏充簬鍥句功棣嗘暟瀛椾汉鏂囩殑鐮旂┒[J].灞辫タ妗ｆ��,2019(02):109-110.\n",
      "\n",
      "[441]琚佺埍鑾�.鏁板瓧浜烘枃鑳屾櫙涓嬮珮鏍″浘涔﹂�嗙壒鑹插�︾�戣祫婧愬缓璁句笌鏈嶅姟鐮旂┒[J].鏅哄簱鏃朵唬,2019(16):271-272.\n",
      "\n",
      "[442]钂嬭悓.鏂版椂浠ｅ浘涔﹂�嗘暟瀛椾汉鏂囧彂灞曠爺绌禰J].鍥句功棣嗗伐浣滀笌鐮旂┒,2019(04):16-21+48.\n",
      "\n",
      "[443]閮佹尟鍗�.浜烘枃瀛︽湳濡備綍杩庢帴鎶�鏈�鏃朵唬[J].鎺㈢储涓庝簤楦�,2019(04):5-7.\n",
      "\n",
      "[444]澶忕繝濞�.鏁板瓧浜烘枃涔嬬儹娴�娼�涓庡喎鎬濊�僛J].鍥句功鎯呮姤鐭ヨ瘑,2019(02):2.\n",
      "\n",
      "[445]姹よ悓,瀛欑繉,鍒樺畞闈�,闄堝槈鎳�.寰藉窞鏂囦功鐗硅壊璧勬簮鐨勪富棰樿�捐�′笌鏍囧紩鏂规硶鐮旂┒[J].鍥句功棣嗘潅蹇�,2019,38(04):61-68.\n",
      "\n",
      "[446]鐜嬫檽闃�,閮�鏅�.鍝堜經澶у�︽暟瀛楀�︽湳鏈嶅姟鏆‵DS鍩硅��椤圭洰瀹炶返涓庡惎绀篬J].鍥句功棣嗘潅蹇�,2019,38(04):69-73.\n",
      "\n",
      "[447]琚佸啗.鏁板瓧浜烘枃鑳屾櫙涓嬮珮鏍″浘涔﹂�嗘湇鍔＄瓥鐣ョ爺绌禰J].娌冲崡鍥句功棣嗗�﹀垔,2019,39(04):73-75.\n",
      "\n",
      "[448]瀛熷缓,鑳″�﹀嘲.鏁板瓧浜烘枃:濯掍粙椹卞姩鐨勫�︽湳鐢熶骇鏂瑰紡鍙橀潻[J].鐜颁唬浼犳挱(涓�鍥戒紶濯掑ぇ瀛﹀�︽姤),2019,41(04):24-28+54.\n",
      "\n",
      "[449]鏉庣珛鐫�,鐜嬪崥闆�.鍥藉�杋Schools楂樻牎鍥句功棣嗘暟瀛楀�︽湳鏈嶅姟璋冩煡涓庡垎鏋怺J].鎯呮姤鐞嗚�轰笌瀹炶返,2019,42(06):172-176.\n",
      "\n",
      "[450]鍒樿寽.鏁板瓧浜烘枃鑳屾櫙涓嬮珮鏍″浘涔﹂�嗙ぞ浼氭湇鍔℃帰鏋怺J].杈圭枂缁忔祹涓庢枃鍖�,2019(04):119-120.\n",
      "\n",
      "[451]寮犻潤. 鏁板瓧浜烘枃涓�鍘嗗彶浜虹墿鏁版嵁鐨勫彲瑙嗗寲搴旂敤鐮旂┒[D].婀栧崡澶у��,2019.\n",
      "\n",
      "[452]Tibor K谩lm谩n,Matej 膸ur膷o,Frank Fischer,Nicolas Larrousse,Claudio Leone,Karlheinz M枚rth,Carsten Thiel. A landscape of data 鈥� working with digital resources within and beyond DARIAH[J]. Springer International Publishing,2019,1(1).\n",
      "\n",
      "[453]鍒樺啲.鏁板瓧瀛︽湳瑙嗗煙涓嬬殑鏁版嵁搴撹�捐�′笌寤鸿�锯�斺�斾互鏃ф捣鍏冲垔杞戒腑鍥借繎浠ｅ彶鏂欐暟鎹�搴撲负渚媅J].绉戞妧涓庡嚭鐗�,2019(04):17-22.\n",
      "\n",
      "[454]鎴村��,闄堢惓,鏇圭�嬫櫁.璇�瑷�璁＄畻鍓嶆部鐑�鐐圭殑鍙�瑙嗗寲鍒嗘瀽鈥斺�斾互銆婃枃瀛︿笌璇�瑷�璁＄畻銆嬫湡鍒婃暟鎹�涓轰緥[J].娴峰�栬嫳璇�,2019(07):198-201.\n",
      "\n",
      "[455]鑻忕��,闊╁┃,寮犲箍钀�,褰�杩滅孩,鐭崇��,鐜嬫檽鍏�.鑱氱劍闈㈠悜鏁板瓧瀛︽湳鐨勬暟鎹�鍑虹増[J].绉戞妧涓庡嚭鐗�,2019(04):5.\n",
      "\n",
      "[456]鐜嬫稕.鏁板瓧瀛︽湳瑙嗚�掍笅鐨勪汉鏂囨暟鎹�搴撳嚭鐗圼J].绉戞妧涓庡嚭鐗�,2019(04):12-16.\n",
      "\n",
      "[457]璋㈢倻.瀛︽湳鍑虹増鍦ㄦ暟瀛楀�︽湳鍙戝睍涓�鐨勫畾浣嶄笌浣滅敤[J].绉戞妧涓庡嚭鐗�,2019(04):22-28.\n",
      "\n",
      "[458]璁搁懌,闄堣矾閬�,鏉ㄤ匠棰�.鏁板瓧浜烘枃鐮旂┒棰嗗煙鐨勭煡璇嗗浘璋辨瀯寤轰笌鍒嗘瀽鈥斺�斿熀浜嶹oS鏂囩尞鍏抽敭璇嶅拰寮曟枃涓婁笅鏂囩殑瀹炶瘉[J].鍥句功鎯呮姤宸ヤ綔,2019,63(07):86-95.\n",
      "\n",
      "[459]Sparks. Digital Humanities as Anamorphosis[J]. Routledge,2019,31(2).\n",
      "\n",
      "[460]Twyla Gibson. Digital humanities, libraries, and collaborative research: New technologies for digital textual studies[J]. Routledge,2019,26(2).\n",
      "\n",
      "[461]Mari Lee Mifsud. To the humanities: what does communication studies give?[J]. Routledge,2019,19(2).\n",
      "\n",
      "[462]Rosa. Digital Library Polona : Digitization, Technology, Cooperation[J]. Routledge,2019,20(1-2).\n",
      "\n",
      "[463]Jonas Svensson. Computing Qur鈥檃ns: A Suggestion for a Digital Humanities Approach to the Question of Interrelations between English Qur鈥檃n Translations[J]. Routledge,2019,30(2).\n",
      "\n",
      "[464]Murtha Baca,Anne Helmreich,Melissa Gill. Digital Art History[J]. Routledge,2019,35(1-2).\n",
      "\n",
      "[465]Dombrowski,Alexander,Zhobov. Bulgarian Dialectology as Living Tradition: A Labor of Love[J]. Routledge,2019,20(1-2).\n",
      "\n",
      "[466]Paul B. Jaskot. Digital Art History as the Social History of Art: Towards the Disciplinary Relevance of Digital Methods[J]. Routledge,2019,35(1-2).\n",
      "\n",
      "[467]Kathryn M. Wissel,Jeff Hiroshi Gima. AMICAL: Building Bridges through Digital Connections[J]. Routledge,2019,51(2).\n",
      "\n",
      "[468]Jordana Cox,Lauren Tilton. The digital public humanities: giving new arguments and new ways to argue[J]. Routledge,2019,19(2).\n",
      "\n",
      "[469]Pendse. The Armenian Periodical Press of Baku 1877-1920: A Survey[J]. Routledge,2019,20(1-2).\n",
      "\n",
      "[470]Francesca Giannetti. Book Review: Digital Humanities, Libraries, and Partnerships: A Critical Examination of Labor, Networks, and Community[J]. Reference & User Services Quarterly,2019,58(3).\n",
      "\n",
      "[471]. Scrolling through Nature: Reflections on the Digital Humanities and Michigan's Environmental History[J]. Central Michigan University,2019,45(1).\n",
      "\n",
      "[472]鏈辨�濊嫅. 鏁板瓧浜烘枃鐜�澧冧笅楂樻牎鍥句功棣嗚�掕壊瀹氫綅鐮旂┒[D].姹熻嫃澶у��,2019.\n",
      "\n",
      "[473]氚办垬歆�. 雽�頃欔竴鞊瓣赴鞚� 瓿检牅鞕� 牍呺嵃鞚错劙毳� 頇滌毄頃� 鞚鸽�戈硠 旮�鞊瓣赴 氚╈晥[J]. 頃滉淡鞏措�疙暀須�,2019,143.\n",
      "\n",
      "[474]氚办垯頋�. 霐旍��韯� 鞚鸽�疙暀瓿� 靻‰寑靷� 鞐瓣惮[J]. 霃欖枒靷�頃欗殞,2019,146.\n",
      "\n",
      "[475]旯�氚旊��. 霐旍��韯� 鞚鸽�疙暀鞚� 甏�鞝愳棎靹� 氤� 霐旍��韯� 攵堦祼頃橻J]. 霃欔淡雽�頃欔祼 攵堦祼氍疙檾鞐瓣惮鞗�,2019,86.\n",
      "\n",
      "[476]钖涙�㈤洩.鍥句功棣嗘暟瀛椾汉鏂囬」鐩�涓�鍙�绉诲姩鏂囩墿鏁版嵁搴撶殑寮�鍙戞瀯寤篬J].鍏�鍏卞浘涔﹂��,2019(01):36-40.\n",
      "\n",
      "[477]鑳＄粛鍚�.鏁板瓧浜烘枃棣嗗憳鑱屼笟鑳藉姏鏋勬垚鍙婂煿鍏荤瓥鐣�[J].鍥句功棣嗙悊璁轰笌瀹炶返,2019(03):87-93.\n",
      "\n",
      "[478]閭变紵浜�.楠岃瘉銆佷慨姝ｃ�佸垱鏂�:鏁板瓧鍙插�︽柟娉曠殑涓夐噸鍔熻兘[J].鍗椾含澶у�﹀�︽姤(鍝插�β蜂汉鏂囩�戝�β风ぞ浼氱�戝��),2019,56(02):87-90.\n",
      "\n",
      "[479]Pineda Luis A.,Hern脙隆ndez No脙漏,Torres Iv脙隆n,Fuentes Gibr脙隆n,Pineda De Avila Nydia. Practical non-monotonic knowledge-base system for un-regimented domains: A Case-study in digital humanities[J]. Elsevier Ltd,2019,57(3).\n",
      "\n",
      "[480]楂樺缓杈�.鏁板瓧浜烘枃瑙嗗煙涓嬪皯鏁版皯鏃忓彛杩板巻鍙茶祫鏂欑殑淇濇姢鐮旂┒[J].鍥句功棣嗗�︾爺绌�,2019(06):34-39.\n",
      "\n",
      "[481]鏉庨洩绾�,閽辨�濇櫒.鍩轰簬CiteSpace鐨勫浗鍐呮暟瀛楀�︽湳鐮旂┒鍙�瑙嗗寲鍒嗘瀽[J].鍥句功鎯呮姤瀵煎垔,2019,4(03):69-77.\n",
      "\n",
      "[482]琚佹偊,鐜嬩笢娉�,榛勬按娓�,鏉庢枌.涓嶅悓璇嶆�ф爣璁伴泦鍦ㄥ吀绫嶅疄浣撴娊鍙栦笂鐨勫樊寮傛�ф帰绌禰J].鏁版嵁鍒嗘瀽涓庣煡璇嗗彂鐜�,2019,3(03):57-65.\n",
      "\n",
      "[483]鍒橀潤.鏁板瓧浜烘枃鑳屾櫙涓嬮珮鏍″浘涔﹂�嗗憳涓撲笟绱犲吇鍩硅偛鐮旂┒[J].姝︽眽鑸硅埗鑱屼笟鎶�鏈�瀛﹂櫌瀛︽姤,2019,18(01):78-81.\n",
      "\n",
      "[484]璁搁懌,闄堣矾閬�,鏉ㄤ匠棰�.鏁板瓧浜烘枃鐮旂┒棰嗗煙鐨勭煡璇嗙綉缁滄紨鍖栤�斺�斿熀浜庨�樺綍淇℃伅鍜屽紩鏂囦笂涓嬫枃鐨勫叧閿�璇嶅叡璇嶅垎鏋怺J].鎯呮姤瀛︽姤,2019,38(03):322-334.\n",
      "\n",
      "[485]寮犱箖甯�,瀛欒秴.鍖椾含澶у�﹀浘涔﹂�嗛暱鏈熶繚瀛樼郴缁熷缓璁句笌鎺㈢储[J].澶у�﹀浘涔﹂�嗗�︽姤,2019,37(02):62-66.\n",
      "\n",
      "[486]寮犳緧闆�.鏁板瓧浜烘枃鐜�澧冧笅妗ｆ�堜俊鎭�浼犳挱鏈嶅姟鐨勬柊鎬濊�僛J].妗ｆ�堜笌寤鸿��,2019(03):37-40+32.\n",
      "\n",
      "[487]榄忎細娲�,琚佹洣涓�.绀句細缃戠粶鍒嗘瀽鍦ㄦ枃瀛﹂槄璇荤爺绌朵腑鐨勯�傜敤鎬ч棶棰樷�斺�斾互鏁板瓧浜烘枃瑙嗚�掍笅鐨勩�婄櫧楣垮師銆嬩汉鐗╁叧绯婚槓閲婁负渚媅J].鏂颁笘绾�鍥句功棣�,2019(03):30-34.\n",
      "\n",
      "[488]閮�鍒╂晱,钁涗寒,鍒樻偊濡�.鍗风Н绁炵粡缃戠粶鍦ㄥ彜绫嶆眽瀛楄瘑鍒�涓�鐨勫簲鐢ㄥ疄璺礫J].鍥句功棣嗚�哄潧,2019,39(10):142-148.\n",
      "\n",
      "[489]鏉ㄨ寽鑼�.鏁板瓧浜烘枃瑙嗛噹涓嬬殑鍘嗗彶妗ｆ�堣祫婧愭暣鐞嗕笌寮�鍙戣矾寰勬帰鏋愨�斺�斿吋璁烘。妗堢�＄悊涓�鐨勫巻鍙蹭富涔変笌閫昏緫涓讳箟鎬濇兂[J].妗ｆ�堝�﹂�氳��,2019(02):17-22.\n",
      "\n",
      "[490]Sunimal Mendis. Public open collaborative creation (POCC): A new archetype of authorship?[J]. The Journal of World Intellectual Property,2019,22(1-2).\n",
      "\n",
      "[491]Helen Bones. Linked digital archives and the historical publishing world: An Australasian perspective[J]. History Compass,2019,17(3).\n",
      "\n",
      "[492]闄嗙嚂,鍗㈢珷骞�.缇庡浗楂樻牎鍥句功棣嗙殑鏈嶅姟杞�鍨嬩笌鍙戝睍鈥斺�擟LSEP2018璁垮�︽姤鍛奫J].鍥句功鎯呮姤鐮旂┒,2019,12(01):68-73.\n",
      "\n",
      "[493]閮戜附澶�,璁告槬婕�.缇庡浗楂樻牎鍥句功棣嗘暟瀛楀�︽湳棣嗗憳闃熶紞寤鸿�惧強鍚�绀篬J].鎯呮姤璧勬枡宸ヤ綔,2019,40(02):100-112.\n",
      "\n",
      "[494]鐜嬭緣,缃楄寽,闄堝渾,璋�鍏夎緣.鏁板瓧鏃朵唬浜烘枃瀛︾�戠殑鍛借繍鈥斺�斺�滃紑鏄庤�哄潧鈥濈��鍥涙湡璁ㄨ�轰細缁艰堪[J].缁甸槼甯堣寖瀛﹂櫌瀛︽姤,2019,38(03):109-114+144.\n",
      "\n",
      "[495]鍒橀煹楦�.鈥滃ぇ鏁版嵁涓庝汉鏂囧湴鐞嗕俊鎭�鏁版嵁搴撳缓璁锯�濆浗闄呭�︽湳浼氳��鍙�寮�[J].鏂囧�﹂仐浜�,2019(02):140.\n",
      "\n",
      "[496]鍒樻檵濡�.涓�缇庢。妗堟暟瀛椾汉鏂囬」鐩�姣旇緝鐮旂┒[J].妗ｆ�堢�＄悊,2019(02):33-36.\n",
      "\n",
      "[497]鍚寸�嬫嚳.鍩轰簬鏁板瓧浜烘枃鐨勯珮鑱岄櫌鏍″簲鐢ㄥ啓浣滄暀瀛︽敼闈╃爺绌垛�斺�斾互鍐呰挋鍙ゅ缓绛戣亴涓氭妧鏈�瀛﹂櫌搴旂敤鍐欎綔鏁欏�︽敼闈╀负渚媅J].鍝堝皵婊ㄨ亴涓氭妧鏈�瀛﹂櫌瀛︽姤,2019(02):62-64.\n",
      "\n",
      "[498]榛勬按娓�.浜烘枃璁＄畻涓庢暟瀛椾汉鏂�:姒傚康銆侀棶棰樸�佽寖寮忓強鍏抽敭鐜�鑺俒J].鍥句功棣嗗缓璁�,2019(05):68-78.\n",
      "\n",
      "[499]鍙朵孩.鏁板瓧浜烘枃涓庤嫳鍥芥姤鍒婂彶鐮旂┒鈥斺�斾互鈥淕ale鍘熷�嬫。妗堟暟鎹�搴撯�濅负鏍稿績[J].鏂伴椈鐮旂┒瀵煎垔,2019,10(05):17-19.\n",
      "\n",
      "[500]Gerhard Heyer,Jochen Tiepmar. A聽Big Data Case Study in Digital Humanities[J]. Springer Berlin Heidelberg,2019,19(1).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将文末参考文献按字段整理成表格形式\n",
    "#　国标引文.txt 文件来自cnki的导出题录, 导出为国标GB/T 7714-2015 参考文献格式.\n",
    "!type data\\国标引文.txt\n",
    "# 中文字符, 需要指定encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]梁军.基于数字人文的智慧图书馆建设路径探索[J].才智,2020(18):241.\n",
      "\n",
      "[2]张婷.台湾地区数字人文研究特色与实践启示[J].高校图书馆工作,2020,40(04):41-45.\n",
      "\n",
      "[3]杨佳颖,邓璐芗,许鑫.觅江南佳馔：多源古今文本数据融合的沪上饮食图谱构建[J/OL].图书馆论坛:1-9[2020-06-26].http://kns.cnki.net/kcms/detail/44.1306.G2.20200616.1533.002.html.\n",
      "\n",
      "[4]姚啸华,贺晨芝,徐孝娟,全石峰.面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例[J].图书馆杂志,2020,39(06):105-112.\n",
      "\n",
      "[5]范桂红,赵纯洋.基于知识图谱的古籍数字化研究前沿热点及演化趋势分析[J].出版广角,2020(11):85-87.\n",
      "\n",
      "[6]徐晨飞,叶海影,包平.基于深度学习的方志物产资料实体自动识别模型构建研究[J/OL].数据分析与知识发现:1-17[2020-06-26].http://kns.cnki.net/kcms/detail/10.1478.G2.20200609.1040.006.html.\n",
      "\n",
      "[7]刘昱彤,吴斌,白婷.古诗词图谱的构建及分析研究[J].计算机研究与发展,2020,57(06):1252-1268.\n",
      "\n",
      "[8]巴特,邓君.数字人文视域下我国高校口述校史档案建设路径思考[J].兰台世界,2020(06):31-34.\n",
      "\n",
      "[9]李松涛,张卫东,左娜.数字人文视角下人文研究者利用档案馆藏的行为与激励研究[J].山西档案,2020(03):77-97.\n",
      "\n",
      "[10]姜育彦,李雅茹.基于数字人文视角的“情感——时空”模型探析[J].农业图书情报学报,2020,32(06):23-33.\n",
      "\n",
      "[11]梁继文,江川,王东波.基于多特征融合的先秦典籍汉英句子对齐研究[J/OL].数据分析与知识发现:1-13[2020-06-26].http://kns.cnki.net/kcms/detail/10.1478.G2.20200603.0948.002.html.\n",
      "\n",
      "[12]卢丹丹,聂云霞.数字人文视角下地方特色档案资源开发路径[J/OL].山西档案:1-6[2020-06-26].http://kns.cnki.net/kcms\n"
     ]
    }
   ],
   "source": [
    "# 第一步, 指定编码,读取文本\n",
    "ref=open('data\\国标引文.txt',encoding='utf8').read()\n",
    "print(ref[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [1]梁军.基于数字人文的智慧图书馆建设路径探索[J].才智,2020(18):241.\n",
       "2    [2]张婷.台湾地区数字人文研究特色与实践启示[J].高校图书馆工作,2020,40(04)...\n",
       "4    [3]杨佳颖,邓璐芗,许鑫.觅江南佳馔：多源古今文本数据融合的沪上饮食图谱构建[J/OL]....\n",
       "6    [4]姚啸华,贺晨芝,徐孝娟,全石峰.面向数字人文的图书馆众包平台构建研究——以上海图书馆历...\n",
       "8    [5]范桂红,赵纯洋.基于知识图谱的古籍数字化研究前沿热点及演化趋势分析[J].出版广角,2...\n",
       "dtype: object"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第二步,按换行符分割后转为Series对象,以便于使用Series的 .str 类方法\n",
    "refs=pd.Series(ref.split('\\n')).replace('',pd.NA).dropna()\n",
    "refs.head()\n",
    "# 有空行,可以先 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>0_</th>\n",
       "      <th>1_</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2]张婷</td>\n",
       "      <td>台湾地区数字人文研究特色与实践启示[J]</td>\n",
       "      <td>高校图书馆工作</td>\n",
       "      <td>2020</td>\n",
       "      <td>41-45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4]姚啸华,贺晨芝,徐孝娟,全石峰</td>\n",
       "      <td>面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例[J]</td>\n",
       "      <td>图书馆杂志</td>\n",
       "      <td>2020</td>\n",
       "      <td>105-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[7]刘昱彤,吴斌,白婷</td>\n",
       "      <td>古诗词图谱的构建及分析研究[J]</td>\n",
       "      <td>计算机研究与发展</td>\n",
       "      <td>2020</td>\n",
       "      <td>1252-1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[10]姜育彦,李雅茹</td>\n",
       "      <td>基于数字人文视角的“情感——时空”模型探析[J]</td>\n",
       "      <td>农业图书情报学报</td>\n",
       "      <td>2020</td>\n",
       "      <td>23-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[13]Tanja Wissik,Jennifer Edmond,Frank Fischer...</td>\n",
       "      <td>国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心[J]</td>\n",
       "      <td>图书馆论坛</td>\n",
       "      <td>2020</td>\n",
       "      <td>1-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0  \\\n",
       "2                                               [2]张婷   \n",
       "6                                  [4]姚啸华,贺晨芝,徐孝娟,全石峰   \n",
       "12                                       [7]刘昱彤,吴斌,白婷   \n",
       "18                                        [10]姜育彦,李雅茹   \n",
       "24  [13]Tanja Wissik,Jennifer Edmond,Frank Fischer...   \n",
       "\n",
       "                                          1        0_    1_          1  \n",
       "2                      台湾地区数字人文研究特色与实践启示[J]   高校图书馆工作  2020      41-45  \n",
       "6   面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例[J]     图书馆杂志  2020    105-112  \n",
       "12                         古诗词图谱的构建及分析研究[J]  计算机研究与发展  2020  1252-1268  \n",
       "18                 基于数字人文视角的“情感——时空”模型探析[J]  农业图书情报学报  2020      23-33  \n",
       "24  国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心[J]     图书馆论坛  2020       1-27  "
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第三步,拆分\n",
    "##第一次拆分,使用 .\n",
    "refs=refs.str.split(r'.',expand=True)\n",
    "##第二次拆分,使用 , 将未分拆的列进一步拆分,并与其他列join\n",
    "refs=refs[[0,1]].join(refs[2].str.split(',',expand=True),rsuffix='_')\n",
    "##第三次拆分,选取有用的非空列,将未拆分的列进一步拆分,并于其他有用的列join\n",
    "refs=refs[['0','1','0_','1_']].join(refs[2].str.split(':',expand=True),rsuffix='_')\n",
    "# 第四步,丢掉空行\n",
    "refs=refs[['0','1','0_','1_',1]].dropna()\n",
    "refs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>作者</th>\n",
       "      <th>标题</th>\n",
       "      <th>期刊</th>\n",
       "      <th>年</th>\n",
       "      <th>页码</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2]张婷</td>\n",
       "      <td>台湾地区数字人文研究特色与实践启示[J]</td>\n",
       "      <td>高校图书馆工作</td>\n",
       "      <td>2020</td>\n",
       "      <td>41-45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[4]姚啸华,贺晨芝,徐孝娟,全石峰</td>\n",
       "      <td>面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例[J]</td>\n",
       "      <td>图书馆杂志</td>\n",
       "      <td>2020</td>\n",
       "      <td>105-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[7]刘昱彤,吴斌,白婷</td>\n",
       "      <td>古诗词图谱的构建及分析研究[J]</td>\n",
       "      <td>计算机研究与发展</td>\n",
       "      <td>2020</td>\n",
       "      <td>1252-1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[10]姜育彦,李雅茹</td>\n",
       "      <td>基于数字人文视角的“情感——时空”模型探析[J]</td>\n",
       "      <td>农业图书情报学报</td>\n",
       "      <td>2020</td>\n",
       "      <td>23-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[13]Tanja Wissik,Jennifer Edmond,Frank Fischer...</td>\n",
       "      <td>国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心[J]</td>\n",
       "      <td>图书馆论坛</td>\n",
       "      <td>2020</td>\n",
       "      <td>1-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>[488]郭利敏,葛亮,刘悦如</td>\n",
       "      <td>卷积神经网络在古籍汉字识别中的应用实践[J]</td>\n",
       "      <td>图书馆论坛</td>\n",
       "      <td>2019</td>\n",
       "      <td>142-148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>[492]陆燕,卢章平</td>\n",
       "      <td>美国高校图书馆的服务转型与发展——CLSEP2018访学报告[J]</td>\n",
       "      <td>图书情报研究</td>\n",
       "      <td>2019</td>\n",
       "      <td>68-73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>[493]郑丽央,许春漫</td>\n",
       "      <td>美国高校图书馆数字学术馆员队伍建设及启示[J]</td>\n",
       "      <td>情报资料工作</td>\n",
       "      <td>2019</td>\n",
       "      <td>100-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>[494]王辉,罗茜,陈圆,谭光辉</td>\n",
       "      <td>数字时代人文学科的命运——“开明论坛”第四期讨论会综述[J]</td>\n",
       "      <td>绵阳师范学院学报</td>\n",
       "      <td>2019</td>\n",
       "      <td>109-114+144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>[499]叶亢</td>\n",
       "      <td>数字人文与英国报刊史研究——以“Gale原始档案数据库”为核心[J]</td>\n",
       "      <td>新闻研究导刊</td>\n",
       "      <td>2019</td>\n",
       "      <td>17-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    作者  \\\n",
       "2                                                [2]张婷   \n",
       "6                                   [4]姚啸华,贺晨芝,徐孝娟,全石峰   \n",
       "12                                        [7]刘昱彤,吴斌,白婷   \n",
       "18                                         [10]姜育彦,李雅茹   \n",
       "24   [13]Tanja Wissik,Jennifer Edmond,Frank Fischer...   \n",
       "..                                                 ...   \n",
       "974                                    [488]郭利敏,葛亮,刘悦如   \n",
       "982                                        [492]陆燕,卢章平   \n",
       "984                                       [493]郑丽央,许春漫   \n",
       "986                                  [494]王辉,罗茜,陈圆,谭光辉   \n",
       "996                                            [499]叶亢   \n",
       "\n",
       "                                          标题        期刊     年           页码  \n",
       "2                       台湾地区数字人文研究特色与实践启示[J]   高校图书馆工作  2020        41-45  \n",
       "6    面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例[J]     图书馆杂志  2020      105-112  \n",
       "12                          古诗词图谱的构建及分析研究[J]  计算机研究与发展  2020    1252-1268  \n",
       "18                  基于数字人文视角的“情感——时空”模型探析[J]  农业图书情报学报  2020        23-33  \n",
       "24   国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心[J]     图书馆论坛  2020         1-27  \n",
       "..                                       ...       ...   ...          ...  \n",
       "974                   卷积神经网络在古籍汉字识别中的应用实践[J]     图书馆论坛  2019      142-148  \n",
       "982        美国高校图书馆的服务转型与发展——CLSEP2018访学报告[J]    图书情报研究  2019        68-73  \n",
       "984                  美国高校图书馆数字学术馆员队伍建设及启示[J]    情报资料工作  2019      100-112  \n",
       "986           数字时代人文学科的命运——“开明论坛”第四期讨论会综述[J]  绵阳师范学院学报  2019  109-114+144  \n",
       "996       数字人文与英国报刊史研究——以“Gale原始档案数据库”为核心[J]    新闻研究导刊  2019        17-19  \n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 第五步,为拆分后的列添加列名\n",
    "refs.columns=['作者','标题','期刊','年','页码']\n",
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>作者</th>\n",
       "      <th>标题</th>\n",
       "      <th>期刊</th>\n",
       "      <th>年</th>\n",
       "      <th>页码</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>张婷</td>\n",
       "      <td>台湾地区数字人文研究特色与实践启示</td>\n",
       "      <td>高校图书馆工作</td>\n",
       "      <td>2020</td>\n",
       "      <td>41-45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>姚啸华,贺晨芝,徐孝娟,全石峰</td>\n",
       "      <td>面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例</td>\n",
       "      <td>图书馆杂志</td>\n",
       "      <td>2020</td>\n",
       "      <td>105-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>刘昱彤,吴斌,白婷</td>\n",
       "      <td>古诗词图谱的构建及分析研究</td>\n",
       "      <td>计算机研究与发展</td>\n",
       "      <td>2020</td>\n",
       "      <td>1252-1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>姜育彦,李雅茹</td>\n",
       "      <td>基于数字人文视角的“情感——时空”模型探析</td>\n",
       "      <td>农业图书情报学报</td>\n",
       "      <td>2020</td>\n",
       "      <td>23-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Tanja Wissik,Jennifer Edmond,Frank Fischer,Fra...</td>\n",
       "      <td>国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心</td>\n",
       "      <td>图书馆论坛</td>\n",
       "      <td>2020</td>\n",
       "      <td>1-27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>郭利敏,葛亮,刘悦如</td>\n",
       "      <td>卷积神经网络在古籍汉字识别中的应用实践</td>\n",
       "      <td>图书馆论坛</td>\n",
       "      <td>2019</td>\n",
       "      <td>142-148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>陆燕,卢章平</td>\n",
       "      <td>美国高校图书馆的服务转型与发展——CLSEP2018访学报告</td>\n",
       "      <td>图书情报研究</td>\n",
       "      <td>2019</td>\n",
       "      <td>68-73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>郑丽央,许春漫</td>\n",
       "      <td>美国高校图书馆数字学术馆员队伍建设及启示</td>\n",
       "      <td>情报资料工作</td>\n",
       "      <td>2019</td>\n",
       "      <td>100-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>王辉,罗茜,陈圆,谭光辉</td>\n",
       "      <td>数字时代人文学科的命运——“开明论坛”第四期讨论会综述</td>\n",
       "      <td>绵阳师范学院学报</td>\n",
       "      <td>2019</td>\n",
       "      <td>109-114+144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>叶亢</td>\n",
       "      <td>数字人文与英国报刊史研究——以“Gale原始档案数据库”为核心</td>\n",
       "      <td>新闻研究导刊</td>\n",
       "      <td>2019</td>\n",
       "      <td>17-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    作者  \\\n",
       "2                                                   张婷   \n",
       "6                                      姚啸华,贺晨芝,徐孝娟,全石峰   \n",
       "12                                           刘昱彤,吴斌,白婷   \n",
       "18                                             姜育彦,李雅茹   \n",
       "24   Tanja Wissik,Jennifer Edmond,Frank Fischer,Fra...   \n",
       "..                                                 ...   \n",
       "974                                         郭利敏,葛亮,刘悦如   \n",
       "982                                             陆燕,卢章平   \n",
       "984                                            郑丽央,许春漫   \n",
       "986                                       王辉,罗茜,陈圆,谭光辉   \n",
       "996                                                 叶亢   \n",
       "\n",
       "                                       标题        期刊     年           页码  \n",
       "2                       台湾地区数字人文研究特色与实践启示   高校图书馆工作  2020        41-45  \n",
       "6    面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例     图书馆杂志  2020      105-112  \n",
       "12                          古诗词图谱的构建及分析研究  计算机研究与发展  2020    1252-1268  \n",
       "18                  基于数字人文视角的“情感——时空”模型探析  农业图书情报学报  2020        23-33  \n",
       "24   国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心     图书馆论坛  2020         1-27  \n",
       "..                                    ...       ...   ...          ...  \n",
       "974                   卷积神经网络在古籍汉字识别中的应用实践     图书馆论坛  2019      142-148  \n",
       "982        美国高校图书馆的服务转型与发展——CLSEP2018访学报告    图书情报研究  2019        68-73  \n",
       "984                  美国高校图书馆数字学术馆员队伍建设及启示    情报资料工作  2019      100-112  \n",
       "986           数字时代人文学科的命运——“开明论坛”第四期讨论会综述  绵阳师范学院学报  2019  109-114+144  \n",
       "996       数字人文与英国报刊史研究——以“Gale原始档案数据库”为核心    新闻研究导刊  2019        17-19  \n",
       "\n",
       "[141 rows x 5 columns]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 再使用 srt.replace 方法将各列里无关的信息用正则表达式替换掉--作者\n",
    "refs.作者=refs.作者.str.replace(r'\\[\\d+\\]','')\n",
    "refs.标题=refs.标题.str.replace(r'\\[J\\]','')\n",
    "refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>作者</th>\n",
       "      <th>标题</th>\n",
       "      <th>期刊</th>\n",
       "      <th>年</th>\n",
       "      <th>页码</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>张婷</td>\n",
       "      <td>台湾地区数字人文研究特色与实践启示</td>\n",
       "      <td>高校图书馆工作</td>\n",
       "      <td>2020</td>\n",
       "      <td>41-45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>姚啸华,贺晨芝,徐孝娟,全石峰</td>\n",
       "      <td>面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例</td>\n",
       "      <td>图书馆杂志</td>\n",
       "      <td>2020</td>\n",
       "      <td>105-112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>刘昱彤,吴斌,白婷</td>\n",
       "      <td>古诗词图谱的构建及分析研究</td>\n",
       "      <td>计算机研究与发展</td>\n",
       "      <td>2020</td>\n",
       "      <td>1252-1268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>姜育彦,李雅茹</td>\n",
       "      <td>基于数字人文视角的“情感——时空”模型探析</td>\n",
       "      <td>农业图书情报学报</td>\n",
       "      <td>2020</td>\n",
       "      <td>23-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>Tanja Wissik,Jennifer Edmond,Frank Fischer,Fra...</td>\n",
       "      <td>国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心</td>\n",
       "      <td>图书馆论坛</td>\n",
       "      <td>2020</td>\n",
       "      <td>1-27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                 作者  \\\n",
       "0      2                                                 张婷   \n",
       "1      6                                    姚啸华,贺晨芝,徐孝娟,全石峰   \n",
       "2     12                                          刘昱彤,吴斌,白婷   \n",
       "3     18                                            姜育彦,李雅茹   \n",
       "4     24  Tanja Wissik,Jennifer Edmond,Frank Fischer,Fra...   \n",
       "\n",
       "                                     标题        期刊     年         页码  \n",
       "0                     台湾地区数字人文研究特色与实践启示   高校图书馆工作  2020      41-45  \n",
       "1  面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例     图书馆杂志  2020    105-112  \n",
       "2                         古诗词图谱的构建及分析研究  计算机研究与发展  2020  1252-1268  \n",
       "3                 基于数字人文视角的“情感——时空”模型探析  农业图书情报学报  2020      23-33  \n",
       "4  国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心     图书馆论坛  2020       1-27  "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 再把refs拼接成参考文献样式\n",
    "refs=refs.reset_index()\n",
    "refs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[2] 张婷. 台湾地区数字人文研究特色与实践启示[J]. 高校图书馆工作, 2020: 41-45',\n",
       " '[6] 姚啸华,贺晨芝,徐孝娟,全石峰. 面向数字人文的图书馆众包平台构建研究——以上海图书馆历史文献众包平台为例[J]. 图书馆杂志, 2020: 105-112',\n",
       " '[12] 刘昱彤,吴斌,白婷. 古诗词图谱的构建及分析研究[J]. 计算机研究与发展, 2020: 1252-1268',\n",
       " '[18] 姜育彦,李雅茹. 基于数字人文视角的“情感——时空”模型探析[J]. 农业图书情报学报, 2020: 23-33',\n",
       " '[24] Tanja Wissik,Jennifer Edmond,Frank Fischer,Franciska de Jong,Stefania Scagliola,Andrea Scharnhorst,Hendrik Schmeer,Walter Scholger,Leon Wessels,郑炜楠,肖鹏. 国际视野下的数字人文教育——基础设施视角的社区驱动型数字人文课程登记中心[J]. 图书馆论坛, 2020: 1-27',\n",
       " '[32] 朱思苑,卢章平. 美国高校图书馆数字人文中心网站建设对我国的启示[J]. 图书情报研究, 2020: 47-54',\n",
       " '[38] 邓君,孙绍丹,王阮,宋先智. 美国29所高校数字人文项目研究内容解析[J]. 情报资料工作, 2020: 31-40',\n",
       " '[40] 马昭仪,何捷,刘帅帅. 中国古典叙事文学的时空叙事数字模型研究——以《李娃传》为例[J]. 地球信息科学学报, 2020: 967-977',\n",
       " '[42] 许刚,秦昆. “空间综合人文学与社会科学的昨天、今天和明天”沙龙纪要[J]. 地球信息科学学报, 2020: 1176-1177',\n",
       " '[58] 王丽华,刘炜,刘圣婴. 数字人文的理论化趋势前瞻[J]. 中国图书馆学报, 2020: 17-23',\n",
       " '[60] 夏翠娟. 面向人文研究的“数据基础设施”建设——试论图书馆学对数字人文的方法论贡献[J]. 中国图书馆学报, 2020: 24-37',\n",
       " '[62] 李惠,侯君明,陈涛,朱庆华,刘炜. 星汉窈渺——书信网络中蕴藏的人际关系挖掘[J]. 图书馆杂志, 2020: 86-92+80',\n",
       " '[64] 施晓华,王昕. 数字人文社会网络分析方法应用与研究[J]. 图书馆杂志, 2020: 93-99',\n",
       " '[86] 吕璐成,韩涛. AI在图情：人工智能赋能图情服务——2019年图书馆前沿技术论坛(IT4L)会议综述[J]. 农业图书情报学报, 2020: 13-18',\n",
       " '[92] 彭文虎. 数字人文环境下高校图书馆阅读推广创新研究[J]. 图书馆学刊, 2020: 45-49',\n",
       " '[96] 郑永. 数字人文时代公共图书馆公共数字服务体系建设研究[J]. 江苏科技信息, 2020: 15-17+24',\n",
       " '[118] 郑永晓,段海蓉. 古籍数字化、数字人文与古代文学研究——访中国社会科学院郑永晓教授[J]. 吉首大学学报(社会科学版), 2020: 144-151',\n",
       " '[122] 孙辉. 认知科学视角下对数字史学的透视[J]. 文献与数据学报, 2020: 57-67',\n",
       " '[130] 周文杰. 图情档学科发展的数据化趋向解析——基于2019年度学术热点的系统性文献调查[J]. 情报资料工作, 2020: 20-30',\n",
       " '[132] 闫慧. 2019年中国图书情报与档案管理领域研究热点回顾[J]. 情报资料工作, 2020: 5-19']"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [1]肖鹏,彭嗣禹,王蕾. 基本原则与关键问题——学术型图书馆馆员如何启动数字人文项目[J]. 图书馆论坛, 2017, 卷缺失(期缺失): 20-25.\n",
    "ref_= pd.Series('[',index=refs.index).str.cat(refs['index'].astype('str')).str.cat(refs.作者, sep='] ').str.cat(refs.标题,sep='. ').str.cat(refs.期刊,sep='[J]. ').str.cat(refs.年,sep=', ').str.cat(refs.页码,sep=': ').to_list()\n",
    "ref_[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一些问题: 编号应该从1开始;卷期缺失需加进去;结尾应该再加一个'.';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 练习\n",
    "#### 【练习一】 现有一份关于字符串的数据集，请解决以下问题：\n",
    "#### （a）现对字符串编码存储人员信息（在编号后添加ID列），使用如下格式：“×××（名字）：×国人，性别×，生于×年×月×日”\n",
    "#### （b）将（a）中的人员生日信息部分修改为用中文表示（如一九七四年十月二十三日），其余返回格式不变。\n",
    "#### （c）将（b）中的ID列结果拆分为原列表相应的5列，并使用equals检验是否一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>姓名</th>\n",
       "      <th>国籍</th>\n",
       "      <th>性别</th>\n",
       "      <th>出生年</th>\n",
       "      <th>出生月</th>\n",
       "      <th>出生日</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>人员编号</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aesfd</td>\n",
       "      <td>2</td>\n",
       "      <td>男</td>\n",
       "      <td>1942</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fasefa</td>\n",
       "      <td>5</td>\n",
       "      <td>女</td>\n",
       "      <td>1985</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aeagd</td>\n",
       "      <td>4</td>\n",
       "      <td>女</td>\n",
       "      <td>1946</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aef</td>\n",
       "      <td>4</td>\n",
       "      <td>男</td>\n",
       "      <td>1999</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eaf</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          姓名  国籍 性别   出生年  出生月  出生日\n",
       "人员编号                               \n",
       "1      aesfd   2  男  1942    8   10\n",
       "2     fasefa   5  女  1985   10    4\n",
       "3      aeagd   4  女  1946   10   15\n",
       "4        aef   4  男  1999    5   13\n",
       "5        eaf   1  女  2010    6   24"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/String_data_one.csv',index_col='人员编号').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>姓名</th>\n",
       "      <th>国籍</th>\n",
       "      <th>性别</th>\n",
       "      <th>出生年</th>\n",
       "      <th>出生月</th>\n",
       "      <th>出生日</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>人员编号</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aesfd</td>\n",
       "      <td>2</td>\n",
       "      <td>男</td>\n",
       "      <td>1942</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fasefa</td>\n",
       "      <td>5</td>\n",
       "      <td>女</td>\n",
       "      <td>1985</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aeagd</td>\n",
       "      <td>4</td>\n",
       "      <td>女</td>\n",
       "      <td>1946</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aef</td>\n",
       "      <td>4</td>\n",
       "      <td>男</td>\n",
       "      <td>1999</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eaf</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>faefdf</td>\n",
       "      <td>1</td>\n",
       "      <td>男</td>\n",
       "      <td>1942</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daf</td>\n",
       "      <td>4</td>\n",
       "      <td>女</td>\n",
       "      <td>1957</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>faefdf</td>\n",
       "      <td>3</td>\n",
       "      <td>男</td>\n",
       "      <td>1998</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fs</td>\n",
       "      <td>3</td>\n",
       "      <td>女</td>\n",
       "      <td>1957</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>faefdffa</td>\n",
       "      <td>3</td>\n",
       "      <td>女</td>\n",
       "      <td>1988</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>arege</td>\n",
       "      <td>4</td>\n",
       "      <td>男</td>\n",
       "      <td>1933</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ase</td>\n",
       "      <td>3</td>\n",
       "      <td>女</td>\n",
       "      <td>1941</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>fasf</td>\n",
       "      <td>3</td>\n",
       "      <td>男</td>\n",
       "      <td>1987</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gfa</td>\n",
       "      <td>5</td>\n",
       "      <td>女</td>\n",
       "      <td>1967</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>asf</td>\n",
       "      <td>3</td>\n",
       "      <td>男</td>\n",
       "      <td>1990</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>eter</td>\n",
       "      <td>4</td>\n",
       "      <td>男</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>esf</td>\n",
       "      <td>4</td>\n",
       "      <td>女</td>\n",
       "      <td>1987</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>srgdsrg</td>\n",
       "      <td>5</td>\n",
       "      <td>男</td>\n",
       "      <td>1993</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>aefase</td>\n",
       "      <td>4</td>\n",
       "      <td>男</td>\n",
       "      <td>1935</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>szf</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>1936</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            姓名  国籍 性别   出生年  出生月  出生日\n",
       "人员编号                                 \n",
       "1        aesfd   2  男  1942    8   10\n",
       "2       fasefa   5  女  1985   10    4\n",
       "3        aeagd   4  女  1946   10   15\n",
       "4          aef   4  男  1999    5   13\n",
       "5          eaf   1  女  2010    6   24\n",
       "6       faefdf   1  男  1942    1   21\n",
       "7          daf   4  女  1957   11    5\n",
       "8       faefdf   3  男  1998    4    8\n",
       "9           fs   3  女  1957    4   14\n",
       "10    faefdffa   3  女  1988    9   15\n",
       "11       arege   4  男  1933    4   12\n",
       "12         ase   3  女  1941    6   18\n",
       "13        fasf   3  男  1987    7    3\n",
       "14         gfa   5  女  1967   10    3\n",
       "15         asf   3  男  1990   10   20\n",
       "16        eter   4  男  2001    2   13\n",
       "17         esf   4  女  1987    4   28\n",
       "18     srgdsrg   5  男  1993    8   12\n",
       "19      aefase   4  男  1935   10    2\n",
       "20         szf   1  女  1936    6    3"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv('data/String_data_one.csv',index_col='人员编号')#.reset_index()\n",
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['ID']=df1.姓名.str.cat(pd.Series('(名字)',index=df1.index)).str.cat(df1.国籍.astype('str'),sep=':').str.cat(df1.性别,sep='国人,性别').str.cat(df1.出生年.astype('str'),sep=',生于').str.cat(df1.出生月.astype('str'),sep='年').str.cat(df1.出生日.astype('str'),sep='月').str.cat(pd.Series('日',index=df1.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1     aesfd(名字):2国人,性别男,生于1942年8月10日\n",
       "2    fasefa(名字):5国人,性别女,生于1985年10月4日\n",
       "3    aeagd(名字):4国人,性别女,生于1946年10月15日\n",
       "4       aef(名字):4国人,性别男,生于1999年5月13日\n",
       "5       eaf(名字):1国人,性别女,生于2010年6月24日\n",
       "Name: ID, dtype: object"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['ID'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "5       2\n",
       "       ..\n",
       "1996    1\n",
       "1997    1\n",
       "1998    2\n",
       "1999    2\n",
       "2000    1\n",
       "Name: 出生年, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 616,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.出生年.apply(lambda x:list(str(x))).str[0]#.replace('','一')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2c={'1':'一','2':'二','3':'三','4':'四','5':'五','6':'六','7':'七','8':'八','9':'九','0':'零'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1       1942\n",
       "2       1985\n",
       "3       1946\n",
       "4       1999\n",
       "5       2010\n",
       "        ... \n",
       "1996    1984\n",
       "1997    1943\n",
       "1998    2018\n",
       "1999    2005\n",
       "2000    1962\n",
       "Name: 出生年, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 467,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.出生年"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1       一九四二\n",
       "2       一九八五\n",
       "3       一九四六\n",
       "4       一九九九\n",
       "5       二零一零\n",
       "        ... \n",
       "1996    一九八四\n",
       "1997    一九四三\n",
       "1998    二零一八\n",
       "1999    二零零五\n",
       "2000    一九六二\n",
       "Name: 出生年, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.出生年=df1.出生年.astype('str').str.replace(r'(\\d)(\\d)(\\d)(\\d)',lambda x:x.group(1)).replace(num2c).str.cat(df1.出生年.astype('str').str.replace(r'(\\d)(\\d)(\\d)(\\d)',lambda x:x.group(2)).replace(num2c)).str.cat(df1.出生年.astype('str').str.replace(r'(\\d)(\\d)(\\d)(\\d)',lambda x:x.group(3)).replace(num2c)).str.cat(df1.出生年.astype('str').str.replace(r'(\\d)(\\d)(\\d)(\\d)',lambda x:x.group(4)).replace(num2c))\n",
    "df1.出生年"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1       一九四二\n",
       "2       一九八五\n",
       "3       一九四六\n",
       "4       一九九九\n",
       "5       二零一零\n",
       "        ... \n",
       "1996    一九八四\n",
       "1997    一九四三\n",
       "1998    二零一八\n",
       "1999    二零零五\n",
       "2000    一九六二\n",
       "Name: 出生年, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改进,使用\"+\"来匹配一个或多个数字\n",
    "# 年份必然有四个数字\n",
    "%time df1.出生年.astype('str').str.replace(r'(\\d)(\\d)(\\d)(\\d)',lambda x:x.group(4)).replace(num2c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一九四二'"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1=['1', '9', '4', '2']\n",
    "pd.Series(l1).replace(num2c).str.cat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1       一九四二\n",
       "2       一九八五\n",
       "3       一九四六\n",
       "4       一九九九\n",
       "5       二零一零\n",
       "        ... \n",
       "1996    一九八四\n",
       "1997    一九四三\n",
       "1998    二零一八\n",
       "1999    二零零五\n",
       "2000    一九六二\n",
       "Name: 出生年, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 改进为适用于不定长的年份,但耗时久\n",
    "%time df1.出生年.astype('str').apply(lambda x:list(x)).apply(lambda x:pd.Series(x).replace(num2c).str.cat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1       一九四二\n",
       "2       一九八五\n",
       "3       一九四六\n",
       "4       一九九九\n",
       "5       二零一零\n",
       "        ... \n",
       "1996    一九八四\n",
       "1997    一九四三\n",
       "1998    二零一八\n",
       "1999    二零零五\n",
       "2000    一九六二\n",
       "Name: 出生年, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 继续改进--耗时较长\n",
    "df1.出生年.astype('str').apply(lambda x:pd.Series(list(x)).replace(num2c).str.cat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1      八\n",
       "2      十\n",
       "3      十\n",
       "4      五\n",
       "5      六\n",
       "6      一\n",
       "7     十一\n",
       "8      四\n",
       "9      四\n",
       "10     九\n",
       "11     四\n",
       "12     六\n",
       "13     七\n",
       "14     十\n",
       "15     十\n",
       "16     二\n",
       "17     四\n",
       "18     八\n",
       "19     十\n",
       "20     六\n",
       "Name: 出生月, dtype: object"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 出生月\n",
    "month2c={'1':'一','2':'二','3':'三','4':'四','5':'五','6':'六','7':'七','8':'八','9':'九','10':'十','11':'十一','12':'十二'}\n",
    "df1.出生月.astype('str').apply(lambda x:pd.Series(x).replace(month2c).str.cat()).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "人员编号\n",
       "1       十\n",
       "2       四\n",
       "3      十五\n",
       "4      十三\n",
       "5     二十四\n",
       "6     二十一\n",
       "7       五\n",
       "8       八\n",
       "9      十四\n",
       "10     十五\n",
       "11     十二\n",
       "12     十八\n",
       "13      三\n",
       "14      三\n",
       "15     二十\n",
       "16     十三\n",
       "17    二十八\n",
       "18     十二\n",
       "19      二\n",
       "20      三\n",
       "Name: 出生日, dtype: object"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 出生日\n",
    "# 解决方案类似出生月,将31以下的数字和对应的汉字组成字典.\n",
    "day2c={'1':'一','2':'二','3':'三','4':'四','5':'五','6':'六','7':'七','8':'八','9':'九',\n",
    "       '10':'十','11':'十一','12':'十二','13':'十三','14':'十四','15':'十五','16':'十六','17':'十七','18':'十八','19':'十九',\n",
    "       '20':'二十','21':'二十一','22':'二十二','23':'二十三','24':'二十四','25':'二十五','26':'二十六','27':'二十七','28':'二十八','29':'二十九',\n",
    "       '30':'二十','31':'三十一'}\n",
    "df1.出生日.astype('str').apply(lambda x:pd.Series(x).replace(day2c).str.cat()).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个问题:\n",
    "#         如何将阿拉伯数字转为中文表示?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'九零零六四二一二五四四'"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 先定义一个函数: 将阿拉伯数字逐位转为中文\n",
    "def num_to_c(num):\n",
    "    listnum=list(str(num))\n",
    "    num_dict={\"0\":u\"零\",\"1\":u\"一\",\"2\":u\"二\",\"3\":u\"三\",\"4\":u\"四\",\"5\":u\"五\",\"6\":u\"六\",\"7\":u\"七\",\"8\":u\"八\",\"9\":u\"九\"}\n",
    "    lst=[]\n",
    "    for i in listnum:\n",
    "        lst.append(num_dict[i])\n",
    "    c_num=''.join(lst)\n",
    "    return c_num\n",
    "num_to_c(90064212544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'五千四百二十一亿五千四百五十二万一千五百四十四'"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在上述函数基础上,逆序分别增加十百千万亿等单位\n",
    "def num_to_c1(num):\n",
    "    listnum=list(str(num))\n",
    "    listnum.reverse()\n",
    "    num_dict={\"0\":u\"零\",\"1\":u\"一\",\"2\":u\"二\",\"3\":u\"三\",\"4\":u\"四\",\"5\":u\"五\",\"6\":u\"六\",\"7\":u\"七\",\"8\":u\"八\",\"9\":u\"九\"}\n",
    "    p=['','十','百','千','万','十','百','千','亿','十','百','千','兆','十','百','千']\n",
    "    lst=[]\n",
    "    for i in range(len(listnum)):\n",
    "        lst.append(num_dict[listnum[len(listnum)-i-1]])\n",
    "        lst.append(p[len(listnum)-i-1])\n",
    "    c_num=''.join(lst)\n",
    "    return c_num\n",
    "num_to_c1(542154521544)\n",
    "# 存在的问题:\n",
    "##       1.for 循环需要改进得更加易读 \n",
    "##       2.如果中间存在0, 例如 1024 ,应该转为 一千零二十四, 1204 应该转为一千二百零四,1002 应该转为一千零一.\n",
    "##       3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'五亿四千零百零十五万零千五百四十四'"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_to_c1(540050544)\n",
    "#　这是错误的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>姓名</th>\n",
       "      <th>国籍</th>\n",
       "      <th>性别</th>\n",
       "      <th>出生年</th>\n",
       "      <th>出生月</th>\n",
       "      <th>出生日</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>人员编号</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aesfd</td>\n",
       "      <td>2</td>\n",
       "      <td>男</td>\n",
       "      <td>1942</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fasefa</td>\n",
       "      <td>5</td>\n",
       "      <td>女</td>\n",
       "      <td>1985</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aeagd</td>\n",
       "      <td>4</td>\n",
       "      <td>女</td>\n",
       "      <td>1946</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aef</td>\n",
       "      <td>4</td>\n",
       "      <td>男</td>\n",
       "      <td>1999</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>eaf</td>\n",
       "      <td>1</td>\n",
       "      <td>女</td>\n",
       "      <td>2010</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>sdf</td>\n",
       "      <td>5</td>\n",
       "      <td>男</td>\n",
       "      <td>1984</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>hx</td>\n",
       "      <td>1</td>\n",
       "      <td>男</td>\n",
       "      <td>1943</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>drg</td>\n",
       "      <td>5</td>\n",
       "      <td>女</td>\n",
       "      <td>2018</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>zfgzdrg</td>\n",
       "      <td>5</td>\n",
       "      <td>男</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>fsdf</td>\n",
       "      <td>3</td>\n",
       "      <td>女</td>\n",
       "      <td>1962</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           姓名 国籍 性别   出生年 出生月 出生日\n",
       "人员编号                             \n",
       "1       aesfd  2  男  1942   8  10\n",
       "2      fasefa  5  女  1985  10   4\n",
       "3       aeagd  4  女  1946  10  15\n",
       "4         aef  4  男  1999   5  13\n",
       "5         eaf  1  女  2010   6  24\n",
       "...       ... .. ..   ...  ..  ..\n",
       "1996      sdf  5  男  1984   4  17\n",
       "1997       hx  1  男  1943   7  16\n",
       "1998      drg  5  女  2018   4   6\n",
       "1999  zfgzdrg  5  男  2005   1   3\n",
       "2000     fsdf  3  女  1962   7  23\n",
       "\n",
       "[2000 rows x 6 columns]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "df11=df1['ID'].str.split(r'\\(名字\\)\\:',expand=True)\n",
    "df11=df11.iloc[:,0].to_frame().join(df11[1].str.split('国人,性别',expand=True),rsuffix='_')\n",
    "df11=df11.iloc[:,:2].join(df11[1].str.split(',生于',expand=True),rsuffix='_')\n",
    "df11=df11.iloc[:,:3].join(df11[1].str.split('年',expand=True),rsuffix='_')\n",
    "df11=df11.iloc[:,:4].join(df11[1].str.split('月',expand=True),rsuffix='_')\n",
    "df11=df11.iloc[:,:5].join(df11[1].str.split('日',expand=True),rsuffix='_')\n",
    "df11=df11.iloc[:,:6]\n",
    "\n",
    "df11.columns=df1.columns[:6]\n",
    "df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df11.equals(pd.read_csv('data/String_data_one.csv',index_col='人员编号').astype('str'))\n",
    "# 确实是和刚读入的数据是相等的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 【练习二】 现有一份半虚拟的数据集，第一列包含了新型冠状病毒的一些新闻标题，请解决以下问题：\n",
    "#### （a）选出所有关于北京市和上海市新闻标题的所在行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>鄂尔多斯市第2例确诊患者治愈出院</td>\n",
       "      <td>19</td>\n",
       "      <td>363.6923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>云南新增2例，累计124例</td>\n",
       "      <td>-67</td>\n",
       "      <td>-152.281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>武汉协和医院14名感染医护出院</td>\n",
       "      <td>-86</td>\n",
       "      <td>325.6221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>山东新增9例，累计307例</td>\n",
       "      <td>-74</td>\n",
       "      <td>-204.9313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>上海开学日期延至3月</td>\n",
       "      <td>-95</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               col1 col2     col3  \n",
       "0  鄂尔多斯市第2例确诊患者治愈出院   19   363.6923\n",
       "1     云南新增2例，累计124例  -67   -152.281\n",
       "2   武汉协和医院14名感染医护出院  -86   325.6221\n",
       "3     山东新增9例，累计307例  -74  -204.9313\n",
       "4        上海开学日期延至3月  -95       4.05"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('data/String_data_two.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.read_csv('data/String_data_two.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>上海开学日期延至3月</td>\n",
       "      <td>-95</td>\n",
       "      <td>4.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>北京新增25例确诊病例，累计确诊253例</td>\n",
       "      <td>-4</td>\n",
       "      <td>-289.1719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>上海新增10例，累计243例</td>\n",
       "      <td>2</td>\n",
       "      <td>-73.7105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>好消息！青海3名新冠肺炎患者出院</td>\n",
       "      <td>50</td>\n",
       "      <td>-87.4806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>海南新增11例累计89例</td>\n",
       "      <td>3</td>\n",
       "      <td>-135.2709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>青海新增0例确诊病例，疑似新增1例</td>\n",
       "      <td>-40</td>\n",
       "      <td>-264.0617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>海南新增3例，累计46例</td>\n",
       "      <td>-9</td>\n",
       "      <td>-240.5715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>青海新增0例确诊病例</td>\n",
       "      <td>-32</td>\n",
       "      <td>-343.4422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>河北新增17例，累计65例</td>\n",
       "      <td>-54</td>\n",
       "      <td>69.6604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>上海新增5例，累计101例</td>\n",
       "      <td>-95</td>\n",
       "      <td>157.951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     col1 col2     col3  \n",
       "4              上海开学日期延至3月  -95       4.05\n",
       "5    北京新增25例确诊病例，累计确诊253例   -4  -289.1719\n",
       "6          上海新增10例，累计243例    2   -73.7105\n",
       "8        好消息！青海3名新冠肺炎患者出院   50   -87.4806\n",
       "19           海南新增11例累计89例    3  -135.2709\n",
       "..                    ...  ...        ...\n",
       "475     青海新增0例确诊病例，疑似新增1例  -40  -264.0617\n",
       "479          海南新增3例，累计46例   -9  -240.5715\n",
       "480            青海新增0例确诊病例  -32  -343.4422\n",
       "497         河北新增17例，累计65例  -54    69.6604\n",
       "499         上海新增5例，累计101例  -95    157.951\n",
       "\n",
       "[103 rows x 3 columns]"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 北京\n",
    "df2[df2.col1.str.contains('北京')]\n",
    "#上海\n",
    "df2[df2.col1.str.contains('上海')]\n",
    "# 北京或上海\n",
    "df2[df2.col1.str.contains('[上海]|[北京]')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （b）求col2的均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.016"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法１：　直接定义一个函数来处理\n",
    "lst=[]\n",
    "for v in df2.col2.values:\n",
    "    try:\n",
    "        lst.append(int(v))\n",
    "    except:\n",
    "        lst.append(0)\n",
    "lst\n",
    "np.array(lst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0221327967806841"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法2:　利用问题 2 的方法识别出数字, 然后把能转为数字的转为数字,其他转为0后,求均值\n",
    "#df2[['col2','col3  ']].apply(lambda x: float(x) if x.str.contains(r'^-?(\\d+)(\\.\\d+)?$') else 0).mean()\n",
    "df2['col2'][df2['col2'].str.contains(r'^-?(\\d+)(\\.\\d+)?$')].astype('float').mean()\n",
    "# 不一样,这是由于这种方法把不能识别为数字的排除在外了, 求的是能识别为数字的行的均值."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "309    0-\n",
       "396    9`\n",
       "485    /7\n",
       "Name: col2, dtype: object"
      ]
     },
     "execution_count": 486,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 找出有问题的行\n",
    "df2['col2'][~df2['col2'].str.contains(r'^-?(\\d+)(\\.\\d+)?$')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.016"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['col2'][df2['col2'].str.contains(r'^-?(\\d+)(\\.\\d+)?$')].astype('float').sum()/len(df2)\n",
    "# 用总和除以行数才一致."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0221327967806841"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['col2'][df2['col2'].str.contains(r'^-?(\\d+)(\\.\\d+)?$')].astype('float').sum()/(len(df2)-3)\n",
    "# 如果除以的是总行数减去有问题的行数,结果就是直接使用mean得到的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### （c）求col3的均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1849898000000014"
      ]
     },
     "execution_count": 489,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst=[]\n",
    "for v in df2['col3  '].values:\n",
    "    try:\n",
    "        lst.append(float(v))\n",
    "    except:\n",
    "        lst.append(0)\n",
    "lst\n",
    "np.array(lst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['col1', 'col2', 'col3  '], dtype='object')"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns\n",
    "# 第三列的列名有空格."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1921426559356132"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['col3  '][df2['col3  '].str.contains(r'^-?(\\d+)(\\.\\d+)?$')].astype('float').sum()/(len(df2)-len(df2['col3  '][~df2['col3  '].str.contains(r'^-?(\\d+)(\\.\\d+)?$')]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1849897999999996"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['col3  '][df2['col3  '].str.contains(r'^-?(\\d+)(\\.\\d+)?$')].astype('float').sum()/(len(df2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
